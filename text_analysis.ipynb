{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from nltk.corpus import stopwords\\nstop_words = stopwords.words(\"russian\")'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# импорты\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "from string import punctuation\n",
    "import math\n",
    "from tqdm import tqdm_notebook\n",
    "import csv\n",
    "import gensim\n",
    "import random\n",
    "\n",
    "import pymystem3\n",
    "m = pymystem3.Mystem() #для использования лемматизации\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "'''from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"russian\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посчитать информативность слов из б1 и меньше - скачать списки, используемые в РКИ. Исходя из этого вычислять сложность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpack(data):\n",
    "    input = open(data, 'rb')\n",
    "    obj = pickle.load(input)\n",
    "    input.close()\n",
    "    return obj\n",
    "\n",
    "# загружаем частоты лем униграмм\n",
    "unigrams = unpack('1stemgrams.data')\n",
    "# убрали пробел из начала слов\n",
    "unigrams = {w[1:]:f for w,f in unigrams.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigrams = unpack('2grams.data')\n",
    "# убрали пробел из начала слов, пунктуацию (кроме дефисов) и двойные пробелы\n",
    "bigrams  = {''.join([i for i in w[1:] if i not in punctuation.replace('-','')]).replace('  ',' '):f for w,f in bigrams.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['дальнейший допрос',\n",
       " 'оканчивать преступление',\n",
       " 'развитие христианство',\n",
       " 'и затонуть',\n",
       " 'подразумевать она',\n",
       " 'и однозначный',\n",
       " '2 введение',\n",
       " 'желать услышать',\n",
       " 'являться недооценка',\n",
       " 'закрепляться по']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigrams = unpack('3grams.data')\n",
    "# убрали пробел из начала слов, пунктуацию (кроме дефисов) и двойные пробелы\n",
    "trigrams = {''.join([i for i in w[1:] if i not in punctuation.replace('-','')]).replace('  ',' '):f for w,f in trigrams.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['в лифт я',\n",
       " 'что сам русский',\n",
       " 'почему ты думать',\n",
       " 'фестиваль в кольмар',\n",
       " 'твердость и прочность',\n",
       " 'тот число руководитель',\n",
       " 'судьба решать иначе',\n",
       " 'мысль рождаться в',\n",
       " 'мой учение то',\n",
       " 'да только что']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trigrams.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# читает из файла, убирает двойные пробелы и ручные переносы, последний \\n\n",
    "def reading(file):\n",
    "    f = open('texts\\\\{}.txt'.format(file), 'r', encoding='utf-8')\n",
    "    text = f.read()\n",
    "    text = text.replace('  ', ' ')\n",
    "    text = text.replace('-\\n', '')\n",
    "    if text[-1] == '\\n': # убираем последний \\n, если такой есть \n",
    "        text = text[:-1]\n",
    "    f.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minimummaker(): #ф-ция превращения текстового файла с минимумом в питоновский список. Запаковка списка\n",
    "    with open('min.txt', encoding='utf-8') as file:\n",
    "        lemtokens = [morph.parse(i)[0].normal_form for i in re.findall('\\w+', file.read().lower())] #делаем список лемм слов\n",
    "        minimum = list(set(lemtokens)) #убираем повторы\n",
    "        output = open('minimum.pkl', 'wb')\n",
    "        pickle.dump(minimum, output, 2)\n",
    "        output.close()\n",
    "\n",
    "def loadminimum(): #распаковка cписка с минимумом. Возвращает неупорядоченный список\n",
    "    input = open('minimum.pkl', 'rb')\n",
    "    minimum = pickle.load(input)\n",
    "    input.close()\n",
    "    return minimum\n",
    "\n",
    "#minimummaker()\n",
    "minimum = loadminimum()\n",
    "#print(minimum[-50:]) #проверка работы\n",
    "#print(len(minimum)) #2549 слов в списке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('texts1.csv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>texts</th>\n",
       "      <th>rubrics</th>\n",
       "      <th>links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Трамп назвал главу Нью-Йорка худшим мэром в США</td>\n",
       "      <td>Президент США Дональд Трамп назвал главу Нью-Й...</td>\n",
       "      <td>world</td>\n",
       "      <td>https://news.yandex.ru//story/Tramp_nazval_gla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>В Германии недовольные «Северным потоком — 2» ...</td>\n",
       "      <td>Протестующие против строительства «Северного п...</td>\n",
       "      <td>world</td>\n",
       "      <td>https://news.yandex.ru//story/V_Germanii_nedov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>США внесут Huawei в черный список</td>\n",
       "      <td>Американский Минторг объявил, что внесёт крупн...</td>\n",
       "      <td>world</td>\n",
       "      <td>https://news.yandex.ru//story/SSHA_vnesut_Huaw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Мадуро прокомментировал решение США запретить ...</td>\n",
       "      <td>Президент Венесуэлы Николас Мадуро заявил, что...</td>\n",
       "      <td>world</td>\n",
       "      <td>https://news.yandex.ru//story/Maduro_prokommen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Под завалами рухнувшего в Шанхае здания погибл...</td>\n",
       "      <td>Спасателям удалось обнаружить и вытащить из по...</td>\n",
       "      <td>world</td>\n",
       "      <td>https://news.yandex.ru//story/Pod_zavalami_ruk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Появились тизерные изображения новой BMW 1-Series</td>\n",
       "      <td>Новый BMW 1-Series разработан на базе платформ...</td>\n",
       "      <td>world</td>\n",
       "      <td>https://news.yandex.ru//story/Poyavilis_tizern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Philips перешла в Нидерландах на возобновляему...</td>\n",
       "      <td>Нидерландские предприятия Royal Philips NV пол...</td>\n",
       "      <td>world</td>\n",
       "      <td>https://news.yandex.ru//story/Philips_pereshla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Виновницей крупнейшего в Калифорнии пожара наз...</td>\n",
       "      <td>Представители калифорнийского управления лесно...</td>\n",
       "      <td>world</td>\n",
       "      <td>https://news.yandex.ru//story/Vinovnicej_krupn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Рождаемость в США достигла минимума</td>\n",
       "      <td>По данным экспертов, снижение рождаемости набл...</td>\n",
       "      <td>world</td>\n",
       "      <td>https://news.yandex.ru//story/Rozhdaemost_v_SS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>В Турции анализ ДНК выявил новую тайну древней...</td>\n",
       "      <td>Международная группа генетиков и археологов из...</td>\n",
       "      <td>world</td>\n",
       "      <td>https://news.yandex.ru//story/V_Turcii_analiz_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>В США два человека застряли в подъемнике на не...</td>\n",
       "      <td>Пожарная служба американского города Оклахома ...</td>\n",
       "      <td>world</td>\n",
       "      <td>https://news.yandex.ru//story/V_SSHA_dva_chelo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>В Британии избили ребенка из Латвии со словами...</td>\n",
       "      <td>12-летний уроженец Латвии Эдвард Пургайлис был...</td>\n",
       "      <td>world</td>\n",
       "      <td>https://news.yandex.ru//story/V_Britanii_izbil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Рыбак поймал осьминога с огромными щупальцами</td>\n",
       "      <td>В прибрежных водах английского графства Девон ...</td>\n",
       "      <td>world</td>\n",
       "      <td>https://news.yandex.ru//story/Rybak_pojmal_osm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>В Китае автомобиль совершил наезд на прохожих</td>\n",
       "      <td>Отмечается, что водитель продолжил движение по...</td>\n",
       "      <td>world</td>\n",
       "      <td>https://news.yandex.ru//story/V_Kitae_avtomobi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ЧП на испытаниях: немецкая подлодка в Норвегии...</td>\n",
       "      <td>Во время совершения маневров в гавани военно-м...</td>\n",
       "      <td>world</td>\n",
       "      <td>https://news.yandex.ru//story/CHP_na_ispytaniy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Мэрия Екатеринбурга согласилась провести опрос...</td>\n",
       "      <td>Мэрия Екатеринбурга готова провести опрос насе...</td>\n",
       "      <td>politics</td>\n",
       "      <td>https://news.yandex.ru//story/Mehriya_Ekaterin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>С чиновников должен быть особый спрос, заявил ...</td>\n",
       "      <td>Президент РФ Владимир Путин считает, что с чин...</td>\n",
       "      <td>politics</td>\n",
       "      <td>https://news.yandex.ru//story/S_chinovnikov_do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Верховная рада назначила инаугурацию Зеленског...</td>\n",
       "      <td>Верховная рада определила, что приведение к пр...</td>\n",
       "      <td>politics</td>\n",
       "      <td>https://news.yandex.ru//story/Verkhovnaya_rada...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Путин допустил возвращение системы распределен...</td>\n",
       "      <td>Президент России Владимир Путин допустил возро...</td>\n",
       "      <td>politics</td>\n",
       "      <td>https://news.yandex.ru//story/Putin_dopustil_v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Путин обсудит с Собяниным вывоз мусора из Моск...</td>\n",
       "      <td>Президент России Владимир Путин сообщил, что п...</td>\n",
       "      <td>politics</td>\n",
       "      <td>https://news.yandex.ru//story/Putin_obsudit_s_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Путин поручил пересмотреть ценообразование в с...</td>\n",
       "      <td>Правительство работает над изменением устаревш...</td>\n",
       "      <td>politics</td>\n",
       "      <td>https://news.yandex.ru//story/Putin_poruchil_p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Путин назвал пал травы одной из основных причи...</td>\n",
       "      <td>Одной из основных причин пожаров в Забайкалье ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>https://news.yandex.ru//story/Putin_nazval_pal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Тори пытаются определить дату отставки Мэй</td>\n",
       "      <td>Члены Консервативной партии пытались на состоя...</td>\n",
       "      <td>politics</td>\n",
       "      <td>https://news.yandex.ru//story/Tori_pytayutsya_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Меркель подтвердила намерение уйти из политики...</td>\n",
       "      <td>Канцлер Германии Ангела Меркель подтвердила, ч...</td>\n",
       "      <td>politics</td>\n",
       "      <td>https://news.yandex.ru//story/Merkel_podtverdi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Путин пообещал приехать на Чукотку</td>\n",
       "      <td>Президент России Владимир Путин пообещал приех...</td>\n",
       "      <td>politics</td>\n",
       "      <td>https://news.yandex.ru//story/Putin_poobeshhal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Госдума приняла во втором чтении законопроект ...</td>\n",
       "      <td>Государственная дума приняла во втором чтении ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>https://news.yandex.ru//story/Gosduma_prinyala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>«Роскосмос» прокомментировал слухи о скорой от...</td>\n",
       "      <td>Руководитель пресс-службы \"Роскосмоса\" Владими...</td>\n",
       "      <td>politics</td>\n",
       "      <td>https://news.yandex.ru//story/Roskosmos_prokom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Представитель Киева в гуманитарной подгруппе п...</td>\n",
       "      <td>Представитель Украины в гуманитарной подгруппе...</td>\n",
       "      <td>politics</td>\n",
       "      <td>https://news.yandex.ru//story/Predstavitel_Kie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Украинский генерал предупредил о последствиях ...</td>\n",
       "      <td>Если произойдет реальное военное столкновение ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>https://news.yandex.ru//story/Ukrainskij_gener...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>С-400 отгрузят Турции уже в следующем месяце</td>\n",
       "      <td>Российские зенитно-ракетные комплексы С-400 мо...</td>\n",
       "      <td>politics</td>\n",
       "      <td>https://news.yandex.ru//story/S-400_otgruzyat_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Павел Дуров раскритиковал WhatsApp</td>\n",
       "      <td>Основатель Telegram Павел Дуров рассказал о пр...</td>\n",
       "      <td>computers</td>\n",
       "      <td>https://news.yandex.ru//story/Pavel_Durov_rask...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Google создал переводчик, имитирующий голос по...</td>\n",
       "      <td>Google разработала специальный алгоритм, котор...</td>\n",
       "      <td>computers</td>\n",
       "      <td>https://news.yandex.ru//story/Google_sozdal_pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Выпущена первая в мире карта microSD на 1 Тб</td>\n",
       "      <td>Американская компания Sandisk начала онлайн-пр...</td>\n",
       "      <td>computers</td>\n",
       "      <td>https://news.yandex.ru//story/Vypushhena_perva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Любую игру из Steam теперь можно бесплатно зап...</td>\n",
       "      <td>Приложение Steam Link от Valve теперь доступно...</td>\n",
       "      <td>computers</td>\n",
       "      <td>https://news.yandex.ru//story/Lyubuyu_igru_iz_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>«Билайн» пообещал заменить все базовые станции...</td>\n",
       "      <td>К концу 2020 года «ВымпелКом» (бренд «Билайн»)...</td>\n",
       "      <td>computers</td>\n",
       "      <td>https://news.yandex.ru//story/Bilajn_poobeshha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Vivo радикально снизила цены на свои смартфоны...</td>\n",
       "      <td>Компания Vivo сообщила о серьезном снижении це...</td>\n",
       "      <td>computers</td>\n",
       "      <td>https://news.yandex.ru//story/Vivo_radikalno_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>В Epic Games Store открылся предзаказ Detroit:...</td>\n",
       "      <td>Ранее все три проекта были доступны лишь на ко...</td>\n",
       "      <td>computers</td>\n",
       "      <td>https://news.yandex.ru//story/V_Epic_Games_Sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>94% студентов мечтают о работе в игровой индус...</td>\n",
       "      <td>Компания Riot Games, известная по успешной МОВ...</td>\n",
       "      <td>computers</td>\n",
       "      <td>https://news.yandex.ru//story/94_studentov_mec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Представлен компактный смартфон Sony Xperia Ace</td>\n",
       "      <td>Стали известны точные характеристики модели и ...</td>\n",
       "      <td>computers</td>\n",
       "      <td>https://news.yandex.ru//story/Predstavlen_komp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Раскрыты технические характеристики Xiaomi Mi ...</td>\n",
       "      <td>Презентация новинки еще не названа, а вот техн...</td>\n",
       "      <td>computers</td>\n",
       "      <td>https://news.yandex.ru//story/Raskryty_tekhnic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>В Германии прошли испытания воздушного электро...</td>\n",
       "      <td>Немецкая компания Lilium провела успешные бесп...</td>\n",
       "      <td>computers</td>\n",
       "      <td>https://news.yandex.ru//story/V_Germanii_prosh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Samsung исправила недостатки Galaxy Fold</td>\n",
       "      <td>Компания Samsung устранила недостатки складног...</td>\n",
       "      <td>computers</td>\n",
       "      <td>https://news.yandex.ru//story/Samsung_ispravil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>OnePlus 7 Pro стоит в Китае всего $580</td>\n",
       "      <td>OnePlus 7 Pro в Китае же стоит $580-725. Таким...</td>\n",
       "      <td>computers</td>\n",
       "      <td>https://news.yandex.ru//story/OnePlus_7_Pro_st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Релиз Skull Bones от Ubisoft вновь отложен на ...</td>\n",
       "      <td>Пиратский приключенческий экшен Skull &amp;amp; Bo...</td>\n",
       "      <td>computers</td>\n",
       "      <td>https://news.yandex.ru//story/Reliz_Skull_Bone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>МВД может создать портал для обращений людей п...</td>\n",
       "      <td>Согласно концепции, МВД предстоит создать ситу...</td>\n",
       "      <td>computers</td>\n",
       "      <td>https://news.yandex.ru//story/MVD_mozhet_sozda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Легендарный манускрипт Войнича расшифровали в ...</td>\n",
       "      <td>Манускрипт, написанный, предположительно, в XV...</td>\n",
       "      <td>science</td>\n",
       "      <td>https://news.yandex.ru//story/Legendarnyj_manu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Ученые рассказали, какие пары чаще всего заним...</td>\n",
       "      <td>Частоту и качество половых контактов обычно св...</td>\n",
       "      <td>science</td>\n",
       "      <td>https://news.yandex.ru//story/Uchenye_rasskaza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Искусственный интеллект создал уникальный реце...</td>\n",
       "      <td>Уникальный рецепт односолодового виски создал ...</td>\n",
       "      <td>science</td>\n",
       "      <td>https://news.yandex.ru//story/Iskusstvennyj_in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Отит начнут выявлять с помощью смартфона</td>\n",
       "      <td>Специалисты Вашингтонского университета в США ...</td>\n",
       "      <td>science</td>\n",
       "      <td>https://news.yandex.ru//story/Otit_nachnut_vyy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Стратосферный планер испытают на высоте 27 тыс...</td>\n",
       "      <td>Подразделение Perlan Project европейского авиа...</td>\n",
       "      <td>science</td>\n",
       "      <td>https://news.yandex.ru//story/Stratosfernyj_pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Неандертальцы и современные люди разошлись 800...</td>\n",
       "      <td>Группа ученых предоставила выводы о том, что э...</td>\n",
       "      <td>science</td>\n",
       "      <td>https://news.yandex.ru//story/Neandertalcy_i_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Ученые показали, как меняется голова ребенка в...</td>\n",
       "      <td>Группа американских и французских ученых прове...</td>\n",
       "      <td>science</td>\n",
       "      <td>https://news.yandex.ru//story/Uchenye_pokazali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Тяжелые инфекционные заболевания лягушек разви...</td>\n",
       "      <td>Ученые провели исследование, которое показало ...</td>\n",
       "      <td>science</td>\n",
       "      <td>https://news.yandex.ru//story/Tyazhelye_infekc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Учёные: Отрицательной калорийности не бывает</td>\n",
       "      <td>Новое исследование опровергло имеющуюся до эти...</td>\n",
       "      <td>science</td>\n",
       "      <td>https://news.yandex.ru//story/Uchyonye_Otricat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Российские ученые МФТИ впервые в мире получили...</td>\n",
       "      <td>Российские ученые впервые в мире получили «дву...</td>\n",
       "      <td>science</td>\n",
       "      <td>https://news.yandex.ru//story/Rossijskie_uchen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Бесплатный способ омоложения предложили ученые</td>\n",
       "      <td>Аниш Бхува проанализировал данные о 139 начина...</td>\n",
       "      <td>science</td>\n",
       "      <td>https://news.yandex.ru//story/Besplatnyj_sposo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Быструю ходьбу связали с высокой продолжительн...</td>\n",
       "      <td>Средний возраст включенных в анализ людей сост...</td>\n",
       "      <td>science</td>\n",
       "      <td>https://news.yandex.ru//story/Bystruyu_khodbu_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Сканированием мозга вместо детектора лжи легко...</td>\n",
       "      <td>Натренированным обманщикам часто удается успеш...</td>\n",
       "      <td>science</td>\n",
       "      <td>https://news.yandex.ru//story/Skanirovaniem_mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Создана «самая искусственная» форма жизни на З...</td>\n",
       "      <td>Эксперты утверждают, что на сегодняшний день э...</td>\n",
       "      <td>science</td>\n",
       "      <td>https://news.yandex.ru//story/Sozdana_samaya_i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Сильная магнитная буря началась на Земле</td>\n",
       "      <td>На планете Земля зарегистрирована самая крупна...</td>\n",
       "      <td>science</td>\n",
       "      <td>https://news.yandex.ru//story/Silnaya_magnitna...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             headlines  \\\n",
       "0      Трамп назвал главу Нью-Йорка худшим мэром в США   \n",
       "1    В Германии недовольные «Северным потоком — 2» ...   \n",
       "2                    США внесут Huawei в черный список   \n",
       "3    Мадуро прокомментировал решение США запретить ...   \n",
       "4    Под завалами рухнувшего в Шанхае здания погибл...   \n",
       "5    Появились тизерные изображения новой BMW 1-Series   \n",
       "6    Philips перешла в Нидерландах на возобновляему...   \n",
       "7    Виновницей крупнейшего в Калифорнии пожара наз...   \n",
       "8                  Рождаемость в США достигла минимума   \n",
       "9    В Турции анализ ДНК выявил новую тайну древней...   \n",
       "10   В США два человека застряли в подъемнике на не...   \n",
       "11   В Британии избили ребенка из Латвии со словами...   \n",
       "12       Рыбак поймал осьминога с огромными щупальцами   \n",
       "13       В Китае автомобиль совершил наезд на прохожих   \n",
       "14   ЧП на испытаниях: немецкая подлодка в Норвегии...   \n",
       "15   Мэрия Екатеринбурга согласилась провести опрос...   \n",
       "16   С чиновников должен быть особый спрос, заявил ...   \n",
       "17   Верховная рада назначила инаугурацию Зеленског...   \n",
       "18   Путин допустил возвращение системы распределен...   \n",
       "19   Путин обсудит с Собяниным вывоз мусора из Моск...   \n",
       "20   Путин поручил пересмотреть ценообразование в с...   \n",
       "21   Путин назвал пал травы одной из основных причи...   \n",
       "22          Тори пытаются определить дату отставки Мэй   \n",
       "23   Меркель подтвердила намерение уйти из политики...   \n",
       "24                  Путин пообещал приехать на Чукотку   \n",
       "25   Госдума приняла во втором чтении законопроект ...   \n",
       "26   «Роскосмос» прокомментировал слухи о скорой от...   \n",
       "27   Представитель Киева в гуманитарной подгруппе п...   \n",
       "28   Украинский генерал предупредил о последствиях ...   \n",
       "29        С-400 отгрузят Турции уже в следующем месяце   \n",
       "..                                                 ...   \n",
       "105                 Павел Дуров раскритиковал WhatsApp   \n",
       "106  Google создал переводчик, имитирующий голос по...   \n",
       "107       Выпущена первая в мире карта microSD на 1 Тб   \n",
       "108  Любую игру из Steam теперь можно бесплатно зап...   \n",
       "109  «Билайн» пообещал заменить все базовые станции...   \n",
       "110  Vivo радикально снизила цены на свои смартфоны...   \n",
       "111  В Epic Games Store открылся предзаказ Detroit:...   \n",
       "112  94% студентов мечтают о работе в игровой индус...   \n",
       "113    Представлен компактный смартфон Sony Xperia Ace   \n",
       "114  Раскрыты технические характеристики Xiaomi Mi ...   \n",
       "115  В Германии прошли испытания воздушного электро...   \n",
       "116           Samsung исправила недостатки Galaxy Fold   \n",
       "117             OnePlus 7 Pro стоит в Китае всего $580   \n",
       "118  Релиз Skull Bones от Ubisoft вновь отложен на ...   \n",
       "119  МВД может создать портал для обращений людей п...   \n",
       "120  Легендарный манускрипт Войнича расшифровали в ...   \n",
       "121  Ученые рассказали, какие пары чаще всего заним...   \n",
       "122  Искусственный интеллект создал уникальный реце...   \n",
       "123           Отит начнут выявлять с помощью смартфона   \n",
       "124  Стратосферный планер испытают на высоте 27 тыс...   \n",
       "125  Неандертальцы и современные люди разошлись 800...   \n",
       "126  Ученые показали, как меняется голова ребенка в...   \n",
       "127  Тяжелые инфекционные заболевания лягушек разви...   \n",
       "128       Учёные: Отрицательной калорийности не бывает   \n",
       "129  Российские ученые МФТИ впервые в мире получили...   \n",
       "130     Бесплатный способ омоложения предложили ученые   \n",
       "131  Быструю ходьбу связали с высокой продолжительн...   \n",
       "132  Сканированием мозга вместо детектора лжи легко...   \n",
       "133  Создана «самая искусственная» форма жизни на З...   \n",
       "134           Сильная магнитная буря началась на Земле   \n",
       "\n",
       "                                                 texts    rubrics  \\\n",
       "0    Президент США Дональд Трамп назвал главу Нью-Й...      world   \n",
       "1    Протестующие против строительства «Северного п...      world   \n",
       "2    Американский Минторг объявил, что внесёт крупн...      world   \n",
       "3    Президент Венесуэлы Николас Мадуро заявил, что...      world   \n",
       "4    Спасателям удалось обнаружить и вытащить из по...      world   \n",
       "5    Новый BMW 1-Series разработан на базе платформ...      world   \n",
       "6    Нидерландские предприятия Royal Philips NV пол...      world   \n",
       "7    Представители калифорнийского управления лесно...      world   \n",
       "8    По данным экспертов, снижение рождаемости набл...      world   \n",
       "9    Международная группа генетиков и археологов из...      world   \n",
       "10   Пожарная служба американского города Оклахома ...      world   \n",
       "11   12-летний уроженец Латвии Эдвард Пургайлис был...      world   \n",
       "12   В прибрежных водах английского графства Девон ...      world   \n",
       "13   Отмечается, что водитель продолжил движение по...      world   \n",
       "14   Во время совершения маневров в гавани военно-м...      world   \n",
       "15   Мэрия Екатеринбурга готова провести опрос насе...   politics   \n",
       "16   Президент РФ Владимир Путин считает, что с чин...   politics   \n",
       "17   Верховная рада определила, что приведение к пр...   politics   \n",
       "18   Президент России Владимир Путин допустил возро...   politics   \n",
       "19   Президент России Владимир Путин сообщил, что п...   politics   \n",
       "20   Правительство работает над изменением устаревш...   politics   \n",
       "21   Одной из основных причин пожаров в Забайкалье ...   politics   \n",
       "22   Члены Консервативной партии пытались на состоя...   politics   \n",
       "23   Канцлер Германии Ангела Меркель подтвердила, ч...   politics   \n",
       "24   Президент России Владимир Путин пообещал приех...   politics   \n",
       "25   Государственная дума приняла во втором чтении ...   politics   \n",
       "26   Руководитель пресс-службы \"Роскосмоса\" Владими...   politics   \n",
       "27   Представитель Украины в гуманитарной подгруппе...   politics   \n",
       "28   Если произойдет реальное военное столкновение ...   politics   \n",
       "29   Российские зенитно-ракетные комплексы С-400 мо...   politics   \n",
       "..                                                 ...        ...   \n",
       "105  Основатель Telegram Павел Дуров рассказал о пр...  computers   \n",
       "106  Google разработала специальный алгоритм, котор...  computers   \n",
       "107  Американская компания Sandisk начала онлайн-пр...  computers   \n",
       "108  Приложение Steam Link от Valve теперь доступно...  computers   \n",
       "109  К концу 2020 года «ВымпелКом» (бренд «Билайн»)...  computers   \n",
       "110  Компания Vivo сообщила о серьезном снижении це...  computers   \n",
       "111  Ранее все три проекта были доступны лишь на ко...  computers   \n",
       "112  Компания Riot Games, известная по успешной МОВ...  computers   \n",
       "113  Стали известны точные характеристики модели и ...  computers   \n",
       "114  Презентация новинки еще не названа, а вот техн...  computers   \n",
       "115  Немецкая компания Lilium провела успешные бесп...  computers   \n",
       "116  Компания Samsung устранила недостатки складног...  computers   \n",
       "117  OnePlus 7 Pro в Китае же стоит $580-725. Таким...  computers   \n",
       "118  Пиратский приключенческий экшен Skull &amp; Bo...  computers   \n",
       "119  Согласно концепции, МВД предстоит создать ситу...  computers   \n",
       "120  Манускрипт, написанный, предположительно, в XV...    science   \n",
       "121  Частоту и качество половых контактов обычно св...    science   \n",
       "122  Уникальный рецепт односолодового виски создал ...    science   \n",
       "123  Специалисты Вашингтонского университета в США ...    science   \n",
       "124  Подразделение Perlan Project европейского авиа...    science   \n",
       "125  Группа ученых предоставила выводы о том, что э...    science   \n",
       "126  Группа американских и французских ученых прове...    science   \n",
       "127  Ученые провели исследование, которое показало ...    science   \n",
       "128  Новое исследование опровергло имеющуюся до эти...    science   \n",
       "129  Российские ученые впервые в мире получили «дву...    science   \n",
       "130  Аниш Бхува проанализировал данные о 139 начина...    science   \n",
       "131  Средний возраст включенных в анализ людей сост...    science   \n",
       "132  Натренированным обманщикам часто удается успеш...    science   \n",
       "133  Эксперты утверждают, что на сегодняшний день э...    science   \n",
       "134  На планете Земля зарегистрирована самая крупна...    science   \n",
       "\n",
       "                                                 links  \n",
       "0    https://news.yandex.ru//story/Tramp_nazval_gla...  \n",
       "1    https://news.yandex.ru//story/V_Germanii_nedov...  \n",
       "2    https://news.yandex.ru//story/SSHA_vnesut_Huaw...  \n",
       "3    https://news.yandex.ru//story/Maduro_prokommen...  \n",
       "4    https://news.yandex.ru//story/Pod_zavalami_ruk...  \n",
       "5    https://news.yandex.ru//story/Poyavilis_tizern...  \n",
       "6    https://news.yandex.ru//story/Philips_pereshla...  \n",
       "7    https://news.yandex.ru//story/Vinovnicej_krupn...  \n",
       "8    https://news.yandex.ru//story/Rozhdaemost_v_SS...  \n",
       "9    https://news.yandex.ru//story/V_Turcii_analiz_...  \n",
       "10   https://news.yandex.ru//story/V_SSHA_dva_chelo...  \n",
       "11   https://news.yandex.ru//story/V_Britanii_izbil...  \n",
       "12   https://news.yandex.ru//story/Rybak_pojmal_osm...  \n",
       "13   https://news.yandex.ru//story/V_Kitae_avtomobi...  \n",
       "14   https://news.yandex.ru//story/CHP_na_ispytaniy...  \n",
       "15   https://news.yandex.ru//story/Mehriya_Ekaterin...  \n",
       "16   https://news.yandex.ru//story/S_chinovnikov_do...  \n",
       "17   https://news.yandex.ru//story/Verkhovnaya_rada...  \n",
       "18   https://news.yandex.ru//story/Putin_dopustil_v...  \n",
       "19   https://news.yandex.ru//story/Putin_obsudit_s_...  \n",
       "20   https://news.yandex.ru//story/Putin_poruchil_p...  \n",
       "21   https://news.yandex.ru//story/Putin_nazval_pal...  \n",
       "22   https://news.yandex.ru//story/Tori_pytayutsya_...  \n",
       "23   https://news.yandex.ru//story/Merkel_podtverdi...  \n",
       "24   https://news.yandex.ru//story/Putin_poobeshhal...  \n",
       "25   https://news.yandex.ru//story/Gosduma_prinyala...  \n",
       "26   https://news.yandex.ru//story/Roskosmos_prokom...  \n",
       "27   https://news.yandex.ru//story/Predstavitel_Kie...  \n",
       "28   https://news.yandex.ru//story/Ukrainskij_gener...  \n",
       "29   https://news.yandex.ru//story/S-400_otgruzyat_...  \n",
       "..                                                 ...  \n",
       "105  https://news.yandex.ru//story/Pavel_Durov_rask...  \n",
       "106  https://news.yandex.ru//story/Google_sozdal_pe...  \n",
       "107  https://news.yandex.ru//story/Vypushhena_perva...  \n",
       "108  https://news.yandex.ru//story/Lyubuyu_igru_iz_...  \n",
       "109  https://news.yandex.ru//story/Bilajn_poobeshha...  \n",
       "110  https://news.yandex.ru//story/Vivo_radikalno_s...  \n",
       "111  https://news.yandex.ru//story/V_Epic_Games_Sto...  \n",
       "112  https://news.yandex.ru//story/94_studentov_mec...  \n",
       "113  https://news.yandex.ru//story/Predstavlen_komp...  \n",
       "114  https://news.yandex.ru//story/Raskryty_tekhnic...  \n",
       "115  https://news.yandex.ru//story/V_Germanii_prosh...  \n",
       "116  https://news.yandex.ru//story/Samsung_ispravil...  \n",
       "117  https://news.yandex.ru//story/OnePlus_7_Pro_st...  \n",
       "118  https://news.yandex.ru//story/Reliz_Skull_Bone...  \n",
       "119  https://news.yandex.ru//story/MVD_mozhet_sozda...  \n",
       "120  https://news.yandex.ru//story/Legendarnyj_manu...  \n",
       "121  https://news.yandex.ru//story/Uchenye_rasskaza...  \n",
       "122  https://news.yandex.ru//story/Iskusstvennyj_in...  \n",
       "123  https://news.yandex.ru//story/Otit_nachnut_vyy...  \n",
       "124  https://news.yandex.ru//story/Stratosfernyj_pl...  \n",
       "125  https://news.yandex.ru//story/Neandertalcy_i_s...  \n",
       "126  https://news.yandex.ru//story/Uchenye_pokazali...  \n",
       "127  https://news.yandex.ru//story/Tyazhelye_infekc...  \n",
       "128  https://news.yandex.ru//story/Uchyonye_Otricat...  \n",
       "129  https://news.yandex.ru//story/Rossijskie_uchen...  \n",
       "130  https://news.yandex.ru//story/Besplatnyj_sposo...  \n",
       "131  https://news.yandex.ru//story/Bystruyu_khodbu_...  \n",
       "132  https://news.yandex.ru//story/Skanirovaniem_mo...  \n",
       "133  https://news.yandex.ru//story/Sozdana_samaya_i...  \n",
       "134  https://news.yandex.ru//story/Silnaya_magnitna...  \n",
       "\n",
       "[135 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def measure potential(n = 5, base = ['yarn','model','asis'][0]):\n",
    "    \n",
    "    def make_sample(n):\n",
    "        rubric = list(set(df['rubrics']))\n",
    "        text_rubrics = df.filter(items=['texts', 'rubrics'])\n",
    "        text_to_potential = []\n",
    "        for r in rubric:\n",
    "            rub_texts = []\n",
    "            for l in range(len(text_rubrics)):\n",
    "                if text_rubrics.loc[l][1]==r:\n",
    "                    rub_texts.append(text_rubrics.loc[l][0])\n",
    "            text_to_potential.append(random.sample(rub_texts,n))\n",
    "        return text_to_potential\n",
    "    \n",
    "    text_to_potential = make_sample(n)\n",
    "    \n",
    "    positif = 0\n",
    "    total = len(text_to_potential)\n",
    "    \n",
    "    for text in text_to_potential:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# всякие непонятные куски, портящие статистику по минимумам:\n",
    "to_remove = ['ми', 'вод', 'кий', 'ре','вая', 'ча', 'то быть', 'чка', 'стен']\n",
    "with open('B1.txt', 'r', encoding='utf-8') as file:\n",
    "    lemtokens = [morph.parse(i)[0].normal_form for i in file.read().lower().split('\\n') if morph.parse(i)[0].normal_form not in to_remove] #делаем список лемм слов\n",
    "    minimum = list(set(lemtokens)) #убираем повторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['пессимист', 'постараться ', 'серебро ', 'лёгкий', 'бой', 'зарубежный', 'стоить ', 'сфотографировать ', 'учиться ', 'экспорт ', 'сейчас', 'приехать ', 'бензин', 'момент', 'мороз', 'шуметь ', 'надежда', 'чувствовать ', 'проверить ', 'вырасти ', 'представитель', 'салют', 'церковь', 'таблетка', 'такси ', 'загорать ', 'мы', 'вести ', 'чтобы', 'бог', 'семья', 'тихо', 'жаль ', 'самостоятельно', 'небо', 'выражать ', 'навстречу ', 'фестиваль', 'выполнять', 'приглашать ', 'должный', 'жестокий', 'по-твоему', 'считать ', 'сдать ', 'эксперимент', 'официальный', 'никакой', 'модель ', 'посольство']\n"
     ]
    }
   ],
   "source": [
    "print(minimum[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# штука, которая рассчитывает порог коэффициента информативности слова на основе лексического минимума - Я ПРИДУМАЛА!\n",
    "# отрицательный. чем коэффициент информативности меньше, чем сложнее слово (с уменьшением дроби логарифм уменьшается тоже)\n",
    "from collections import Counter\n",
    "\n",
    "det = sum(f for f in unigrams.values())+1\n",
    "inf_coef = Counter([round(math.log((unigrams.get(w)+1)/det),1) for w in minimum if w in unigrams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "for w in minimum:\n",
    "    if w in unigrams:\n",
    "        if round(math.log((unigrams.get(w)+1)/det),1)<-17:\n",
    "            l.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['съесть']\n"
     ]
    }
   ],
   "source": [
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.median(list(inf_coef.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-9.3, 32),\n",
       " (-9.0, 24),\n",
       " (-8.9, 21),\n",
       " (-9.2, 20),\n",
       " (-9.4, 20),\n",
       " (-8.6, 19),\n",
       " (-8.8, 18),\n",
       " (-9.7, 18),\n",
       " (-9.8, 18),\n",
       " (-9.6, 17)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_coef.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# загрузка модели\n",
    "def model_loading(file):\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(file, binary=False)\n",
    "    model.init_sims(replace=True)\n",
    "    print('Done!') \n",
    "    return model\n",
    "\n",
    "model = model_loading('news_upos_cbow_600_2_2018.vec') #на загрузку тратится минут 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# загрузка словаря ASIS\n",
    "with open('syns.data', 'rb') as f:\n",
    "     asis = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Таблица конверсии в UPoS из тэгов Mystem\n",
    "# словарь, переводящий теги mystem в universal теги моделей\n",
    "mystem_tags = {'A' : 'ADJ',\n",
    "       'ADV' : 'ADV',\n",
    "       'ADVPRO' : 'ADV',\n",
    "       'ANUM' : 'ADJ',\n",
    "       'APRO' : 'DET',\n",
    "       'COM' : 'ADJ',\n",
    "       'CONJ' : 'SCONJ',\n",
    "       'INTJ' : 'INTJ',\n",
    "       'NONLEX' : 'X',\n",
    "       'NUM' : 'NUM',\n",
    "       'PART' : 'PART',\n",
    "       'PR' : 'ADP',\n",
    "       'S' : 'NOUN',\n",
    "       'SPRO' : 'PRON',\n",
    "       'UNKN' : 'X',\n",
    "       'V' : 'VERB',\n",
    "       'X' : 'X',\n",
    "      'PROPN' : 'PROPN'} #последних 2 тегов в майстеме нет, но они задаются в классе для слов в соответсвующими пометами\n",
    "\n",
    "# словарь, переводящий теги пайморфи в universal теги моделей\n",
    "pymorphy_tags = {'ADJF':'ADJ',\n",
    "    'ADJS' : 'ADJ',\n",
    "    'ADVB' : 'ADV',\n",
    "    'COMP' : 'ADV',\n",
    "    'GRND' : 'VERB',\n",
    "    'INFN' : 'VERB',\n",
    "    'NOUN' : 'NOUN',\n",
    "    'PRED' : 'ADV',\n",
    "    'PRTF' : 'ADJ',\n",
    "    'PRTS' : 'VERB',\n",
    "    'VERB' : 'VERB'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Пример вывода атрибутов объектов класса \\ntoken.num, token.text, token.lem, token.pos, token.complexity, token.is_complex(threshold, use_min):\\n\\n223 Кукушкин _NAMED_ENTITY_ S -19.06687873346149 False\\n224   _SPACE_ None -19.06687873346149 False\\n225 является являться V -7.502177090710664 False\\n226   _SPACE_ None -19.06687873346149 False\\n227 должником должник S -11.184186527172464 True\\n228   _SPACE_ None -19.06687873346149 False\\n229 банка банк S -9.300701131889967 False\\n230 .  _PUNKTUATION_ None -19.06687873346149 False\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Token():\n",
    "    def __init__(self, w):\n",
    "        \n",
    "        self.num = None # номер в тексте\n",
    "        self.complexity = None # сложность слова\n",
    "        self.av_similarity = None\n",
    "        \n",
    "        # три варианта инициализации: \n",
    "        ## из анализа текста, \n",
    "        ## из уже имеющегося объекта (для дочернего класса ComplexWord) \n",
    "        ## из строки\n",
    "        \n",
    "        if isinstance(w, dict): # если получили результат работы mystem\n",
    "            \n",
    "            self.text = w['text']  # сам токен\n",
    "            self.len = len(w['text']) # его длина\n",
    "            \n",
    "            # определяет, сделан ли анализ и, соответственно, рассматривать ли как слово, требующее упрощения\n",
    "            gram = w.get('analysis')\n",
    "            if gram:\n",
    "                self.lexem = gram[0]['lex']  # лемма\n",
    "                \n",
    "                if not self.named_entity(gram[0]):  # именованная сущность или нет\n",
    "                    self.pos = self.pos_tag(gram[0]['gr'])  # часть речи\n",
    "                else:\n",
    "                    self.pos = 'PROPN' # universal tag for named entity - у майстема таких нет\n",
    "                \n",
    "                    \n",
    "            elif any(p in w['text'] for p in punctuation+'–«»'): # если это знак пунктуации (может быть с пробелом!)\n",
    "                self.lexem = '_PUNKTUATION_'\n",
    "                self.pos = None\n",
    "            \n",
    "            elif not re.findall('\\S',w['text']): # если это только пробельные символы\n",
    "                self.lexem = '_SPACE_'\n",
    "                self.pos = None\n",
    "                \n",
    "            # остальное - неизвестная и ненужная ерунда?\n",
    "            else:\n",
    "                self.lexem = '_UNK_'\n",
    "                self.pos = 'X' # universal tag for unknown\n",
    "            \n",
    "            \n",
    "        elif isinstance(w, Token): # для определения объектов дочернего класса ComplexWord\n",
    "            self.text = w.text\n",
    "            self.num = w.num\n",
    "            self.lexem = w.lexem \n",
    "            self.len = w.len\n",
    "            self.pos = w.pos\n",
    "            self.complexity = w.complexity\n",
    "            \n",
    "        \n",
    "        elif isinstance(w, str): # если хотим как класс токен определить строку, полученную из словаря или модели\n",
    "            self.text = w\n",
    "            self.pos = None\n",
    "            self.lexem = w\n",
    "            self.len = len(w)\n",
    "            self.num = None\n",
    "            self.complexity = None\n",
    "            \n",
    "        \n",
    "    # вытаскивает часть речи из разбора майстем\n",
    "    def pos_tag(self,gram):\n",
    "        if ',' in gram:\n",
    "            gram = gram.split(',')[0]\n",
    "        if '=' in gram:\n",
    "            gram = gram.split('=')[0]\n",
    "        return gram\n",
    "        \n",
    "    # определяет по тегам, является ли именованной сущностью\n",
    "    def named_entity(self,gram):\n",
    "        markers = {'сокр': ' - сокращение', 'фам': ' - фамилия', 'имя': ' - имя собственное', 'гео': ' - название места', }\n",
    "        if any(m in gram['gr'] for m in markers.keys()):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def complexity_params(self, param = 'freq'):\n",
    "        # если по частотности\n",
    "        if param == 'freq':\n",
    "            self.complexity = unigrams.get(self.lexem, 0)\n",
    "            \n",
    "        # если по коэффициенту информативности. Отрицательное значение. Чем он меньше, тем сложнее\n",
    "        elif param == 'inf':\n",
    "            self.complexity = math.log((unigrams.get(self.lexem, 0)+1)/(sum(f for f in unigrams.values())+1))\n",
    "        \n",
    "    \n",
    "    def is_complex(self, threshold = '600', use_min = False, len_threshold = 1000):\n",
    "        exceptions = ['_PUNKTUATION_', '_SPACE_', '_UNK_']\n",
    "        # проверка, что это слово и что его нужно рассматривать как сложное (не нарицательное)\n",
    "        if not any(exception in self.lexem for exception in exceptions) and self.pos not in ['PROPN']:\n",
    "            \n",
    "            # если показатель сложности - вхождение в минимум\n",
    "            if use_min:\n",
    "                if self.lexem not in minimum:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "\n",
    "            # если показатель сложности - пороговое значение сложности\n",
    "            # также может использоваться длина. По умолчанию слишком большое - 1000 (т.е. этот параметр не учитывается)\n",
    "            else:\n",
    "                if self.complexity < float(threshold) or self.len > len_threshold:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def convert_universal(self):\n",
    "        if self.pos in mystem_tags:\n",
    "            self.pos = mystem_tags[self.pos]\n",
    "        else:\n",
    "            self.pos = 'X' # Х - universal тег для неизвестных слов\n",
    "        return self\n",
    "    \n",
    "    # нужна в двух случаях: для Замен, полученных из словарей, чтобы приписывать им соответствующий параметр,\n",
    "    # и для подсчета адекватности слова контексту\n",
    "    def cos_sim(self, context):\n",
    "        #print(self.lexem, target.lexem, self.pos, target.pos)\n",
    "        subst_query = str(self.lexem+'_'+self.pos)\n",
    "        target_query = str(context.lexem+'_'+context.pos)\n",
    "        if subst_query in model and target_query in model:\n",
    "            return model.similarity(subst_query, target_query)\n",
    "        else:\n",
    "            return 0.0\n",
    "    \n",
    "'''Пример вывода атрибутов объектов класса \n",
    "token.num, token.text, token.lem, token.pos, token.complexity, token.is_complex(threshold, use_min):\n",
    "\n",
    "223 Кукушкин _NAMED_ENTITY_ S -19.06687873346149 False\n",
    "224   _SPACE_ None -19.06687873346149 False\n",
    "225 является являться V -7.502177090710664 False\n",
    "226   _SPACE_ None -19.06687873346149 False\n",
    "227 должником должник S -11.184186527172464 True\n",
    "228   _SPACE_ None -19.06687873346149 False\n",
    "229 банка банк S -9.300701131889967 False\n",
    "230 .  _PUNKTUATION_ None -19.06687873346149 False\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''# вытаскивает часть речи из разбора майстем\n",
    "def pos(gram):\n",
    "    if ',' in gram:\n",
    "        gram = gram.split(',')[0]\n",
    "    if '=' in gram:\n",
    "        gram = gram.split('=')[0]\n",
    "    return gram\n",
    "'''\n",
    "\n",
    "#анализ текста \n",
    "def text_structuring(text, param, threshold, use_min):\n",
    "    # анализирует текст \n",
    "    analysis = m.analyze(text)\n",
    "    tokens = []\n",
    "    for i, w in enumerate(analysis): # состаляем список объектов Tokens\n",
    "        token = Token(w)\n",
    "        token.num = i # добавляем токену в атрибуты его номер в тексте\n",
    "        token.complexity_params(param) # переопределяем сложность на основе выбранного параметра\n",
    "        token.convert_universal() # превращаем POS в universal формат\n",
    "        print(token.num, token.text, token.lexem, token.pos, token.complexity, token.is_complex(threshold, use_min))\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ПАРАМЕТРЫ анализа слов\n",
    "complexity_type = 'inf'\n",
    "#global_threshold = -8.5\n",
    "global_threshold = np.median(list(inf_coef.keys()))\n",
    "use_min=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Бракованные бракованный ADJ -14.010632928113182 True\n",
      "1   _SPACE_ X -19.06687873346149 False\n",
      "2 белорусские белорусский ADJ -11.414807987345007 True\n",
      "3   _SPACE_ X -19.06687873346149 False\n",
      "4 лекарства лекарство NOUN -10.277066347270518 True\n",
      "5   _SPACE_ X -19.06687873346149 False\n",
      "6 изъяли изымать VERB -11.391796875745156 True\n",
      "7   _SPACE_ X -19.06687873346149 False\n",
      "8 из из ADP -5.430146815838169 False\n",
      "9   _SPACE_ X -19.06687873346149 False\n",
      "10 обращения обращение NOUN -9.762410698760608 True\n",
      "11 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "12 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "13 Лекарственные лекарственный ADJ -11.538009476819239 True\n",
      "14   _SPACE_ X -19.06687873346149 False\n",
      "15 препараты препарат NOUN -10.247213384120835 True\n",
      "16   _SPACE_ X -19.06687873346149 False\n",
      "17 из из ADP -5.430146815838169 False\n",
      "18   _SPACE_ X -19.06687873346149 False\n",
      "19 Белоруссии белоруссия PROPN -11.274942776523432 False\n",
      "20 ,  _PUNKTUATION_ X -19.06687873346149 False\n",
      "21 в в ADP -3.486155131943022 False\n",
      "22   _SPACE_ X -19.06687873346149 False\n",
      "23 которых который DET -5.3951392436881935 False\n",
      "24   _SPACE_ X -19.06687873346149 False\n",
      "25 нашли находить VERB -7.451604282941571 False\n",
      "26   _SPACE_ X -19.06687873346149 False\n",
      "27 осколки осколок NOUN -10.869615362047153 True\n",
      "28   _SPACE_ X -19.06687873346149 False\n",
      "29 стекла стекло NOUN -9.211531556253297 True\n",
      "30 ,  _PUNKTUATION_ X -19.06687873346149 False\n",
      "31 не не PART -4.031841462794542 False\n",
      "32   _SPACE_ X -19.06687873346149 False\n",
      "33 использовались использоваться VERB -9.97565979239317 True\n",
      "34   _SPACE_ X -19.06687873346149 False\n",
      "35 для для ADP -5.761858855363178 False\n",
      "36   _SPACE_ X -19.06687873346149 False\n",
      "37 инъекций инъекция NOUN -12.724757314740337 True\n",
      "38   _SPACE_ X -19.06687873346149 False\n",
      "39 пациентам пациент NOUN -10.707509627238819 True\n",
      "40   _SPACE_ X -19.06687873346149 False\n",
      "41 и и SCONJ -3.24763166905925 False\n",
      "42   _SPACE_ X -19.06687873346149 False\n",
      "43 были быть VERB -4.356829513848956 False\n",
      "44   _SPACE_ X -19.06687873346149 False\n",
      "45 выведены выводить VERB -9.570382453029572 True\n",
      "46   _SPACE_ X -19.06687873346149 False\n",
      "47 из из ADP -5.430146815838169 False\n",
      "48   _SPACE_ X -19.06687873346149 False\n",
      "49 обращения обращение NOUN -9.762410698760608 True\n",
      "50 .  _PUNKTUATION_ X -19.06687873346149 False\n",
      "51 Об об ADP -7.262754885511919 False\n",
      "52   _SPACE_ X -19.06687873346149 False\n",
      "53 этом это PRON -4.869477438777567 False\n",
      "54   _SPACE_ X -19.06687873346149 False\n",
      "55 сообщили сообщать VERB -8.634205145827439 False\n",
      "56   _SPACE_ X -19.06687873346149 False\n",
      "57 в в ADP -3.486155131943022 False\n",
      "58   _SPACE_ X -19.06687873346149 False\n",
      "59 пресс-службе пресс-служба NOUN -12.117981511148177 True\n",
      "60   _SPACE_ X -19.06687873346149 False\n",
      "61 Минздрава минздрав NOUN -12.792116712219551 True\n",
      "62   _SPACE_ X -19.06687873346149 False\n",
      "63 РФ рф PROPN -9.309573691103443 False\n",
      "64 . _PUNKTUATION_ X -19.06687873346149 False\n",
      "65 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "66 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "67 —  _UNK_ X -19.06687873346149 False\n",
      "68 Какой-либо какой-либо DET -9.574748614738363 True\n",
      "69   _SPACE_ X -19.06687873346149 False\n",
      "70 информации информация NOUN -8.870385802442234 False\n",
      "71   _SPACE_ X -19.06687873346149 False\n",
      "72 о о ADP -5.629415606889456 False\n",
      "73   _SPACE_ X -19.06687873346149 False\n",
      "74 введении введение NOUN -10.220813542768608 True\n",
      "75   _SPACE_ X -19.06687873346149 False\n",
      "76 данных данный ADJ -8.35450750188282 False\n",
      "77   _SPACE_ X -19.06687873346149 False\n",
      "78 препаратов препарат NOUN -10.247213384120835 True\n",
      "79   _SPACE_ X -19.06687873346149 False\n",
      "80 и и SCONJ -3.24763166905925 False\n",
      "81   _SPACE_ X -19.06687873346149 False\n",
      "82 возникших возникать VERB -8.524383446466064 False\n",
      "83   _SPACE_ X -19.06687873346149 False\n",
      "84 в в ADP -3.486155131943022 False\n",
      "85   _SPACE_ X -19.06687873346149 False\n",
      "86 связи связь NOUN -8.158650107867322 False\n",
      "87   _SPACE_ X -19.06687873346149 False\n",
      "88 с с ADP -4.45008658009755 False\n",
      "89   _SPACE_ X -19.06687873346149 False\n",
      "90 этим этот DET -5.5857855248461705 False\n",
      "91   _SPACE_ X -19.06687873346149 False\n",
      "92 осложнениях осложнение NOUN -11.743048167259172 True\n",
      "93   _SPACE_ X -19.06687873346149 False\n",
      "94 у у ADP -5.476072944878379 False\n",
      "95   _SPACE_ X -19.06687873346149 False\n",
      "96 пациентов пациент NOUN -10.707509627238819 True\n",
      "97   _SPACE_ X -19.06687873346149 False\n",
      "98 в в ADP -3.486155131943022 False\n",
      "99   _SPACE_ X -19.06687873346149 False\n",
      "100 министерство министерство NOUN -9.431989838900876 True\n",
      "101   _SPACE_ X -19.06687873346149 False\n",
      "102 не не PART -4.031841462794542 False\n",
      "103   _SPACE_ X -19.06687873346149 False\n",
      "104 поступало поступать VERB -8.77253222520656 False\n",
      "105 , —  _PUNKTUATION_ X -19.06687873346149 False\n",
      "106 отметили отмечать VERB -8.84723098704463 False\n",
      "107   _SPACE_ X -19.06687873346149 False\n",
      "108 в в ADP -3.486155131943022 False\n",
      "109   _SPACE_ X -19.06687873346149 False\n",
      "110 ведомстве ведомство NOUN -10.171249106325007 True\n",
      "111 . _PUNKTUATION_ X -19.06687873346149 False\n",
      "112 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "113 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "114 Также также ADV -7.603921125869184 False\n",
      "115   _SPACE_ X -19.06687873346149 False\n",
      "116 в в ADP -3.486155131943022 False\n",
      "117   _SPACE_ X -19.06687873346149 False\n",
      "118 Минздраве минздрав NOUN -12.792116712219551 True\n",
      "119   _SPACE_ X -19.06687873346149 False\n",
      "120 отметили отмечать VERB -8.84723098704463 False\n",
      "121 ,  _PUNKTUATION_ X -19.06687873346149 False\n",
      "122 что что SCONJ -4.319951223892165 False\n",
      "123   _SPACE_ X -19.06687873346149 False\n",
      "124 за за ADP -5.532348753209741 False\n",
      "125   _SPACE_ X -19.06687873346149 False\n",
      "126 два два NUM -6.59735779584951 False\n",
      "127   _SPACE_ X -19.06687873346149 False\n",
      "128 года год NOUN -5.9731535421957815 False\n",
      "129   _SPACE_ X -19.06687873346149 False\n",
      "130 были быть VERB -4.356829513848956 False\n",
      "131   _SPACE_ X -19.06687873346149 False\n",
      "132 забракованы забраковывать VERB -13.166981379878997 True\n",
      "133   _SPACE_ X -19.06687873346149 False\n",
      "134 две два NUM -6.59735779584951 False\n",
      "135   _SPACE_ X -19.06687873346149 False\n",
      "136 серии серия NOUN -10.13611999790322 True\n",
      "137   _SPACE_ X -19.06687873346149 False\n",
      "138 двух два NUM -6.59735779584951 False\n",
      "139   _SPACE_ X -19.06687873346149 False\n",
      "140 торговых торговый ADJ -9.816933106892947 True\n",
      "141   _SPACE_ X -19.06687873346149 False\n",
      "142 наименований наименование NOUN -11.075963270370165 True\n",
      "143   _SPACE_ X -19.06687873346149 False\n",
      "144 лекарственных лекарственный ADJ -11.538009476819239 True\n",
      "145   _SPACE_ X -19.06687873346149 False\n",
      "146 препаратов препарат NOUN -10.247213384120835 True\n",
      "147   _SPACE_ X -19.06687873346149 False\n",
      "148 производства производство NOUN -8.59864571516994 False\n",
      "149   _SPACE_ X -19.06687873346149 False\n",
      "150 РУП руп PROPN -14.804198856420173 False\n",
      "151   _SPACE_ X -19.06687873346149 False\n",
      "152 Белмедпрепараты белмедпрепарат NOUN -19.06687873346149 True\n",
      "153   _SPACE_ X -19.06687873346149 False\n",
      "154 и и SCONJ -3.24763166905925 False\n",
      "155   _SPACE_ X -19.06687873346149 False\n",
      "156 14 _UNK_ X -19.06687873346149 False\n",
      "157   _SPACE_ X -19.06687873346149 False\n",
      "158 серий серия NOUN -10.13611999790322 True\n",
      "159   _SPACE_ X -19.06687873346149 False\n",
      "160 семи семь NUM -9.24454700788637 True\n",
      "161   _SPACE_ X -19.06687873346149 False\n",
      "162 торговых торговый ADJ -9.816933106892947 True\n",
      "163   _SPACE_ X -19.06687873346149 False\n",
      "164 наименований наименование NOUN -11.075963270370165 True\n",
      "165   _SPACE_ X -19.06687873346149 False\n",
      "166 лекарственных лекарственный ADJ -11.538009476819239 True\n",
      "167   _SPACE_ X -19.06687873346149 False\n",
      "168 препаратов препарат NOUN -10.247213384120835 True\n",
      "169   _SPACE_ X -19.06687873346149 False\n",
      "170 производства производство NOUN -8.59864571516994 False\n",
      "171   _SPACE_ X -19.06687873346149 False\n",
      "172 ОАО оао NOUN -10.528511306813845 True\n",
      "173   _SPACE_ X -19.06687873346149 False\n",
      "174 Борисовский борисовский ADJ -14.432149745231854 True\n",
      "175   _SPACE_ X -19.06687873346149 False\n",
      "176 завод завод NOUN -8.703501020839123 False\n",
      "177   _SPACE_ X -19.06687873346149 False\n",
      "178 медицинских медицинский ADJ -9.828534165075709 True\n",
      "179   _SPACE_ X -19.06687873346149 False\n",
      "180 препаратов препарат NOUN -10.247213384120835 True\n",
      "181 . _PUNKTUATION_ X -19.06687873346149 False\n",
      "182 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "183 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "184 Ранее ранее ADV -9.877966308898927 True\n",
      "185   _SPACE_ X -19.06687873346149 False\n",
      "186 в в ADP -3.486155131943022 False\n",
      "187   _SPACE_ X -19.06687873346149 False\n",
      "188 СМИ сми PROPN -10.691940589626123 False\n",
      "189   _SPACE_ X -19.06687873346149 False\n",
      "190 появилась появляться VERB -8.062597938932536 False\n",
      "191   _SPACE_ X -19.06687873346149 False\n",
      "192 информация информация NOUN -8.870385802442234 False\n",
      "193   _SPACE_ X -19.06687873346149 False\n",
      "194 о о ADP -5.629415606889456 False\n",
      "195   _SPACE_ X -19.06687873346149 False\n",
      "196 том то PRON -5.111209789976896 False\n",
      "197 ,  _PUNKTUATION_ X -19.06687873346149 False\n",
      "198 что что SCONJ -4.319951223892165 False\n",
      "199   _SPACE_ X -19.06687873346149 False\n",
      "200 Росздравнадзор росздравнадзор NOUN -19.06687873346149 True\n",
      "201   _SPACE_ X -19.06687873346149 False\n",
      "202 по по ADP -5.1820044394556914 False\n",
      "203   _SPACE_ X -19.06687873346149 False\n",
      "204 итогам итог NOUN -9.545896884898616 True\n",
      "205   _SPACE_ X -19.06687873346149 False\n",
      "206 проверок проверка NOUN -10.048425523148962 True\n",
      "207   _SPACE_ X -19.06687873346149 False\n",
      "208 в в ADP -3.486155131943022 False\n",
      "209   _SPACE_ X -19.06687873346149 False\n",
      "210 Мордовии мордовия PROPN -13.68698137992103 False\n",
      "211 ,  _PUNKTUATION_ X -19.06687873346149 False\n",
      "212 Вологодской вологодский ADJ -12.322819547150143 True\n",
      "213   _SPACE_ X -19.06687873346149 False\n",
      "214 области область NOUN -8.188095962474803 False\n",
      "215   _SPACE_ X -19.06687873346149 False\n",
      "216 и и SCONJ -3.24763166905925 False\n",
      "217   _SPACE_ X -19.06687873346149 False\n",
      "218 Калининграде калининград PROPN -12.442813505661595 False\n",
      "219   _SPACE_ X -19.06687873346149 False\n",
      "220 отозвал отзывать VERB -11.963556670935377 True\n",
      "221   _SPACE_ X -19.06687873346149 False\n",
      "222 недоброкачественные недоброкачественный ADJ -14.191681410260339 True\n",
      "223   _SPACE_ X -19.06687873346149 False\n",
      "224 растворы раствор NOUN -9.911839677270397 True\n",
      "225   _SPACE_ X -19.06687873346149 False\n",
      "226 для для ADP -5.761858855363178 False\n",
      "227   _SPACE_ X -19.06687873346149 False\n",
      "228 инъекций инъекция NOUN -12.724757314740337 True\n",
      "229 ,  _PUNKTUATION_ X -19.06687873346149 False\n",
      "230 где где ADV -6.780378838818701 False\n",
      "231   _SPACE_ X -19.06687873346149 False\n",
      "232 якобы якобы PART -10.318256411144269 True\n",
      "233   _SPACE_ X -19.06687873346149 False\n",
      "234 обнаружены обнаруживать VERB -9.052834190360718 False\n",
      "235   _SPACE_ X -19.06687873346149 False\n",
      "236 стекло стекло NOUN -9.211531556253297 True\n",
      "237   _SPACE_ X -19.06687873346149 False\n",
      "238 и и SCONJ -3.24763166905925 False\n",
      "239   _SPACE_ X -19.06687873346149 False\n",
      "240 посторонние посторонний ADJ -10.16333529779677 True\n",
      "241   _SPACE_ X -19.06687873346149 False\n",
      "242 механические механический ADJ -10.420588968710842 True\n",
      "243   _SPACE_ X -19.06687873346149 False\n",
      "244 частицы частица NOUN -10.096192464926506 True\n",
      "245 . _PUNKTUATION_ X -19.06687873346149 False\n",
      "246 \n",
      " _SPACE_ X -19.06687873346149 False\n"
     ]
    }
   ],
   "source": [
    "text = reading('news2')\n",
    "tokens = text_structuring(text, complexity_type, global_threshold, use_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Substitution(Token):\n",
    "    def __init__(self, w):\n",
    "        super().__init__(w)\n",
    "        self.similarity = None\n",
    "        self.fitness = None\n",
    "        self.closeness = None\n",
    "    \n",
    "    # для слов из словаря и тезауруса: определяем тег пайморфи, переводим в формат universal - так быстрее, чем майстемом\n",
    "    def tagging(self, w):\n",
    "        tag = morph.parse(w)[0].tag.POS\n",
    "        if tag in pymorphy_tags:\n",
    "            return pymorphy_tags[tag]\n",
    "        else:\n",
    "            return 'X' # Х - universal тег для неизвестных слов\n",
    "    \n",
    "    # приписываем недостающие атрибуты словам, взятым из словаря или тезауруса\n",
    "    def setting_atr(self, target):\n",
    "        self.pos = self.tagging(self.lexem)\n",
    "        self.complexity_params(complexity_type)\n",
    "        self.similarity = self.cos_sim(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Complex_word(Token):\n",
    "    def __init__(self, w):\n",
    "        super().__init__(w)\n",
    "        self.substituts = None\n",
    "        self.place = None\n",
    "        self.context = None\n",
    "        self.easier = []\n",
    "        \n",
    "    # список замен, в зависимости от выбранной базы\n",
    "    def search_substituts(self, base_type='model'):\n",
    "        \n",
    "        # поиск по модели\n",
    "        def model_search(lexem, pos):\n",
    "            query = str(lexem+'_'+pos)\n",
    "            #print(query)\n",
    "            if query in model:\n",
    "                # формируем список квазисинонимов той же части речи\n",
    "                # при этом превращаем их в объекты соответствующего класса\n",
    "                syn_tokens = []\n",
    "                for syn, sim in model.most_similar(positive=[query]):\n",
    "                    syn_text = syn[:syn.find('_')] # текст до части речи\n",
    "                    \n",
    "                    syn_tok = Substitution(syn_text) # из текстовой строки инициализируем объект класса\n",
    "                    syn_tok.pos = syn[syn_tok.len+1:] # часть речи\n",
    "                    \n",
    "                    syn_tok.complexity_params(complexity_type) # сложность по функции в зависимости от выбранного параметра\n",
    "                    \n",
    "                    syn_tok.similarity = sim # а близость по параметру модели\n",
    "                    \n",
    "                    syn_tokens.append(syn_tok)\n",
    "                    \n",
    "                return syn_tokens\n",
    "            else:\n",
    "                return [] \n",
    "\n",
    "        # поиск по YARN\n",
    "        def yarn_search(target, filepath = 'yarn-synsets1.csv'):\n",
    "            with open(filepath, \"r\", newline=\"\") as file: # постепенный просмотр файла с синсетами (множествами синонимов)\n",
    "                reader = csv.DictReader(file, delimiter=';')\n",
    "                lst = []\n",
    "                for i,row in enumerate(reader):\n",
    "                    cur_line = row['words'].split(';') # считываем колонку с синсетами\n",
    "                    if len(cur_line)>1:\n",
    "                        if target.lexem in cur_line:\n",
    "                            del(reader)\n",
    "                            for c in cur_line:\n",
    "                                if ' ' not in c and c!=target.lexem: # формируем список однословных синонимов\n",
    "                                    sub_tok = Substitution(c)\n",
    "                                    \n",
    "                                    sub_tok.setting_atr(target)\n",
    "                                    \n",
    "                                    if sub_tok not in lst:\n",
    "                                        lst.append(sub_tok) \n",
    "                            #TODO: выделить неоднословные в отдельный класс и поискать их частотность по n-граммам?\n",
    "                            break\n",
    "                #print(lst)\n",
    "                return lst \n",
    "        \n",
    "        #поиск по ASIS\n",
    "        def asis_search(target):\n",
    "            if target.lexem in asis:\n",
    "                lst = []\n",
    "                for s in asis[target.lexem]:\n",
    "                    if ' ' not in s: # формируем список однословных синонимов\n",
    "                        sub_tok = Substitution(s)\n",
    "                        sub_tok.setting_atr(target)\n",
    "                        lst.append(sub_tok)\n",
    "                return lst\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        \n",
    "        if base_type == 'model':\n",
    "            self.substituts = model_search(self.lexem, self.pos)\n",
    "            \n",
    "        if base_type == 'yarn':\n",
    "            self.substituts = yarn_search(self)\n",
    "        \n",
    "        if base_type == 'asis':\n",
    "            self.substituts = asis_search(self)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def find_easier(self, use_min = False, threshold = global_threshold):\n",
    "        for sub in self.substituts:\n",
    "            if not sub.is_complex(threshold = threshold, use_min = use_min):\n",
    "                if sub.complexity > self.complexity:\n",
    "                    self.easier.append(sub)\n",
    "        return self\n",
    "    \n",
    "    def make_window(self, tokens, window = 10):\n",
    "        context = [self]\n",
    "        left_ind = self.num-1\n",
    "        right_ind = self.num+1\n",
    "        ind = 0\n",
    "        # добавляем по одному слову слева и/или справа, пока не наберется window + само слово\n",
    "        while len(context)<window+1:\n",
    "            while left_ind >= 0:\n",
    "                left_w = tokens[left_ind]\n",
    "                left_ind-=1\n",
    "                # проверка, что это слово\n",
    "                if any(exception in left_w.lexem for exception in ['_PUNKTUATION_', '_SPACE_', '_UNK_']):\n",
    "                    continue\n",
    "                else:\n",
    "                    context[:0] = [left_w] #вставляем слово слева от цепочки\n",
    "                    ind+=1 # индекс слова сдвигается\n",
    "                    break\n",
    "\n",
    "            while right_ind < len(tokens):\n",
    "                right_w = tokens[right_ind]\n",
    "                right_ind+=1\n",
    "                # проверка, что это слово и что его нужно рассматривать как сложное (не нарицательное)\n",
    "                if any(exception in right_w.lexem for exception in ['_PUNKTUATION_', '_SPACE_', '_UNK_']):\n",
    "                    continue\n",
    "                else:\n",
    "                    context.append(right_w) # справа от цепочки\n",
    "                    break\n",
    "        self.place = ind\n",
    "        self.context = context\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "изымать VERB yarn : брать VERB -8.611951584274115 0.10304389896830814\n",
      "пациент NOUN yarn : больной ADJ -8.563840649103 0.3274560641921585\n",
      "выводить VERB yarn : открывать VERB -8.24556163363519 0.2906104975356157\n",
      "осложнение NOUN yarn : проблема NOUN -8.221393716707802 0.18532289510563849\n",
      "осложнение NOUN yarn : задача NOUN -8.337681827282445 0.016228775087925692\n",
      "пациент NOUN yarn : больной ADJ -8.563840649103 0.3274560641921585\n"
     ]
    }
   ],
   "source": [
    "# Отбор сложных слов, превращение их в подкласс сложных слов и поиск замен\n",
    "# несложные слова также остаются на своих местах\n",
    "def selecting_complex(tokens, base_type=['yarn','model','asis'][0], threshold = global_threshold, use_min = False):\n",
    "    complex_words = []\n",
    "    for token in tokens:\n",
    "        if token.is_complex(threshold, use_min):\n",
    "            comp_token = Complex_word(token) # токен становится Сложным словом\n",
    "            \n",
    "            comp_token.search_substituts(base_type=base_type)\n",
    "            \n",
    "            complex_words.append(comp_token)\n",
    "            \n",
    "            # код для принтов\n",
    "            \n",
    "            '''\n",
    "            if comp_token.substituts:\n",
    "                for syn in comp_token.substituts:\n",
    "                    print (comp_token.lexem, comp_token.complexity, base_type, ':', syn.lexem, syn.complexity, syn.similarity)\n",
    "            '''\n",
    "            comp_token.find_easier(threshold, use_min)\n",
    "            if comp_token.easier:\n",
    "                for syn in comp_token.easier:\n",
    "                    print (comp_token.lexem, comp_token.pos, base_type, ':', syn.lexem, syn.pos, syn.complexity, syn.similarity)\n",
    "            \n",
    "        else:\n",
    "            complex_words.append(token)\n",
    "    return complex_words\n",
    "\n",
    "complex_words = selecting_complex(tokens, 'yarn', threshold = global_threshold, use_min = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### При use_min = True в основном возвращает пустоту - очень мало там слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### теперь, когда для каждого сложного слова есть список его синонимов с нужными параметрами, можно делать модель на н-граммах\n",
    "### контекстное окно: 2n+1 слов (по n слева и справа), можно задавать. Вероятность на основе логарифма + Лапласса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngrams_dict = {0: {}, 1: unigrams, 2: bigrams, 3: trigrams}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вообще должно быть не деление на V, а деление на объем корпуса - количество вхождений слов. Те подсчитать сумму значений частот униграмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram_prob(ngram, n, use_log= False):\n",
    "    c1 = ' '.join(ngram[-n:]) # в числителе частота строки длины n\n",
    "    c2 = ' '.join(ngram[-n:-1]) # в знаменателе - строки без последнего символа\n",
    "    d = ngrams_dict.get(n) # для поиска числителя берем словарь n-грамм\n",
    "    d2 = ngrams_dict.get(n-1) # для поиска знаменателя - словарь n-1-грамм\n",
    "    V = len(d)\n",
    "    #len(ngrams_dict.get(n-1,len(unigrams))) # сглаживание лапласса: добавляем размер словаря знаменателя\n",
    "    p1 = d.get(c1,0)+1\n",
    "    p2 = d2.get(c2,0)+V\n",
    "    \n",
    "    result =  p1/p2\n",
    "    \n",
    "    print('\\t',c1, '/', c2, ':' , p1-1, '/', p2-V, '(', p1, '/', p2, ')', '=', result)\n",
    "    \n",
    "    if use_log:\n",
    "        return math.log(p1)-math.log(p2)\n",
    "    else:\n",
    "        return p1/p2\n",
    "    \n",
    "def context_prob(ngrams, use_log= False):\n",
    "    if use_log:\n",
    "        p_context = 0.0\n",
    "    else:\n",
    "        p_context = 1.0\n",
    "    for ngram in ngrams:\n",
    "        p = ngram_prob(ngram, 3, use_log= use_log)\n",
    "        #print(ngram, p)\n",
    "        if use_log:\n",
    "            p_context+=p\n",
    "        else:\n",
    "            p_context*=p\n",
    "    return p_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>лемма</th>\n",
       "      <th>индекс</th>\n",
       "      <th>контекст</th>\n",
       "      <th>информативность</th>\n",
       "      <th>вероятность</th>\n",
       "      <th>близость</th>\n",
       "      <th>словарность</th>\n",
       "      <th>класс</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [лемма, индекс, контекст, информативность, вероятность, близость, словарность, класс]\n",
       "Index: []"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['лемма','индекс','контекст','информативность','вероятность','близость','словарность','класс'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def making_frame(complex_words, window=5, n=3, algorithm = ['ngram','random_three'][0]):\n",
    "    def ngram_algorithm():\n",
    "        fit='НЕТ'\n",
    "        if p_changed_context>p_context:\n",
    "            fit='ДА'\n",
    "            if sub.fitness > best_fitness:\n",
    "                return sub\n",
    "            elif sub.fitness == best_fitness:\n",
    "                if best_sub.complexity < sub.complexity:\n",
    "                    return sub\n",
    "                else:\n",
    "                    return best_sub\n",
    "            else:\n",
    "                return best_sub\n",
    "                    \n",
    "    for token in complex_words:\n",
    "        if isinstance(token, Complex_word):\n",
    "            if token.easier: \n",
    "                cls = [[0]]*len(token.easier) # для таблички - классы\n",
    "                in_min = [] # для таблички - словарность - в минимуме\n",
    "                easier_lexems = [t.lexem for t in token.easier] # для таблички - лексемы замен\n",
    "                easier_inf = [t.complexity for t in token.easier] # для таблички - сложность замен\n",
    "                \n",
    "                # окна контекста строим для тех сложных слов, для которых есть варианты замен проще, чем исходное\n",
    "                token.make_window(complex_words, window = window) # для подсчетов\n",
    "                easier_places = [[token.place]]*len(token.easier) # для таблички - место исходного\n",
    "                \n",
    "                context_lem = [c.lexem for c in token.context] # для подсчета вероятностей нграмм\n",
    "                context_text = [c.text.lower() for c in token.context] # для таблички - контекст\n",
    "                \n",
    "                # определение, насколько слово близко с другими словами из контекста\n",
    "                # cos_sim возвращает 0.0, если пара слов не найдена в модели\n",
    "                closeness = np.mean([token.cos_sim(w) for w in token.context if w.lexem!=token.lexem])\n",
    "                \n",
    "                text_ngrams = [g for g in ngrams(context_lem, n)] # генерация n-грамм\n",
    "                p_context = context_prob(text_ngrams, use_log=True) # вероятность контекста\n",
    "                \n",
    "                best_fitness = -1000  # \n",
    "                best_sub = None\n",
    "                easier_prob = [] # для таблички - контексная вероятность\n",
    "                easier_similarity = [] # для таблички - контексная близость\n",
    "                for sub in token.easier:\n",
    "                    # контекст с новым словом\n",
    "                    sub_context = token.context[:token.place]+[sub]+token.context[token.place+1:]\n",
    "                    # все то же самое\n",
    "                    sub.closeness = np.mean([sub.cos_sim(w) for w in sub_context if w.lexem!=sub.lexem])\n",
    "                    easier_similarity.append(sub.closeness) # для таблички - контексная близость\n",
    "                    \n",
    "                    sub_context_lem = [t.lexem for t in sub_context]\n",
    "                    sub_ngrams = [g for g in ngrams(sub_context_lem, n)]\n",
    "                    p_changed_context = context_prob(sub_ngrams, use_log=True)\n",
    "                    sub.fitness = p_changed_context\n",
    "                    \n",
    "                    easier_prob.append(p_changed_context) # для таблички - контексная вероятность\n",
    "                    \n",
    "                    if sub in minimum:\n",
    "                        in_min.append(1)\n",
    "                    else:\n",
    "                        in_min.append(0)\n",
    "                        \n",
    "                    if algorithm == 'ngram':\n",
    "                        best_sub = ngram_algorithm()\n",
    "                        best_fitness = best_sub.fitness\n",
    "                \n",
    "                if algorithm == 'ngram' and best_sub:\n",
    "                    for i,sub in enumerate(token.easier):\n",
    "                        if sub.lexem == best_sub.lexem:\n",
    "                            cls[i]=1\n",
    "                        else:\n",
    "                            cls[i]=0\n",
    "                    #print('Лучшее значение вероятности: {0}\\n'.format(best_sub.text))\n",
    "                new_lines = np.reshape(np.array([easier_lexems, easier_places, context_text, easier_inf, easier_prob, easier_similarity, in_min, cls]),(8,len(token.easier)))\n",
    "                print(new_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t бракованный белорусский лекарство / бракованный белорусский : 0 / 0 ( 1 / 2876543 ) = 3.476395103427969e-07\n",
      "\t белорусский лекарство изымать / белорусский лекарство : 0 / 0 ( 1 / 2876543 ) = 3.476395103427969e-07\n",
      "\t лекарство изымать из / лекарство изымать : 0 / 0 ( 1 / 2876543 ) = 3.476395103427969e-07\n",
      "\t изымать из обращение / изымать из : 26 / 450 ( 27 / 2876993 ) = 9.38479864219343e-06\n",
      "\t из обращение лекарственный / из обращение : 0 / 158 ( 1 / 2876701 ) = 3.4762041658135483e-07\n",
      "\t бракованный белорусский лекарство / бракованный белорусский : 0 / 0 ( 1 / 2876543 ) = 3.476395103427969e-07\n",
      "\t белорусский лекарство брать / белорусский лекарство : 0 / 0 ( 1 / 2876543 ) = 3.476395103427969e-07\n",
      "\t лекарство брать из / лекарство брать : 0 / 0 ( 1 / 2876543 ) = 3.476395103427969e-07\n",
      "\t брать из обращение / брать из : 0 / 315 ( 1 / 2876858 ) = 3.476014457439331e-07\n",
      "\t из обращение лекарственный / из обращение : 0 / 158 ( 1 / 2876701 ) = 3.4762041658135483e-07\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'fitness'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-d62f926e11a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmaking_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomplex_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-98-3eb7f65b8f7e>\u001b[0m in \u001b[0;36mmaking_frame\u001b[1;34m(complex_words, window, n, algorithm)\u001b[0m\n\u001b[0;32m     61\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'ngram'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                         \u001b[0mbest_sub\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngram_algorithm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                         \u001b[0mbest_fitness\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_sub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'ngram'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbest_sub\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'fitness'"
     ]
    }
   ],
   "source": [
    "making_frame(complex_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "изъяли 3 : ['бракованный', 'белорусский', 'лекарство', 'изымать', 'из', 'обращение', 'лекарственный']\n",
      "[('бракованный', 'белорусский', 'лекарство'), ('белорусский', 'лекарство', 'изымать'), ('лекарство', 'изымать', 'из'), ('изымать', 'из', 'обращение'), ('из', 'обращение', 'лекарственный')]\n",
      "\t бракованный белорусский лекарство / бракованный белорусский : 0 / 0 ( 1 / 2876543 ) = 3.476395103427969e-07\n",
      "\t белорусский лекарство изымать / белорусский лекарство : 0 / 0 ( 1 / 2876543 ) = 3.476395103427969e-07\n",
      "\t лекарство изымать из / лекарство изымать : 0 / 0 ( 1 / 2876543 ) = 3.476395103427969e-07\n",
      "\t изымать из обращение / изымать из : 26 / 450 ( 27 / 2876993 ) = 9.38479864219343e-06\n",
      "\t из обращение лекарственный / из обращение : 0 / 158 ( 1 / 2876701 ) = 3.4762041658135483e-07\n",
      "\n",
      "Вероятность слова _изъяли_ в контексте ['бракованный', 'белорусский', 'лекарство', 'изымать', 'из', 'обращение', 'лекарственный'] = -71.0648734045552, близость к контексту = 0.06318795363628794\n",
      "\n",
      "\t бракованный белорусский лекарство / бракованный белорусский : 0 / 0 ( 1 / 2876543 ) = 3.476395103427969e-07\n",
      "\t белорусский лекарство брать / белорусский лекарство : 0 / 0 ( 1 / 2876543 ) = 3.476395103427969e-07\n",
      "\t лекарство брать из / лекарство брать : 0 / 0 ( 1 / 2876543 ) = 3.476395103427969e-07\n",
      "\t брать из обращение / брать из : 0 / 315 ( 1 / 2876858 ) = 3.476014457439331e-07\n",
      "\t из обращение лекарственный / из обращение : 0 / 158 ( 1 / 2876701 ) = 3.4762041658135483e-07\n",
      "\n",
      "Вероятность замены _брать_ в контексте ['бракованный', 'белорусский', 'лекарство', 'брать', 'из', 'обращение', 'лекарственный'] = -74.36066334546535, разница = 3.2957899409101543, используем? - НЕТ\n",
      "Близость к контексту = 0.006395086993213012, разница с исходным = 0.05679286664307493\n",
      "\n",
      "пациентам 3 : ['использоваться', 'для', 'инъекция', 'пациент', 'и', 'быть', 'выводить']\n",
      "[('использоваться', 'для', 'инъекция'), ('для', 'инъекция', 'пациент'), ('инъекция', 'пациент', 'и'), ('пациент', 'и', 'быть'), ('и', 'быть', 'выводить')]\n",
      "\t использоваться для инъекция / использоваться для : 0 / 1017 ( 1 / 2877560 ) = 3.475166460473457e-07\n",
      "\t для инъекция пациент / для инъекция : 0 / 110 ( 1 / 2876653 ) = 3.4762621699593243e-07\n",
      "\t инъекция пациент и / инъекция пациент : 0 / 0 ( 1 / 2876543 ) = 3.476395103427969e-07\n",
      "\t пациент и быть / пациент и : 0 / 128 ( 1 / 2876671 ) = 3.4762404181778176e-07\n",
      "\t и быть выводить / и быть : 8 / 3 ( 9 / 2876546 ) = 3.128752330051388e-06\n",
      "\n",
      "Вероятность слова _пациентам_ в контексте ['использоваться', 'для', 'инъекция', 'пациент', 'и', 'быть', 'выводить'] = -72.16371160844305, близость к контексту = 0.018838279596461813\n",
      "\n",
      "\t использоваться для инъекция / использоваться для : 0 / 1017 ( 1 / 2877560 ) = 3.475166460473457e-07\n",
      "\t для инъекция больной / для инъекция : 0 / 110 ( 1 / 2876653 ) = 3.4762621699593243e-07\n",
      "\t инъекция больной и / инъекция больной : 0 / 0 ( 1 / 2876543 ) = 3.476395103427969e-07\n",
      "\t больной и быть / больной и : 0 / 505 ( 1 / 2877048 ) = 3.4757849017465123e-07\n",
      "\t и быть выводить / и быть : 8 / 3 ( 9 / 2876546 ) = 3.128752330051388e-06\n",
      "\n",
      "Вероятность замены _больной_ в контексте ['использоваться', 'для', 'инъекция', 'больной', 'и', 'быть', 'выводить'] = -72.16384265411995, разница = 0.00013104567689481428, используем? - НЕТ\n",
      "Близость к контексту = 0.03575225068896881, разница с исходным = -0.016913971092507\n",
      "\n",
      "выведены 3 : ['пациент', 'и', 'быть', 'выводить', 'из', 'обращение', 'об']\n",
      "[('пациент', 'и', 'быть'), ('и', 'быть', 'выводить'), ('быть', 'выводить', 'из'), ('выводить', 'из', 'обращение'), ('из', 'обращение', 'об')]\n",
      "\t пациент и быть / пациент и : 0 / 128 ( 1 / 2876671 ) = 3.4762404181778176e-07\n",
      "\t и быть выводить / и быть : 8 / 3 ( 9 / 2876546 ) = 3.128752330051388e-06\n",
      "\t быть выводить из / быть выводить : 176 / 560 ( 177 / 2877103 ) = 6.152021669019149e-05\n",
      "\t выводить из обращение / выводить из : 0 / 2088 ( 1 / 2878631 ) = 3.473873518349521e-07\n",
      "\t из обращение об / из обращение : 0 / 158 ( 1 / 2876701 ) = 3.4762041658135483e-07\n",
      "\n",
      "Вероятность слова _выведены_ в контексте ['пациент', 'и', 'быть', 'выводить', 'из', 'обращение', 'об'] = -66.98814534204917, близость к контексту = -0.031232112809915818\n",
      "\n",
      "\t пациент и быть / пациент и : 0 / 128 ( 1 / 2876671 ) = 3.4762404181778176e-07\n",
      "\t и быть открывать / и быть : 24 / 3 ( 25 / 2876546 ) = 8.690978694587188e-06\n",
      "\t быть открывать из / быть открывать : 0 / 13 ( 1 / 2876556 ) = 3.4763793925791814e-07\n",
      "\t открывать из обращение / открывать из : 0 / 12 ( 1 / 2876555 ) = 3.47638060110097e-07\n",
      "\t из обращение об / из обращение : 0 / 158 ( 1 / 2876701 ) = 3.4762041658135483e-07\n",
      "\n",
      "Вероятность замены _открывать_ в контексте ['пациент', 'и', 'быть', 'открывать', 'из', 'обращение', 'об'] = -71.14173225090096, разница = 4.153586908851793, используем? - НЕТ\n",
      "Близость к контексту = -0.020023252691917192, разница с исходным = -0.011208860117998626\n",
      "\n",
      "осложнениях 3 : ['связь', 'с', 'этот', 'осложнение', 'у', 'пациент', 'в']\n",
      "[('связь', 'с', 'этот'), ('с', 'этот', 'осложнение'), ('этот', 'осложнение', 'у'), ('осложнение', 'у', 'пациент'), ('у', 'пациент', 'в')]\n",
      "\t связь с этот / связь с : 196 / 4 ( 197 / 2876547 ) = 6.848488830531885e-05\n",
      "\t с этот осложнение / с этот : 0 / 10 ( 1 / 2876553 ) = 3.476383018147067e-07\n",
      "\t этот осложнение у / этот осложнение : 0 / 9 ( 1 / 2876552 ) = 3.476384226671376e-07\n",
      "\t осложнение у пациент / осложнение у : 0 / 4 ( 1 / 2876547 ) = 3.476390269305525e-07\n",
      "\t у пациент в / у пациент : 3 / 203 ( 4 / 2876746 ) = 1.3904599154739416e-06\n",
      "\n",
      "Вероятность слова _осложнениях_ в контексте ['связь', 'с', 'этот', 'осложнение', 'у', 'пациент', 'в'] = -67.69108078420743, близость к контексту = 0.051135679217402336\n",
      "\n",
      "\t связь с этот / связь с : 196 / 4 ( 197 / 2876547 ) = 6.848488830531885e-05\n",
      "\t с этот проблема / с этот : 131 / 10 ( 132 / 2876553 ) = 4.588825583954128e-05\n",
      "\t этот проблема у / этот проблема : 4 / 3244 ( 5 / 2879787 ) = 1.7362395204923142e-06\n",
      "\t проблема у пациент / проблема у : 0 / 4 ( 1 / 2876547 ) = 3.476390269305525e-07\n",
      "\t у пациент в / у пациент : 3 / 203 ( 4 / 2876746 ) = 1.3904599154739416e-06\n",
      "\n",
      "Вероятность замены _проблема_ в контексте ['связь', 'с', 'этот', 'проблема', 'у', 'пациент', 'в'] = -61.199964927583835, разница = -6.491115856623594, используем? - ДА\n",
      "Близость к контексту = 0.022344491239161508, разница с исходным = 0.028791187978240828\n",
      "\n",
      "\t связь с этот / связь с : 196 / 4 ( 197 / 2876547 ) = 6.848488830531885e-05\n",
      "\t с этот задача / с этот : 171 / 10 ( 172 / 2876553 ) = 5.979378791212955e-05\n",
      "\t этот задача у / этот задача : 0 / 2423 ( 1 / 2878966 ) = 3.4734692941840926e-07\n",
      "\t задача у пациент / задача у : 0 / 88 ( 1 / 2876631 ) = 3.4762887558397304e-07\n",
      "\t у пациент в / у пациент : 3 / 203 ( 4 / 2876746 ) = 1.3904599154739416e-06\n",
      "\n",
      "Вероятность замены _задача_ в контексте ['связь', 'с', 'этот', 'задача', 'у', 'пациент', 'в'] = -62.54445435586746, разница = -5.146626428339971, используем? - ДА\n",
      "Близость к контексту = -0.0017856899107119469, разница с исходным = 0.05292136912811428\n",
      "\n",
      "Лучшее значение вероятности: проблема\n",
      "\n",
      "пациентов 3 : ['этот', 'осложнение', 'у', 'пациент', 'в', 'министерство', 'не']\n",
      "[('этот', 'осложнение', 'у'), ('осложнение', 'у', 'пациент'), ('у', 'пациент', 'в'), ('пациент', 'в', 'министерство'), ('в', 'министерство', 'не')]\n",
      "\t этот осложнение у / этот осложнение : 0 / 9 ( 1 / 2876552 ) = 3.476384226671376e-07\n",
      "\t осложнение у пациент / осложнение у : 0 / 4 ( 1 / 2876547 ) = 3.476390269305525e-07\n",
      "\t у пациент в / у пациент : 3 / 203 ( 4 / 2876746 ) = 1.3904599154739416e-06\n",
      "\t пациент в министерство / пациент в : 0 / 13 ( 1 / 2876556 ) = 3.4763793925791814e-07\n",
      "\t в министерство не / в министерство : 8 / 1920 ( 9 / 2878463 ) = 3.1266686422580385e-06\n",
      "\n",
      "Вероятность слова _пациентов_ в контексте ['этот', 'осложнение', 'у', 'пациент', 'в', 'министерство', 'не'] = -70.77772683316874, близость к контексту = 0.035363689602233184\n",
      "\n",
      "\t этот осложнение у / этот осложнение : 0 / 9 ( 1 / 2876552 ) = 3.476384226671376e-07\n",
      "\t осложнение у больной / осложнение у : 0 / 4 ( 1 / 2876547 ) = 3.476390269305525e-07\n",
      "\t у больной в / у больной : 5 / 1124 ( 6 / 2877667 ) = 2.085022346226996e-06\n",
      "\t больной в министерство / больной в : 0 / 4 ( 1 / 2876547 ) = 3.476390269305525e-07\n",
      "\t в министерство не / в министерство : 8 / 1920 ( 9 / 2878463 ) = 3.1266686422580385e-06\n",
      "\n",
      "Вероятность замены _больной_ в контексте ['этот', 'осложнение', 'у', 'больной', 'в', 'министерство', 'не'] = -70.3725786984716, разница = -0.40514813469714284, используем? - ДА\n",
      "Близость к контексту = 0.034535737622063294, разница с исходным = 0.0008279519801698901\n",
      "\n",
      "Лучшее значение вероятности: больной\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in complex_words:\n",
    "    if isinstance(token, Complex_word):\n",
    "        if token.easier: \n",
    "            # окна контекста строим для тех сложных слов, для которых есть варианты замен, которые проще, чем слово\n",
    "            token.make_window(complex_words, window = 5)\n",
    "            \n",
    "            context_lem = [c.lexem for c in token.context]\n",
    "            \n",
    "            # определение, насколько слово близко с другими словами из контекста\n",
    "            closeness = np.mean([token.cos_sim(w) for w in token.context if w.lexem!=token.lexem])\n",
    "            \n",
    "            print(token.text, token.place, ':', context_lem)\n",
    "            \n",
    "            # генерация n-грамм\n",
    "            text_3grams = [n for n in ngrams(context_lem, 3)]\n",
    "            \n",
    "            print(text_3grams)\n",
    "            \n",
    "            p_context = context_prob(text_3grams, use_log=True)\n",
    "                \n",
    "            print('\\nВероятность слова _{0}_ в контексте {1} = {2}, близость к контексту = {3}\\n'.format(token.text, context_lem, p_context,closeness))\n",
    "            \n",
    "            best_fitness = -1000\n",
    "            best_sub = None\n",
    "            \n",
    "            for sub in token.easier:\n",
    "                \n",
    "                # контекст с новым словом\n",
    "                sub_context = token.context[:token.place]+[sub]+token.context[token.place+1:]\n",
    "                \n",
    "                # определение, насколько слово близко с другими словами из контекста\n",
    "                sub.closeness = np.mean([sub.cos_sim(w) for w in sub_context if w.lexem!=sub.lexem])\n",
    "            \n",
    "                sub_context_lem = [t.lexem for t in sub_context]\n",
    "                \n",
    "                sub_3grams = [n for n in ngrams(sub_context_lem, 3)]\n",
    "                \n",
    "                p_changed_context = context_prob(sub_3grams, use_log=True)\n",
    "                \n",
    "                fit='НЕТ'\n",
    "                if p_changed_context>p_context:\n",
    "                    fit='ДА'\n",
    "                    sub.fitness = p_changed_context\n",
    "                    if sub.fitness > best_fitness:\n",
    "                        best_fitness = sub.fitness\n",
    "                        best_sub = sub\n",
    "                    elif sub.fitness == best_fitness:\n",
    "                        if best_sub.complexity < sub.complexity:\n",
    "                            best_fitness = sub.fitness\n",
    "                            best_sub = sub\n",
    "                        \n",
    "                print('\\nВероятность замены _{0}_ в контексте {1} = {2}, разница = {3}, используем? - {4}'.format(sub.text, sub_context_lem, p_changed_context, p_context-p_changed_context, fit))\n",
    "                print('Близость к контексту = {0}, разница с исходным = {1}\\n'.format(sub.closeness, closeness-sub.closeness))\n",
    "            if best_sub:\n",
    "                print('Лучшее значение вероятности: {0}\\n'.format(best_sub.text))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "поправить вычисление вероятности - в знаменателе число уникальных нграмм, а не униграмм, ну и переписать с условием n>/=1\n",
    "изменить вычисление вероятности для логарифма\n",
    "Написать перплексию для получившейся модели - хз как????  \n",
    "поправить нахождение best_sub - если их больше 1, надо как-то это обрабатывать  \n",
    "прогнать статистическую модель на текстах и сформировать таким образом корпус положительных замен, которые потом можно подредачить ручками   \n",
    "заодно при редактировании ручками можно создать еще один файл, в котором редачить, а потом сравнить с первым, чтобы посчитать точность и проч  \n",
    "потом все-таки посчитать "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perplexity(docs):\n",
    "    docs = np.asarray(docs)\n",
    "    out_of_dict = []\n",
    "   \n",
    "    tmp_sum_docs = 0.0\n",
    "    N = 0.0\n",
    "    for doc in log_progress(docs):\n",
    "        tmp_sum_words = 0.0\n",
    "        for word in log_progress(words):\n",
    "            freq = frequencies.get(word, TOL)\n",
    "            freq_empirical = frequencies_empirical.get(word, 0.0)\n",
    "            N += freq_empirical\n",
    "            if freq == TOL:\n",
    "                out_of_dict.append(word)\n",
    "            tmp_sum_words += freq_empirical * np.log(freq)\n",
    "        tmp_sum_docs += tmp_sum_words\n",
    "       \n",
    "    return np.exp(-tmp_sum_docs / N), out_of_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

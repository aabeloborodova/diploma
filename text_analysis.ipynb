{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aleksandra\\AppData\\Roaming\\Python\\Python36\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# импорты\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "from string import punctuation\n",
    "import math\n",
    "from tqdm import tqdm_notebook\n",
    "import csv\n",
    "import gensim\n",
    "\n",
    "import pymystem3\n",
    "m = pymystem3.Mystem() #для использования лемматизации\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpack(data):\n",
    "    input = open(data, 'rb')\n",
    "    obj = pickle.load(input)\n",
    "    input.close()\n",
    "    return obj\n",
    "\n",
    "# загружаем частоты лем униграмм\n",
    "unigrams = unpack('1stemgrams.data')\n",
    "# убрали пробел из начала слов\n",
    "unigrams = {w[1:]:f for w,f in unigrams.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigrams = unpack('2grams.data')\n",
    "# убрали пробел из начала слов, пунктуацию (кроме дефисов) и двойные пробелы\n",
    "bigrams  = {''.join([i for i in w[1:] if i not in punctuation.replace('-','')]).replace('  ',' '):f for w,f in bigrams.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['дальнейший допрос',\n",
       " 'оканчивать преступление',\n",
       " 'развитие христианство',\n",
       " 'и затонуть',\n",
       " 'подразумевать она',\n",
       " 'и однозначный',\n",
       " '2 введение',\n",
       " 'желать услышать',\n",
       " 'являться недооценка',\n",
       " 'закрепляться по']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigrams = unpack('3grams.data')\n",
    "# убрали пробел из начала слов, пунктуацию (кроме дефисов) и двойные пробелы\n",
    "trigrams = {''.join([i for i in w[1:] if i not in punctuation.replace('-','')]).replace('  ',' '):f for w,f in trigrams.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['в лифт я',\n",
       " 'что сам русский',\n",
       " 'почему ты думать',\n",
       " 'фестиваль в кольмар',\n",
       " 'твердость и прочность',\n",
       " 'тот число руководитель',\n",
       " 'судьба решать иначе',\n",
       " 'мысль рождаться в',\n",
       " 'мой учение то',\n",
       " 'да только что']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trigrams.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# читает из файла, убирает двойные пробелы и ручные переносы, последний \\n\n",
    "def reading(file):\n",
    "    f = open('texts\\\\{}.txt'.format(file), 'r', encoding='utf-8')\n",
    "    text = f.read()\n",
    "    text = text.replace('  ', ' ')\n",
    "    text = text.replace('-\\n', '')\n",
    "    if text[-1] == '\\n': # убираем последний \\n, если такой есть \n",
    "        text = text[:-1]\n",
    "    f.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minimummaker(): #ф-ция превращения текстового файла с минимумом в питоновский список. Запаковка списка\n",
    "    with open('min.txt', encoding='utf-8') as file:\n",
    "        lemtokens = [morph.parse(i)[0].normal_form for i in re.findall('\\w+', file.read().lower())] #делаем список лемм слов\n",
    "        minimum = list(set(lemtokens)) #убираем повторы\n",
    "        output = open('minimum.pkl', 'wb')\n",
    "        pickle.dump(minimum, output, 2)\n",
    "        output.close()\n",
    "\n",
    "def loadminimum(): #распаковка cписка с минимумом. Возвращает неупорядоченный список\n",
    "    input = open('minimum.pkl', 'rb')\n",
    "    minimum = pickle.load(input)\n",
    "    input.close()\n",
    "    return minimum\n",
    "\n",
    "#minimummaker()\n",
    "minimum = loadminimum()\n",
    "#print(minimum[-50:]) #проверка работы\n",
    "#print(len(minimum)) #2549 слов в списке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# загрузка модели\n",
    "def model_loading(file):\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(file, binary=False)\n",
    "    model.init_sims(replace=True)\n",
    "    print('Done!') \n",
    "    return model\n",
    "\n",
    "model = model_loading('news_upos_cbow_600_2_2018.vec') #на загрузку тратится минут 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# загрузка словаря ASIS\n",
    "with open('syns.data', 'rb') as f:\n",
    "     asis = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Таблица конверсии в UPoS из тэгов Mystem\n",
    "# словарь, переводящий теги mystem в universal теги моделей\n",
    "mystem_tags = {'A' : 'ADJ',\n",
    "       'ADV' : 'ADV',\n",
    "       'ADVPRO' : 'ADV',\n",
    "       'ANUM' : 'ADJ',\n",
    "       'APRO' : 'DET',\n",
    "       'COM' : 'ADJ',\n",
    "       'CONJ' : 'SCONJ',\n",
    "       'INTJ' : 'INTJ',\n",
    "       'NONLEX' : 'X',\n",
    "       'NUM' : 'NUM',\n",
    "       'PART' : 'PART',\n",
    "       'PR' : 'ADP',\n",
    "       'S' : 'NOUN',\n",
    "       'SPRO' : 'PRON',\n",
    "       'UNKN' : 'X',\n",
    "       'V' : 'VERB',\n",
    "       'X' : 'X',\n",
    "      'PROPN' : 'PROPN'} #последних 2 тегов в майстеме нет, но они задаются в классе для слов в соответсвующими пометами\n",
    "\n",
    "# словарь, переводящий теги пайморфи в universal теги моделей\n",
    "pymorphy_tags = {'ADJF':'ADJ',\n",
    "    'ADJS' : 'ADJ',\n",
    "    'ADVB' : 'ADV',\n",
    "    'COMP' : 'ADV',\n",
    "    'GRND' : 'VERB',\n",
    "    'INFN' : 'VERB',\n",
    "    'NOUN' : 'NOUN',\n",
    "    'PRED' : 'ADV',\n",
    "    'PRTF' : 'ADJ',\n",
    "    'PRTS' : 'VERB',\n",
    "    'VERB' : 'VERB'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Пример вывода атрибутов объектов класса \\ntoken.num, token.text, token.lem, token.pos, token.complexity, token.is_complex(threshold, use_min):\\n\\n223 Кукушкин _NAMED_ENTITY_ S -19.06687873346149 False\\n224   _SPACE_ None -19.06687873346149 False\\n225 является являться V -7.502177090710664 False\\n226   _SPACE_ None -19.06687873346149 False\\n227 должником должник S -11.184186527172464 True\\n228   _SPACE_ None -19.06687873346149 False\\n229 банка банк S -9.300701131889967 False\\n230 .  _PUNKTUATION_ None -19.06687873346149 False\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Token():\n",
    "    def __init__(self, w):\n",
    "        \n",
    "        self.num = None # номер в тексте\n",
    "        self.complexity = None # сложность слова\n",
    "        \n",
    "        # три варианта инициализации: \n",
    "        ## из анализа текста, \n",
    "        ## из уже имеющегося объекта (для дочернего класса ComplexWord) \n",
    "        ## из строки\n",
    "        \n",
    "        if isinstance(w, dict): # если получили результат работы mystem\n",
    "            \n",
    "            self.text = w['text']  # сам токен\n",
    "            self.len = len(w['text']) # его длина\n",
    "            \n",
    "            # определяет, сделан ли анализ и, соответственно, рассматривать ли как слово, требующее упрощения\n",
    "            gram = w.get('analysis')\n",
    "            if gram:\n",
    "                self.lexem = gram[0]['lex']  # лемма\n",
    "                \n",
    "                if not self.named_entity(gram[0]):  # именованная сущность или нет\n",
    "                    self.pos = self.pos_tag(gram[0]['gr'])  # часть речи\n",
    "                else:\n",
    "                    self.pos = 'PROPN' # universal tag for named entity - у майстема таких нет\n",
    "                \n",
    "                    \n",
    "            elif any(p in w['text'] for p in punctuation+'–«»'): # если это знак пунктуации (может быть с пробелом!)\n",
    "                self.lexem = '_PUNKTUATION_'\n",
    "                self.pos = None\n",
    "            \n",
    "            elif not re.findall('\\S',w['text']): # если это только пробельные символы\n",
    "                self.lexem = '_SPACE_'\n",
    "                self.pos = None\n",
    "                \n",
    "            # остальное - неизвестная и ненужная ерунда?\n",
    "            else:\n",
    "                self.lexem = '_UNK_'\n",
    "                self.pos = 'X' # universal tag for unknown\n",
    "            \n",
    "            \n",
    "        elif isinstance(w, Token): # для определения объектов дочернего класса ComplexWord\n",
    "            self.text = w.text\n",
    "            self.num = w.num\n",
    "            self.lexem = w.lexem \n",
    "            self.len = w.len\n",
    "            self.pos = w.pos\n",
    "            self.complexity = w.complexity\n",
    "            \n",
    "        \n",
    "        elif isinstance(w, str): # если хотим как класс токен определить строку, полученную из словаря или модели\n",
    "            self.text = w\n",
    "            self.pos = None\n",
    "            self.lexem = w\n",
    "            self.len = len(w)\n",
    "            self.num = None\n",
    "            self.complexity = None\n",
    "            \n",
    "        \n",
    "    # вытаскивает часть речи из разбора майстем\n",
    "    def pos_tag(self,gram):\n",
    "        if ',' in gram:\n",
    "            gram = gram.split(',')[0]\n",
    "        if '=' in gram:\n",
    "            gram = gram.split('=')[0]\n",
    "        return gram\n",
    "        \n",
    "    # определяет по тегам, является ли именованной сущностью\n",
    "    def named_entity(self,gram):\n",
    "        markers = {'сокр': ' - сокращение', 'фам': ' - фамилия', 'имя': ' - имя собственное', 'гео': ' - название места', }\n",
    "        if any(m in gram['gr'] for m in markers.keys()):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def complexity_params(self, param = 'freq'):\n",
    "        # если по частотности\n",
    "        if param == 'freq':\n",
    "            self.complexity = unigrams.get(self.lexem, 0)\n",
    "            \n",
    "        # если по коэффициенту информативности. Отрицательное значение. Чем он меньше, тем сложнее\n",
    "        elif param == 'inf':\n",
    "            self.complexity = math.log((unigrams.get(self.lexem, 0)+1)/(sum(f for f in unigrams.values())+1))\n",
    "        \n",
    "    \n",
    "    def is_complex(self, threshold = '600', use_min = False, len_threshold = 1000):\n",
    "        exceptions = ['_PUNKTUATION_', '_SPACE_', '_UNK_']\n",
    "        # проверка, что это слово и что его нужно рассматривать как сложное (не нарицательное)\n",
    "        if not any(exception in self.lexem for exception in exceptions) and self.pos not in ['PROPN']:\n",
    "            \n",
    "            # если показатель сложности - вхождение в минимум\n",
    "            if use_min:\n",
    "                if self.lexem not in minimum:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "\n",
    "            # если показатель сложности - пороговое значение сложности\n",
    "            # также может использоваться длина. По умолчанию слишком большое - 1000 (т.е. этот параметр не учитывается)\n",
    "            else:\n",
    "                if self.complexity < float(threshold) or self.len > len_threshold:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def convert_universal(self):\n",
    "        if self.pos in mystem_tags:\n",
    "            self.pos = mystem_tags[self.pos]\n",
    "        else:\n",
    "            self.pos = 'X' # Х - universal тег для неизвестных слов\n",
    "        return self\n",
    "'''Пример вывода атрибутов объектов класса \n",
    "token.num, token.text, token.lem, token.pos, token.complexity, token.is_complex(threshold, use_min):\n",
    "\n",
    "223 Кукушкин _NAMED_ENTITY_ S -19.06687873346149 False\n",
    "224   _SPACE_ None -19.06687873346149 False\n",
    "225 является являться V -7.502177090710664 False\n",
    "226   _SPACE_ None -19.06687873346149 False\n",
    "227 должником должник S -11.184186527172464 True\n",
    "228   _SPACE_ None -19.06687873346149 False\n",
    "229 банка банк S -9.300701131889967 False\n",
    "230 .  _PUNKTUATION_ None -19.06687873346149 False\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''# вытаскивает часть речи из разбора майстем\n",
    "def pos(gram):\n",
    "    if ',' in gram:\n",
    "        gram = gram.split(',')[0]\n",
    "    if '=' in gram:\n",
    "        gram = gram.split('=')[0]\n",
    "    return gram\n",
    "'''\n",
    "\n",
    "#анализ текста \n",
    "def text_structuring(text, param, threshold, use_min):\n",
    "    # анализирует текст \n",
    "    analysis = m.analyze(text)\n",
    "    tokens = []\n",
    "    for i, w in enumerate(analysis): # состаляем список объектов Tokens\n",
    "        token = Token(w)\n",
    "        token.num = i # добавляем токену в атрибуты его номер в тексте\n",
    "        token.complexity_params(param) # переопределяем сложность на основе выбранного параметра\n",
    "        token.convert_universal() # превращаем POS в universal формат\n",
    "        print(token.num, token.text, token.lexem, token.pos, token.complexity, token.is_complex(threshold, use_min))\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ПАРАМЕТРЫ анализа слов\n",
    "complexity_type = 'inf'\n",
    "global_threshold = -8.5\n",
    "use_min=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 В в ADP -3.486155131943022 False\n",
      "1   _SPACE_ X -19.06687873346149 False\n",
      "2 Астраханской астраханский ADJ -11.87544940342511 True\n",
      "3   _SPACE_ X -19.06687873346149 False\n",
      "4 области область NOUN -8.188095962474803 False\n",
      "5   _SPACE_ X -19.06687873346149 False\n",
      "6 ветерану ветеран NOUN -10.97250028848853 True\n",
      "7   _SPACE_ X -19.06687873346149 False\n",
      "8 ВОВ вов PROPN -14.907995650101817 False\n",
      "9   _SPACE_ X -19.06687873346149 False\n",
      "10 вернули вернуть VERB -9.947667294896409 False\n",
      "11   _SPACE_ X -19.06687873346149 False\n",
      "12 похищенные похищать VERB -11.006654493020532 True\n",
      "13   _SPACE_ X -19.06687873346149 False\n",
      "14 медали медаль NOUN -10.586972126831268 True\n",
      "15 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "16 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "17 В в ADP -3.486155131943022 False\n",
      "18   _SPACE_ X -19.06687873346149 False\n",
      "19 Астраханской астраханский ADJ -11.87544940342511 True\n",
      "20   _SPACE_ X -19.06687873346149 False\n",
      "21 области область NOUN -8.188095962474803 False\n",
      "22   _SPACE_ X -19.06687873346149 False\n",
      "23 ветерану ветеран NOUN -10.97250028848853 True\n",
      "24   _SPACE_ X -19.06687873346149 False\n",
      "25 Великой великий ADJ -7.707722354468047 False\n",
      "26   _SPACE_ X -19.06687873346149 False\n",
      "27 Отечественной отечественный ADJ -9.66113606515282 True\n",
      "28   _SPACE_ X -19.06687873346149 False\n",
      "29 войны война NOUN -7.816461543327748 False\n",
      "30   _SPACE_ X -19.06687873346149 False\n",
      "31 вернули вернуть VERB -9.947667294896409 False\n",
      "32   _SPACE_ X -19.06687873346149 False\n",
      "33 украденные украсть VERB -10.412361351182033 True\n",
      "34   _SPACE_ X -19.06687873346149 False\n",
      "35 у у ADP -5.476072944878379 False\n",
      "36   _SPACE_ X -19.06687873346149 False\n",
      "37 него он PRON -4.13305326453326 False\n",
      "38   _SPACE_ X -19.06687873346149 False\n",
      "39 наградные наградные NOUN -19.06687873346149 True\n",
      "40   _SPACE_ X -19.06687873346149 False\n",
      "41 медали медаль NOUN -10.586972126831268 True\n",
      "42 .  _PUNKTUATION_ X -19.06687873346149 False\n",
      "43 Об об ADP -7.262754885511919 True\n",
      "44   _SPACE_ X -19.06687873346149 False\n",
      "45 этом это PRON -4.869477438777567 False\n",
      "46   _SPACE_ X -19.06687873346149 False\n",
      "47 сообщает сообщать VERB -8.634205145827439 False\n",
      "48   _SPACE_ X -19.06687873346149 False\n",
      "49 портал портал NOUN -12.693558943884478 True\n",
      "50  \" _PUNKTUATION_ X -19.06687873346149 False\n",
      "51 Утро утро NOUN -7.969832414948298 False\n",
      "52 . _PUNKTUATION_ X -19.06687873346149 False\n",
      "53 ру _UNK_ X -19.06687873346149 False\n",
      "54 \"  _PUNKTUATION_ X -19.06687873346149 False\n",
      "55 со со ADP -6.752575195748552 True\n",
      "56   _SPACE_ X -19.06687873346149 False\n",
      "57 ссылкой ссылка NOUN -10.338452641756877 True\n",
      "58   _SPACE_ X -19.06687873346149 False\n",
      "59 на на ADP -4.174175661002731 False\n",
      "60   _SPACE_ X -19.06687873346149 False\n",
      "61 пресс-службу пресс-служба NOUN -12.117981511148177 True\n",
      "62   _SPACE_ X -19.06687873346149 False\n",
      "63 регионального региональный ADJ -10.073575594367755 True\n",
      "64   _SPACE_ X -19.06687873346149 False\n",
      "65 МВД мвд PROPN -10.974639326737279 False\n",
      "66 . _PUNKTUATION_ X -19.06687873346149 False\n",
      "67 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "68 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "69 —  _UNK_ X -19.06687873346149 False\n",
      "70 Оперативники оперативник NOUN -12.197864282795782 True\n",
      "71   _SPACE_ X -19.06687873346149 False\n",
      "72 задержали задерживать VERB -9.988813400795125 True\n",
      "73   _SPACE_ X -19.06687873346149 False\n",
      "74 подозреваемую подозревать VERB -10.114143966354618 True\n",
      "75   _SPACE_ X -19.06687873346149 False\n",
      "76 в в ADP -3.486155131943022 False\n",
      "77   _SPACE_ X -19.06687873346149 False\n",
      "78 совершении совершение NOUN -10.903792357878274 True\n",
      "79   _SPACE_ X -19.06687873346149 False\n",
      "80 преступления преступление NOUN -9.447944818702611 True\n",
      "81 .  _PUNKTUATION_ X -19.06687873346149 False\n",
      "82 Ею она PRON -4.9255734554932324 False\n",
      "83   _SPACE_ X -19.06687873346149 False\n",
      "84 оказалась оказываться VERB -7.507967031654563 False\n",
      "85   _SPACE_ X -19.06687873346149 False\n",
      "86 61 _UNK_ X -19.06687873346149 False\n",
      "87 - _PUNKTUATION_ X -19.06687873346149 False\n",
      "88 летняя летний ADJ -9.444031438588096 False\n",
      "89   _SPACE_ X -19.06687873346149 False\n",
      "90 женщина женщина NOUN -7.588223853768708 False\n",
      "91 ,  _PUNKTUATION_ X -19.06687873346149 False\n",
      "92 которая который DET -5.3951392436881935 False\n",
      "93   _SPACE_ X -19.06687873346149 False\n",
      "94 помогала помогать VERB -8.13600640154562 False\n",
      "95   _SPACE_ X -19.06687873346149 False\n",
      "96 ветерану ветеран NOUN -10.97250028848853 True\n",
      "97   _SPACE_ X -19.06687873346149 False\n",
      "98 по по ADP -5.1820044394556914 False\n",
      "99   _SPACE_ X -19.06687873346149 False\n",
      "100 дому дом NOUN -7.446301380034336 False\n",
      "101 , —  _PUNKTUATION_ X -19.06687873346149 False\n",
      "102 сообщили сообщать VERB -8.634205145827439 False\n",
      "103   _SPACE_ X -19.06687873346149 False\n",
      "104 правоохранители правоохранитель NOUN -14.589541918983283 True\n",
      "105 . _PUNKTUATION_ X -19.06687873346149 False\n",
      "106 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "107 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "108 Женщина женщина NOUN -7.588223853768708 False\n",
      "109   _SPACE_ X -19.06687873346149 False\n",
      "110 объяснила объяснять VERB -8.424936082660874 False\n",
      "111   _SPACE_ X -19.06687873346149 False\n",
      "112 свои свой DET -5.485855828512082 False\n",
      "113   _SPACE_ X -19.06687873346149 False\n",
      "114 действия действие NOUN -8.051517218274066 False\n",
      "115   _SPACE_ X -19.06687873346149 False\n",
      "116 тем то PRON -5.111209789976896 False\n",
      "117 ,  _PUNKTUATION_ X -19.06687873346149 False\n",
      "118 что что SCONJ -4.319951223892165 False\n",
      "119   _SPACE_ X -19.06687873346149 False\n",
      "120 между между ADP -7.221517739852308 False\n",
      "121   _SPACE_ X -19.06687873346149 False\n",
      "122 ней она PRON -4.9255734554932324 False\n",
      "123   _SPACE_ X -19.06687873346149 False\n",
      "124 и и SCONJ -3.24763166905925 False\n",
      "125   _SPACE_ X -19.06687873346149 False\n",
      "126 ветераном ветеран NOUN -10.97250028848853 True\n",
      "127   _SPACE_ X -19.06687873346149 False\n",
      "128 возник возникать VERB -8.524383446466064 True\n",
      "129   _SPACE_ X -19.06687873346149 False\n",
      "130 конфликт конфликт NOUN -10.099502040248822 True\n",
      "131 ,  _PUNKTUATION_ X -19.06687873346149 False\n",
      "132 после после ADP -6.881269033189423 False\n",
      "133   _SPACE_ X -19.06687873346149 False\n",
      "134 чего что PRON -4.319951223892165 False\n",
      "135   _SPACE_ X -19.06687873346149 False\n",
      "136 она она PRON -4.9255734554932324 False\n",
      "137   _SPACE_ X -19.06687873346149 False\n",
      "138 забрала забирать VERB -9.812521476068532 True\n",
      "139   _SPACE_ X -19.06687873346149 False\n",
      "140 сковородку сковородка NOUN -12.13736196269784 True\n",
      "141   _SPACE_ X -19.06687873346149 False\n",
      "142 и и SCONJ -3.24763166905925 False\n",
      "143   _SPACE_ X -19.06687873346149 False\n",
      "144 награды награда NOUN -10.257165192953222 False\n",
      "145 ,  _PUNKTUATION_ X -19.06687873346149 False\n",
      "146 которые который DET -5.3951392436881935 False\n",
      "147   _SPACE_ X -19.06687873346149 False\n",
      "148 впоследствии впоследствии ADV -9.601353654626301 True\n",
      "149   _SPACE_ X -19.06687873346149 False\n",
      "150 выбросила выбрасывать VERB -10.114532190235263 True\n",
      "151   _SPACE_ X -19.06687873346149 False\n",
      "152 в в ADP -3.486155131943022 False\n",
      "153   _SPACE_ X -19.06687873346149 False\n",
      "154 урну урна NOUN -12.071112577156638 True\n",
      "155 . _PUNKTUATION_ X -19.06687873346149 False\n",
      "156 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "157 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "158 Полиция полиция NOUN -9.704847848244874 False\n",
      "159   _SPACE_ X -19.06687873346149 False\n",
      "160 обследовала обследовать VERB -11.754325235358891 True\n",
      "161   _SPACE_ X -19.06687873346149 False\n",
      "162 территорию территория NOUN -9.167750669702814 False\n",
      "163   _SPACE_ X -19.06687873346149 False\n",
      "164 городской городской ADJ -9.074556467224872 False\n",
      "165   _SPACE_ X -19.06687873346149 False\n",
      "166 свалки свалка NOUN -11.606963967220384 True\n",
      "167   _SPACE_ X -19.06687873346149 False\n",
      "168 и и SCONJ -3.24763166905925 False\n",
      "169   _SPACE_ X -19.06687873346149 False\n",
      "170 нашла находить VERB -7.451604282941571 False\n",
      "171   _SPACE_ X -19.06687873346149 False\n",
      "172 часть часть NOUN -7.373550217519937 False\n",
      "173   _SPACE_ X -19.06687873346149 False\n",
      "174 похищенных похищать VERB -11.006654493020532 True\n",
      "175   _SPACE_ X -19.06687873346149 False\n",
      "176 наград награда NOUN -10.257165192953222 False\n",
      "177   _SPACE_ X -19.06687873346149 False\n",
      "178 среди среди ADP -8.093024057697555 False\n",
      "179   _SPACE_ X -19.06687873346149 False\n",
      "180 твёрдых твердый ADJ -9.473318862324462 True\n",
      "181   _SPACE_ X -19.06687873346149 False\n",
      "182 бытовых бытовой ADJ -10.493116190557357 False\n",
      "183   _SPACE_ X -19.06687873346149 False\n",
      "184 отходов отходы NOUN -12.430932177774844 True\n",
      "185 .  _PUNKTUATION_ X -19.06687873346149 False\n",
      "186 В в ADP -3.486155131943022 False\n",
      "187   _SPACE_ X -19.06687873346149 False\n",
      "188 отношении отношение NOUN -7.621322458862115 False\n",
      "189   _SPACE_ X -19.06687873346149 False\n",
      "190 задержанной задерживать VERB -9.988813400795125 True\n",
      "191   _SPACE_ X -19.06687873346149 False\n",
      "192 возбуждено возбуждать VERB -9.872871016106204 True\n",
      "193   _SPACE_ X -19.06687873346149 False\n",
      "194 уголовное уголовный ADJ -9.858339983431934 True\n",
      "195   _SPACE_ X -19.06687873346149 False\n",
      "196 дело дело NOUN -6.454827778200301 False\n",
      "197 . _PUNKTUATION_ X -19.06687873346149 False\n",
      "198 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "199 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "200 Напомним напоминать VERB -8.79944275302013 False\n",
      "201 ,  _PUNKTUATION_ X -19.06687873346149 False\n",
      "202 ранее ранее ADV -9.877966308898927 True\n",
      "203   _SPACE_ X -19.06687873346149 False\n",
      "204 Лайф лайф PROPN -14.956004869288178 False\n",
      "205   _SPACE_ X -19.06687873346149 False\n",
      "206 сообщал сообщать VERB -8.634205145827439 False\n",
      "207 ,  _PUNKTUATION_ X -19.06687873346149 False\n",
      "208 что что SCONJ -4.319951223892165 False\n",
      "209   _SPACE_ X -19.06687873346149 False\n",
      "210 у у ADP -5.476072944878379 False\n",
      "211   _SPACE_ X -19.06687873346149 False\n",
      "212 93 _UNK_ X -19.06687873346149 False\n",
      "213 - _PUNKTUATION_ X -19.06687873346149 False\n",
      "214 летнего летний ADJ -9.444031438588096 False\n",
      "215   _SPACE_ X -19.06687873346149 False\n",
      "216 Василия василий PROPN -9.180639074334914 False\n",
      "217   _SPACE_ X -19.06687873346149 False\n",
      "218 Легонького легонький ADJ -13.029007813539351 True\n",
      "219   _SPACE_ X -19.06687873346149 False\n",
      "220 украли украсть VERB -10.412361351182033 True\n",
      "221   _SPACE_ X -19.06687873346149 False\n",
      "222 из из ADP -5.430146815838169 False\n",
      "223   _SPACE_ X -19.06687873346149 False\n",
      "224 квартиры квартира NOUN -8.362735792650131 False\n",
      "225   _SPACE_ X -19.06687873346149 False\n",
      "226 10 _UNK_ X -19.06687873346149 False\n",
      "227   _SPACE_ X -19.06687873346149 False\n",
      "228 наградных наградные NOUN -19.06687873346149 True\n",
      "229   _SPACE_ X -19.06687873346149 False\n",
      "230 медалей медаль NOUN -10.586972126831268 True\n",
      "231  \" _PUNKTUATION_ X -19.06687873346149 False\n",
      "232 За за ADP -5.532348753209741 False\n",
      "233   _SPACE_ X -19.06687873346149 False\n",
      "234 отвагу отвага NOUN -11.840669723360818 True\n",
      "235 \", \" _PUNKTUATION_ X -19.06687873346149 False\n",
      "236 За за ADP -5.532348753209741 False\n",
      "237   _SPACE_ X -19.06687873346149 False\n",
      "238 взятие взятие NOUN -11.427236445603477 True\n",
      "239   _SPACE_ X -19.06687873346149 False\n",
      "240 Берлина берлин PROPN -10.198324692930289 False\n",
      "241 \"  _PUNKTUATION_ X -19.06687873346149 False\n",
      "242 и и SCONJ -3.24763166905925 False\n",
      "243  \" _PUNKTUATION_ X -19.06687873346149 False\n",
      "244 За за ADP -5.532348753209741 False\n",
      "245   _SPACE_ X -19.06687873346149 False\n",
      "246 взятие взятие NOUN -11.427236445603477 True\n",
      "247   _SPACE_ X -19.06687873346149 False\n",
      "248 Кёнигсберга кенигсберг PROPN -12.822711832797753 False\n",
      "249 \" _PUNKTUATION_ X -19.06687873346149 False\n",
      "250 . _PUNKTUATION_ X -19.06687873346149 False\n",
      "251 \n",
      " _SPACE_ X -19.06687873346149 False\n"
     ]
    }
   ],
   "source": [
    "text = reading('news3')\n",
    "tokens = text_structuring(text, complexity_type, global_threshold, use_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Substitution(Token):\n",
    "    def __init__(self, w):\n",
    "        super().__init__(w)\n",
    "        self.similarity = None\n",
    "        self.fitness = None\n",
    "    \n",
    "    # для слов из словаря и тезауруса: определяем тег пайморфи, переводим в формат universal - так быстрее, чем майстемом\n",
    "    def tagging(self, w):\n",
    "        tag = morph.parse(w)[0].tag.POS\n",
    "        if tag in pymorphy_tags:\n",
    "            return pymorphy_tags[tag]\n",
    "        else:\n",
    "            return 'X' # Х - universal тег для неизвестных слов\n",
    "\n",
    "    # в случае, когда Substitution получено не через модель, и его близость неизвестна\n",
    "    def measuse_similarity(self, target):\n",
    "        #print(self.lexem, target.lexem, self.pos, target.pos)\n",
    "        subst_query = str(self.lexem+'_'+self.pos)\n",
    "        target_query = str(target.lexem+'_'+target.pos)\n",
    "        if subst_query in model and target_query in model:\n",
    "            self.similarity = model.similarity(subst_query, target_query)\n",
    "        # может понадобиться return self\n",
    "    \n",
    "    # приписываем атрибуты словам, взятым из словаря или тезауруса\n",
    "    def setting_atr(self, target):\n",
    "        self.pos = self.tagging(self.lexem)\n",
    "        self.complexity_params(complexity_type)\n",
    "        self.measuse_similarity(target)\n",
    "        \n",
    "    def language_model(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Complex_word(Token):\n",
    "    def __init__(self, w):\n",
    "        super().__init__(w)\n",
    "        self.substituts = None\n",
    "        self.place = None\n",
    "        self.context = None\n",
    "        self.easier = []\n",
    "        \n",
    "    # список замен, в зависимости от выбранной базы\n",
    "    def search_substituts(self, base_type='model'):\n",
    "        \n",
    "        # поиск по модели\n",
    "        def model_search(lexem, pos):\n",
    "            query = str(lexem+'_'+pos)\n",
    "            #print(query)\n",
    "            if query in model:\n",
    "                # формируем список квазисинонимов той же части речи\n",
    "                # при этом превращаем их в объекты соответствующего класса\n",
    "                syn_tokens = []\n",
    "                for syn, sim in model.most_similar(positive=[query]):\n",
    "                    syn_text = syn[:syn.find('_')] # текст до части речи\n",
    "                    \n",
    "                    syn_tok = Substitution(syn_text) # из текстовой строки инициализируем объект класса\n",
    "                    syn_tok.pos = syn[syn_tok.len+1:] # часть речи\n",
    "                    \n",
    "                    syn_tok.complexity_params(complexity_type) # сложность по функции в зависимости от выбранного параметра\n",
    "                    \n",
    "                    syn_tok.similarity = sim # а близость по параметру модели\n",
    "                    \n",
    "                    syn_tokens.append(syn_tok)\n",
    "                    \n",
    "                return syn_tokens\n",
    "            else:\n",
    "                return [] \n",
    "\n",
    "        # поиск по YARN\n",
    "        def yarn_search(target, filepath = 'yarn-synsets1.csv'):\n",
    "            with open(filepath, \"r\", newline=\"\") as file: # постепенный просмотр файла с синсетами (множествами синонимов)\n",
    "                reader = csv.DictReader(file, delimiter=';')\n",
    "                lst = []\n",
    "                for i,row in enumerate(reader):\n",
    "                    cur_line = row['words'].split(';') # считываем колонку с синсетами\n",
    "                    if len(cur_line)>1:\n",
    "                        if target.lexem in cur_line:\n",
    "                            del(reader)\n",
    "                            for c in cur_line:\n",
    "                                if ' ' not in c and c!=target.lexem: # формируем список однословных синонимов\n",
    "                                    sub_tok = Substitution(c)\n",
    "                                    \n",
    "                                    sub_tok.setting_atr(target)\n",
    "                                    \n",
    "                                    if sub_tok not in lst:\n",
    "                                        lst.append(sub_tok) \n",
    "                            #TODO: выделить неоднословные в отдельный класс и поискать их частотность по n-граммам?\n",
    "                            break\n",
    "                #print(lst)\n",
    "                return lst \n",
    "        \n",
    "        #поиск по ASIS\n",
    "        def asis_search(target):\n",
    "            if target.lexem in asis:\n",
    "                lst = []\n",
    "                for s in asis[target.lexem]:\n",
    "                    if ' ' not in s: # формируем список однословных синонимов\n",
    "                        sub_tok = Substitution(s)\n",
    "                        sub_tok.setting_atr(target)\n",
    "                        lst.append(sub_tok)\n",
    "                return lst\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        \n",
    "        if base_type == 'model':\n",
    "            self.substituts = model_search(self.lexem, self.pos)\n",
    "            \n",
    "        if base_type == 'yarn':\n",
    "            self.substituts = yarn_search(self)\n",
    "        \n",
    "        if base_type == 'asis':\n",
    "            self.substituts = asis_search(self)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def find_easier(self, use_min = False, threshold = global_threshold):\n",
    "        for sub in self.substituts:\n",
    "            if not sub.is_complex(threshold = threshold, use_min = use_min):\n",
    "                if sub.complexity > self.complexity:\n",
    "                    self.easier.append(sub)\n",
    "        return self\n",
    "    \n",
    "    def make_window(self, tokens, window = 10):\n",
    "        context = [self]\n",
    "        left_ind = self.num-1\n",
    "        right_ind = self.num+1\n",
    "        ind = 0\n",
    "        # добавляем по одному слову слева и/или справа, пока не наберется window + само слово\n",
    "        while len(context)<window+1:\n",
    "            while left_ind >= 0:\n",
    "                left_w = tokens[left_ind]\n",
    "                left_ind-=1\n",
    "                # проверка, что это слово\n",
    "                if any(exception in left_w.lexem for exception in ['_PUNKTUATION_', '_SPACE_', '_UNK_']):\n",
    "                    continue\n",
    "                else:\n",
    "                    context[:0] = [left_w] #вставляем слово слева от цепочки\n",
    "                    ind+=1 # индекс слова сдвигается\n",
    "                    break\n",
    "\n",
    "            while right_ind < len(tokens):\n",
    "                right_w = tokens[right_ind]\n",
    "                right_ind+=1\n",
    "                # проверка, что это слово и что его нужно рассматривать как сложное (не нарицательное)\n",
    "                if any(exception in right_w.lexem for exception in ['_PUNKTUATION_', '_SPACE_', '_UNK_']):\n",
    "                    continue\n",
    "                else:\n",
    "                    context.append(right_w) # справа от цепочки\n",
    "                    break\n",
    "        self.place = ind\n",
    "        self.context = context\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ветеран NOUN yarn : старик NOUN -8.345441216450578 0.23961324224267147\n",
      "ветеран NOUN yarn : старик NOUN -8.345441216450578 0.23961324224267147\n",
      "отечественный ADJ yarn : родной ADJ -9.113982160667819 0.020030045830958295\n",
      "портал NOUN yarn : вход NOUN -9.731845917527988 0.0002885425674101231\n",
      "портал NOUN yarn : дверь NOUN -7.711707098541537 0.04702153840192069\n",
      "подозревать VERB yarn : думать VERB -7.108101460589846 0.03465083751933694\n",
      "ветеран NOUN yarn : старик NOUN -8.345441216450578 0.23961324224267147\n",
      "ветеран NOUN yarn : старик NOUN -8.345441216450578 0.23961324224267147\n",
      "возникать VERB yarn : подниматься VERB -8.366785291997186 0.20159820914507315\n",
      "возникать VERB yarn : появляться VERB -8.062597938932536 0.35273497014407196\n",
      "возникать VERB yarn : начинаться VERB -8.153027397811316 0.3254855920577131\n",
      "возникать VERB yarn : происходить VERB -7.775560500548489 0.4064410044068008\n",
      "забирать VERB yarn : собирать VERB -8.770268596015583 0.24647246517422808\n",
      "впоследствии ADV yarn : затем ADV -8.178040913119712 0.31609775090326603\n",
      "впоследствии ADV yarn : с X -4.45008658009755 None\n",
      "впоследствии ADV yarn : после X -6.881269033189423 None\n",
      "впоследствии ADV yarn : потом ADV -7.003049606872163 None\n",
      "урна NOUN yarn : ваза NOUN -11.285740223616473 0.18364909329940507\n",
      "городской ADJ yarn : социальный ADJ -8.836644657528602 0.1692542972850649\n",
      "твердый ADJ yarn : крепкий ADJ -9.446616532660201 0.19761365385260093\n",
      "бытовой ADJ yarn : обыкновенный ADJ -9.573768333394547 0.09706062873600835\n",
      "возбуждать VERB yarn : вызывать VERB -8.250787236909202 0.19681934685503943\n",
      "возбуждать VERB yarn : приглашать VERB -9.026764583393536 0.07855554080163785\n",
      "отвага NOUN yarn : мужество NOUN -10.635679255212228 0.6157282259832405\n"
     ]
    }
   ],
   "source": [
    "# Отбор сложных слов, превращение их в подкласс сложных слов и поиск замен\n",
    "# несложные слова также остаются на своих местах\n",
    "def selecting_complex(tokens, base_type=['yarn','model','asis'][0], threshold = global_threshold, use_min = False):\n",
    "    complex_words = []\n",
    "    for token in tokens:\n",
    "        if token.is_complex(threshold, use_min):\n",
    "            comp_token = Complex_word(token) # токен становится Сложным словом\n",
    "            \n",
    "            comp_token.search_substituts(base_type=base_type)\n",
    "            \n",
    "            complex_words.append(comp_token)\n",
    "            \n",
    "            # код для принтов\n",
    "            \n",
    "            '''\n",
    "            if comp_token.substituts:\n",
    "                for syn in comp_token.substituts:\n",
    "                    print (comp_token.lexem, comp_token.complexity, base_type, ':', syn.lexem, syn.complexity, syn.similarity)\n",
    "            '''\n",
    "            comp_token.find_easier(threshold, use_min)\n",
    "            if comp_token.easier:\n",
    "                for syn in comp_token.easier:\n",
    "                    print (comp_token.lexem, comp_token.pos, base_type, ':', syn.lexem, syn.pos, syn.complexity, syn.similarity)\n",
    "            \n",
    "        else:\n",
    "            complex_words.append(token)\n",
    "    return complex_words\n",
    "\n",
    "complex_words = selecting_complex(tokens, 'yarn', threshold = global_threshold, use_min = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решить проблему нахождения более простых синонимов. Пока возвращает только пустоту при use_min = True (по видимому, слов нет в минимуме?). Сейчас структура не очень логичная: сложные слова в тексте - те, что не вошли в минимум, но отнесение к сложным/простым синонимов происходит на основании порогового значения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### теперь, когда для каждого сложного слова есть список его синонимов с нужными параметрами, можно делать модель на н-граммах\n",
    "### контекстное окно: 2n слов (по n слева и справа), можно задавать. Вероятность на основе логарифма + Лапласса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngrams_dict = {0: {}, 1: unigrams, 2: bigrams, 3: trigrams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prob(ngram, n):\n",
    "    c1 = ' '.join(ngram[-n:]) # в числителе частота строки длины n\n",
    "    c2 = ' '.join(ngram[-n:-1]) # в знаменателе - строки без последнего символа\n",
    "    d = ngrams_dict.get(n) # для поиска числителя берем словарь n-грамм\n",
    "    d2 = ngrams_dict.get(n-1) # для поиска знаменателя - словарь n-1-грамм\n",
    "    V = len(unigrams)\n",
    "    #len(ngrams_dict.get(n-1,len(unigrams))) # сглаживание лапласса: добавляем размер словаря знаменателя\n",
    "    p1 = d.get(c1,1)\n",
    "    p2 = d2.get(c2,1)\n",
    "    print('\\t',c1, '/', c2, ':' , p1, '/', p2, '(', p1+1, '/', p2+V, ')')\n",
    "    return math.log(p1+1)-math.log(p2+V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'complex_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ebfd687825e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcomplex_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mComplex_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0measier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[1;31m# окна контекста строим для тех сложных слов, для которых есть варианты замен, которые проще, чем слово\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_window\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomplex_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'complex_words' is not defined"
     ]
    }
   ],
   "source": [
    "for token in complex_words:\n",
    "    if isinstance(token, Complex_word):\n",
    "        if token.easier: \n",
    "            # окна контекста строим для тех сложных слов, для которых есть варианты замен, которые проще, чем слово\n",
    "            token.make_window(complex_words, window = 5)\n",
    "            \n",
    "            # вывод\n",
    "            context_lem = [c.lexem for c in token.context]\n",
    "            #print(token.text, token.place, ':', context_lem)\n",
    "            \n",
    "            # вычисляем вероятность цепочки\n",
    "            \n",
    "            # генерация 3-грамм и 2-грамм\n",
    "            text_3grams = [n for n in ngrams(context_lem, 3)]\n",
    "            \n",
    "            # добавляем слева искусственные н-граммы для вычисления вероятности\n",
    "            #text_3grams = [('', '', text_3grams[0][0]), ('', text_3grams[0][0], text_3grams[0][1])] + text_3grams\n",
    "            \n",
    "            #print(text_3grams)\n",
    "            p_context = 1.0\n",
    "            for ngram in text_3grams:\n",
    "                \n",
    "                p = math.log(prob(ngram, 3))\n",
    "                #print(ngram, p)\n",
    "                p_context+=p\n",
    "                \n",
    "            print('\\nВероятность слова _{0}_ в контексте {1} = {2}\\n'.format(token.text, context_lem, p_context))\n",
    "            \n",
    "            best_fitness = -1000\n",
    "            best_sub = None\n",
    "            for sub in token.easier:\n",
    "                sub_context_lem = context_lem[:token.place]+[sub.lexem]+context_lem[token.place+1:]\n",
    "                #сделать из этого куска функцию\n",
    "                sub_3grams = [n for n in ngrams(sub_context_lem, 3)]\n",
    "                # добавляем слева искусственные н-граммы для вычисления вероятности\n",
    "                #sub_3grams = [('', '', sub_3grams[0][0]), ('', sub_3grams[0][0], sub_3grams[0][1])] + sub_3grams\n",
    "            \n",
    "                p_changed_context = 1.0\n",
    "                for ngram in sub_3grams:\n",
    "\n",
    "                    p = prob(ngram, 3)\n",
    "                    #print(ngram, p)\n",
    "                    p_changed_context+=p\n",
    "                    \n",
    "                fit='НЕТ'\n",
    "                if p_changed_context>p_context:\n",
    "                    fit='ДА'\n",
    "                    sub.fitness = p_changed_context\n",
    "                    if sub.fitness > best_fitness:\n",
    "                        best_fitness = sub.fitness\n",
    "                        best_sub = sub\n",
    "                print('\\nВероятность замены _{0}_ в контексте {1} = {2}, разница = {3}, меняем? - {4}\\n'.format(sub.text, sub_context_lem, p_changed_context, p_context-p_changed_context, fit))\n",
    "            \n",
    "            if best_sub:\n",
    "                print('Лучшее значение вероятности: {0}\\n'.format(best_sub.lexem))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams.get('в астраханский',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6b2b4ca340cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "import math\n",
    "math.log(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

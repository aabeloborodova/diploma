{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aleksandra\\AppData\\Roaming\\Python\\Python36\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'from nltk.corpus import stopwords\\nstop_words = stopwords.words(\"russian\")'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# импорты\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "from string import punctuation\n",
    "import math\n",
    "from tqdm import tqdm_notebook\n",
    "import csv\n",
    "import gensim\n",
    "\n",
    "import pymystem3\n",
    "m = pymystem3.Mystem() #для использования лемматизации\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "'''from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"russian\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посчитать информативность слов из б1 и меньше - скачать списки, используемые в РКИ. Исходя из этого вычислять сложность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpack(data):\n",
    "    input = open(data, 'rb')\n",
    "    obj = pickle.load(input)\n",
    "    input.close()\n",
    "    return obj\n",
    "\n",
    "# загружаем частоты лем униграмм\n",
    "unigrams = unpack('1stemgrams.data')\n",
    "# убрали пробел из начала слов\n",
    "unigrams = {w[1:]:f for w,f in unigrams.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigrams = unpack('2grams.data')\n",
    "# убрали пробел из начала слов, пунктуацию (кроме дефисов) и двойные пробелы\n",
    "bigrams  = {''.join([i for i in w[1:] if i not in punctuation.replace('-','')]).replace('  ',' '):f for w,f in bigrams.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['дальнейший допрос',\n",
       " 'оканчивать преступление',\n",
       " 'развитие христианство',\n",
       " 'и затонуть',\n",
       " 'подразумевать она',\n",
       " 'и однозначный',\n",
       " '2 введение',\n",
       " 'желать услышать',\n",
       " 'являться недооценка',\n",
       " 'закрепляться по']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigrams = unpack('3grams.data')\n",
    "# убрали пробел из начала слов, пунктуацию (кроме дефисов) и двойные пробелы\n",
    "trigrams = {''.join([i for i in w[1:] if i not in punctuation.replace('-','')]).replace('  ',' '):f for w,f in trigrams.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['в лифт я',\n",
       " 'что сам русский',\n",
       " 'почему ты думать',\n",
       " 'фестиваль в кольмар',\n",
       " 'твердость и прочность',\n",
       " 'тот число руководитель',\n",
       " 'судьба решать иначе',\n",
       " 'мысль рождаться в',\n",
       " 'мой учение то',\n",
       " 'да только что']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trigrams.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# читает из файла, убирает двойные пробелы и ручные переносы, последний \\n\n",
    "def reading(file):\n",
    "    f = open('texts\\\\{}.txt'.format(file), 'r', encoding='utf-8')\n",
    "    text = f.read()\n",
    "    text = text.replace('  ', ' ')\n",
    "    text = text.replace('-\\n', '')\n",
    "    if text[-1] == '\\n': # убираем последний \\n, если такой есть \n",
    "        text = text[:-1]\n",
    "    f.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minimummaker(): #ф-ция превращения текстового файла с минимумом в питоновский список. Запаковка списка\n",
    "    with open('min.txt', encoding='utf-8') as file:\n",
    "        lemtokens = [morph.parse(i)[0].normal_form for i in re.findall('\\w+', file.read().lower())] #делаем список лемм слов\n",
    "        minimum = list(set(lemtokens)) #убираем повторы\n",
    "        output = open('minimum.pkl', 'wb')\n",
    "        pickle.dump(minimum, output, 2)\n",
    "        output.close()\n",
    "\n",
    "def loadminimum(): #распаковка cписка с минимумом. Возвращает неупорядоченный список\n",
    "    input = open('minimum.pkl', 'rb')\n",
    "    minimum = pickle.load(input)\n",
    "    input.close()\n",
    "    return minimum\n",
    "\n",
    "#minimummaker()\n",
    "minimum = loadminimum()\n",
    "#print(minimum[-50:]) #проверка работы\n",
    "#print(len(minimum)) #2549 слов в списке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# всякие непонятные куски, портящие статистику по минимумам:\n",
    "to_remove = ['ми', 'вод', 'кий', 'ре','вая', 'ча', 'то быть', 'чка', 'стен']\n",
    "with open('A1.txt', 'r', encoding='utf-8') as file:\n",
    "    lemtokens = [morph.parse(i)[0].normal_form for i in file.read().lower().split('\\n') if morph.parse(i)[0].normal_form not in to_remove] #делаем список лемм слов\n",
    "    minimum = list(set(lemtokens)) #убираем повторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['дерево', 'почему', 'вчера', 'скоро', 'гостиница', 'думать ', 'волейбол ', 'потом', 'правый', 'письмо', 'повторять', 'быть', 'жарко', 'интересный', 'вагон', 'нарисовать ', 'кончить', 'грамм', 'дорого', 'композитор', 'жёлтый', 'долго', 'обед', 'лето', 'шапка', 'или', 'ребёнок', 'магнитофон', 'голова', 'теннис', 'французско-русский', 'сахар ', 'тетрадь', 'удовольствие ', 'просить', 'экскурсия', 'взять ', 'какой', 'переводчик', 'родитель', 'надо', 'спрашивать', 'жить', 'делать ', 'звонить', 'жизнь ', 'ужинать ', 'жаль', 'до', 'деньги ']\n"
     ]
    }
   ],
   "source": [
    "print(minimum[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# штука, которая рассчитывает порог коэффициента информативности слова на основе лексического минимума - Я ПРИДУМАЛА!\n",
    "# отрицательный. чем коэффициент информативности меньше, чем сложнее слово (с уменьшением дроби логарифм уменьшается тоже)\n",
    "from collections import Counter\n",
    "\n",
    "det = sum(f for f in unigrams.values())+1\n",
    "inf_coef = Counter([round(math.log((unigrams.get(w)+1)/det),1) for w in minimum if w in unigrams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "for w in minimum:\n",
    "    if w in unigrams:\n",
    "        if round(math.log((unigrams.get(w)+1)/det),1)<-17:\n",
    "            l.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['съесть']\n"
     ]
    }
   ],
   "source": [
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.median(list(inf_coef.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-9.3, 32),\n",
       " (-9.0, 24),\n",
       " (-8.9, 21),\n",
       " (-9.2, 20),\n",
       " (-9.4, 20),\n",
       " (-8.6, 19),\n",
       " (-8.8, 18),\n",
       " (-9.7, 18),\n",
       " (-9.8, 18),\n",
       " (-9.6, 17)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_coef.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# загрузка модели\n",
    "def model_loading(file):\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(file, binary=False)\n",
    "    model.init_sims(replace=True)\n",
    "    print('Done!') \n",
    "    return model\n",
    "\n",
    "model = model_loading('news_upos_cbow_600_2_2018.vec') #на загрузку тратится минут 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# загрузка словаря ASIS\n",
    "with open('syns.data', 'rb') as f:\n",
    "     asis = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Таблица конверсии в UPoS из тэгов Mystem\n",
    "# словарь, переводящий теги mystem в universal теги моделей\n",
    "mystem_tags = {'A' : 'ADJ',\n",
    "       'ADV' : 'ADV',\n",
    "       'ADVPRO' : 'ADV',\n",
    "       'ANUM' : 'ADJ',\n",
    "       'APRO' : 'DET',\n",
    "       'COM' : 'ADJ',\n",
    "       'CONJ' : 'SCONJ',\n",
    "       'INTJ' : 'INTJ',\n",
    "       'NONLEX' : 'X',\n",
    "       'NUM' : 'NUM',\n",
    "       'PART' : 'PART',\n",
    "       'PR' : 'ADP',\n",
    "       'S' : 'NOUN',\n",
    "       'SPRO' : 'PRON',\n",
    "       'UNKN' : 'X',\n",
    "       'V' : 'VERB',\n",
    "       'X' : 'X',\n",
    "      'PROPN' : 'PROPN'} #последних 2 тегов в майстеме нет, но они задаются в классе для слов в соответсвующими пометами\n",
    "\n",
    "# словарь, переводящий теги пайморфи в universal теги моделей\n",
    "pymorphy_tags = {'ADJF':'ADJ',\n",
    "    'ADJS' : 'ADJ',\n",
    "    'ADVB' : 'ADV',\n",
    "    'COMP' : 'ADV',\n",
    "    'GRND' : 'VERB',\n",
    "    'INFN' : 'VERB',\n",
    "    'NOUN' : 'NOUN',\n",
    "    'PRED' : 'ADV',\n",
    "    'PRTF' : 'ADJ',\n",
    "    'PRTS' : 'VERB',\n",
    "    'VERB' : 'VERB'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Пример вывода атрибутов объектов класса \\ntoken.num, token.text, token.lem, token.pos, token.complexity, token.is_complex(threshold, use_min):\\n\\n223 Кукушкин _NAMED_ENTITY_ S -19.06687873346149 False\\n224   _SPACE_ None -19.06687873346149 False\\n225 является являться V -7.502177090710664 False\\n226   _SPACE_ None -19.06687873346149 False\\n227 должником должник S -11.184186527172464 True\\n228   _SPACE_ None -19.06687873346149 False\\n229 банка банк S -9.300701131889967 False\\n230 .  _PUNKTUATION_ None -19.06687873346149 False\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Token():\n",
    "    def __init__(self, w):\n",
    "        \n",
    "        self.num = None # номер в тексте\n",
    "        self.complexity = None # сложность слова\n",
    "        self.av_similarity = None\n",
    "        \n",
    "        # три варианта инициализации: \n",
    "        ## из анализа текста, \n",
    "        ## из уже имеющегося объекта (для дочернего класса ComplexWord) \n",
    "        ## из строки\n",
    "        \n",
    "        if isinstance(w, dict): # если получили результат работы mystem\n",
    "            \n",
    "            self.text = w['text']  # сам токен\n",
    "            self.len = len(w['text']) # его длина\n",
    "            \n",
    "            # определяет, сделан ли анализ и, соответственно, рассматривать ли как слово, требующее упрощения\n",
    "            gram = w.get('analysis')\n",
    "            if gram:\n",
    "                self.lexem = gram[0]['lex']  # лемма\n",
    "                \n",
    "                if not self.named_entity(gram[0]):  # именованная сущность или нет\n",
    "                    self.pos = self.pos_tag(gram[0]['gr'])  # часть речи\n",
    "                else:\n",
    "                    self.pos = 'PROPN' # universal tag for named entity - у майстема таких нет\n",
    "                \n",
    "                    \n",
    "            elif any(p in w['text'] for p in punctuation+'–«»'): # если это знак пунктуации (может быть с пробелом!)\n",
    "                self.lexem = '_PUNKTUATION_'\n",
    "                self.pos = None\n",
    "            \n",
    "            elif not re.findall('\\S',w['text']): # если это только пробельные символы\n",
    "                self.lexem = '_SPACE_'\n",
    "                self.pos = None\n",
    "                \n",
    "            # остальное - неизвестная и ненужная ерунда?\n",
    "            else:\n",
    "                self.lexem = '_UNK_'\n",
    "                self.pos = 'X' # universal tag for unknown\n",
    "            \n",
    "            \n",
    "        elif isinstance(w, Token): # для определения объектов дочернего класса ComplexWord\n",
    "            self.text = w.text\n",
    "            self.num = w.num\n",
    "            self.lexem = w.lexem \n",
    "            self.len = w.len\n",
    "            self.pos = w.pos\n",
    "            self.complexity = w.complexity\n",
    "            \n",
    "        \n",
    "        elif isinstance(w, str): # если хотим как класс токен определить строку, полученную из словаря или модели\n",
    "            self.text = w\n",
    "            self.pos = None\n",
    "            self.lexem = w\n",
    "            self.len = len(w)\n",
    "            self.num = None\n",
    "            self.complexity = None\n",
    "            \n",
    "        \n",
    "    # вытаскивает часть речи из разбора майстем\n",
    "    def pos_tag(self,gram):\n",
    "        if ',' in gram:\n",
    "            gram = gram.split(',')[0]\n",
    "        if '=' in gram:\n",
    "            gram = gram.split('=')[0]\n",
    "        return gram\n",
    "        \n",
    "    # определяет по тегам, является ли именованной сущностью\n",
    "    def named_entity(self,gram):\n",
    "        markers = {'сокр': ' - сокращение', 'фам': ' - фамилия', 'имя': ' - имя собственное', 'гео': ' - название места', }\n",
    "        if any(m in gram['gr'] for m in markers.keys()):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def complexity_params(self, param = 'freq'):\n",
    "        # если по частотности\n",
    "        if param == 'freq':\n",
    "            self.complexity = unigrams.get(self.lexem, 0)\n",
    "            \n",
    "        # если по коэффициенту информативности. Отрицательное значение. Чем он меньше, тем сложнее\n",
    "        elif param == 'inf':\n",
    "            self.complexity = math.log((unigrams.get(self.lexem, 0)+1)/(sum(f for f in unigrams.values())+1))\n",
    "        \n",
    "    \n",
    "    def is_complex(self, threshold = '600', use_min = False, len_threshold = 1000):\n",
    "        exceptions = ['_PUNKTUATION_', '_SPACE_', '_UNK_']\n",
    "        # проверка, что это слово и что его нужно рассматривать как сложное (не нарицательное)\n",
    "        if not any(exception in self.lexem for exception in exceptions) and self.pos not in ['PROPN']:\n",
    "            \n",
    "            # если показатель сложности - вхождение в минимум\n",
    "            if use_min:\n",
    "                if self.lexem not in minimum:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "\n",
    "            # если показатель сложности - пороговое значение сложности\n",
    "            # также может использоваться длина. По умолчанию слишком большое - 1000 (т.е. этот параметр не учитывается)\n",
    "            else:\n",
    "                if self.complexity < float(threshold) or self.len > len_threshold:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def convert_universal(self):\n",
    "        if self.pos in mystem_tags:\n",
    "            self.pos = mystem_tags[self.pos]\n",
    "        else:\n",
    "            self.pos = 'X' # Х - universal тег для неизвестных слов\n",
    "        return self\n",
    "    \n",
    "    # нужна в двух случаях: для Замен, полученных из словарей, чтобы приписывать им соответствующий параметр,\n",
    "    # и для подсчета адекватности слова контексту\n",
    "    def cos_sim(self, context):\n",
    "        #print(self.lexem, target.lexem, self.pos, target.pos)\n",
    "        subst_query = str(self.lexem+'_'+self.pos)\n",
    "        target_query = str(context.lexem+'_'+context.pos)\n",
    "        if subst_query in model and target_query in model:\n",
    "            return model.similarity(subst_query, target_query)\n",
    "        else:\n",
    "            return 0.0\n",
    "    \n",
    "'''Пример вывода атрибутов объектов класса \n",
    "token.num, token.text, token.lem, token.pos, token.complexity, token.is_complex(threshold, use_min):\n",
    "\n",
    "223 Кукушкин _NAMED_ENTITY_ S -19.06687873346149 False\n",
    "224   _SPACE_ None -19.06687873346149 False\n",
    "225 является являться V -7.502177090710664 False\n",
    "226   _SPACE_ None -19.06687873346149 False\n",
    "227 должником должник S -11.184186527172464 True\n",
    "228   _SPACE_ None -19.06687873346149 False\n",
    "229 банка банк S -9.300701131889967 False\n",
    "230 .  _PUNKTUATION_ None -19.06687873346149 False\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''# вытаскивает часть речи из разбора майстем\n",
    "def pos(gram):\n",
    "    if ',' in gram:\n",
    "        gram = gram.split(',')[0]\n",
    "    if '=' in gram:\n",
    "        gram = gram.split('=')[0]\n",
    "    return gram\n",
    "'''\n",
    "\n",
    "#анализ текста \n",
    "def text_structuring(text, param, threshold, use_min):\n",
    "    # анализирует текст \n",
    "    analysis = m.analyze(text)\n",
    "    tokens = []\n",
    "    for i, w in enumerate(analysis): # состаляем список объектов Tokens\n",
    "        token = Token(w)\n",
    "        token.num = i # добавляем токену в атрибуты его номер в тексте\n",
    "        token.complexity_params(param) # переопределяем сложность на основе выбранного параметра\n",
    "        token.convert_universal() # превращаем POS в universal формат\n",
    "        print(token.num, token.text, token.lexem, token.pos, token.complexity, token.is_complex(threshold, use_min))\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ПАРАМЕТРЫ анализа слов\n",
    "complexity_type = 'inf'\n",
    "#global_threshold = -8.5\n",
    "global_threshold = np.median(list(inf_coef.keys()))\n",
    "use_min=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Бракованные бракованный ADJ -14.010632928113182 True\n",
      "1   _SPACE_ X -19.06687873346149 False\n",
      "2 белорусские белорусский ADJ -11.414807987345007 True\n",
      "3   _SPACE_ X -19.06687873346149 False\n",
      "4 лекарства лекарство NOUN -10.277066347270518 True\n",
      "5   _SPACE_ X -19.06687873346149 False\n",
      "6 изъяли изымать VERB -11.391796875745156 True\n",
      "7   _SPACE_ X -19.06687873346149 False\n",
      "8 из из ADP -5.430146815838169 False\n",
      "9   _SPACE_ X -19.06687873346149 False\n",
      "10 обращения обращение NOUN -9.762410698760608 True\n",
      "11 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "12 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "13 Лекарственные лекарственный ADJ -11.538009476819239 True\n",
      "14   _SPACE_ X -19.06687873346149 False\n",
      "15 препараты препарат NOUN -10.247213384120835 True\n",
      "16   _SPACE_ X -19.06687873346149 False\n",
      "17 из из ADP -5.430146815838169 False\n",
      "18   _SPACE_ X -19.06687873346149 False\n",
      "19 Белоруссии белоруссия PROPN -11.274942776523432 False\n",
      "20 ,  _PUNKTUATION_ X -19.06687873346149 False\n",
      "21 в в ADP -3.486155131943022 False\n",
      "22   _SPACE_ X -19.06687873346149 False\n",
      "23 которых который DET -5.3951392436881935 False\n",
      "24   _SPACE_ X -19.06687873346149 False\n",
      "25 нашли находить VERB -7.451604282941571 False\n",
      "26   _SPACE_ X -19.06687873346149 False\n",
      "27 осколки осколок NOUN -10.869615362047153 True\n",
      "28   _SPACE_ X -19.06687873346149 False\n",
      "29 стекла стекло NOUN -9.211531556253297 True\n",
      "30 ,  _PUNKTUATION_ X -19.06687873346149 False\n",
      "31 не не PART -4.031841462794542 False\n",
      "32   _SPACE_ X -19.06687873346149 False\n",
      "33 использовались использоваться VERB -9.97565979239317 True\n",
      "34   _SPACE_ X -19.06687873346149 False\n",
      "35 для для ADP -5.761858855363178 False\n",
      "36   _SPACE_ X -19.06687873346149 False\n",
      "37 инъекций инъекция NOUN -12.724757314740337 True\n",
      "38   _SPACE_ X -19.06687873346149 False\n",
      "39 пациентам пациент NOUN -10.707509627238819 True\n",
      "40   _SPACE_ X -19.06687873346149 False\n",
      "41 и и SCONJ -3.24763166905925 False\n",
      "42   _SPACE_ X -19.06687873346149 False\n",
      "43 были быть VERB -4.356829513848956 False\n",
      "44   _SPACE_ X -19.06687873346149 False\n",
      "45 выведены выводить VERB -9.570382453029572 True\n",
      "46   _SPACE_ X -19.06687873346149 False\n",
      "47 из из ADP -5.430146815838169 False\n",
      "48   _SPACE_ X -19.06687873346149 False\n",
      "49 обращения обращение NOUN -9.762410698760608 True\n",
      "50 .  _PUNKTUATION_ X -19.06687873346149 False\n",
      "51 Об об ADP -7.262754885511919 False\n",
      "52   _SPACE_ X -19.06687873346149 False\n",
      "53 этом это PRON -4.869477438777567 False\n",
      "54   _SPACE_ X -19.06687873346149 False\n",
      "55 сообщили сообщать VERB -8.634205145827439 False\n",
      "56   _SPACE_ X -19.06687873346149 False\n",
      "57 в в ADP -3.486155131943022 False\n",
      "58   _SPACE_ X -19.06687873346149 False\n",
      "59 пресс-службе пресс-служба NOUN -12.117981511148177 True\n",
      "60   _SPACE_ X -19.06687873346149 False\n",
      "61 Минздрава минздрав NOUN -12.792116712219551 True\n",
      "62   _SPACE_ X -19.06687873346149 False\n",
      "63 РФ рф PROPN -9.309573691103443 False\n",
      "64 . _PUNKTUATION_ X -19.06687873346149 False\n",
      "65 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "66 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "67 —  _UNK_ X -19.06687873346149 False\n",
      "68 Какой-либо какой-либо DET -9.574748614738363 True\n",
      "69   _SPACE_ X -19.06687873346149 False\n",
      "70 информации информация NOUN -8.870385802442234 False\n",
      "71   _SPACE_ X -19.06687873346149 False\n",
      "72 о о ADP -5.629415606889456 False\n",
      "73   _SPACE_ X -19.06687873346149 False\n",
      "74 введении введение NOUN -10.220813542768608 True\n",
      "75   _SPACE_ X -19.06687873346149 False\n",
      "76 данных данный ADJ -8.35450750188282 False\n",
      "77   _SPACE_ X -19.06687873346149 False\n",
      "78 препаратов препарат NOUN -10.247213384120835 True\n",
      "79   _SPACE_ X -19.06687873346149 False\n",
      "80 и и SCONJ -3.24763166905925 False\n",
      "81   _SPACE_ X -19.06687873346149 False\n",
      "82 возникших возникать VERB -8.524383446466064 False\n",
      "83   _SPACE_ X -19.06687873346149 False\n",
      "84 в в ADP -3.486155131943022 False\n",
      "85   _SPACE_ X -19.06687873346149 False\n",
      "86 связи связь NOUN -8.158650107867322 False\n",
      "87   _SPACE_ X -19.06687873346149 False\n",
      "88 с с ADP -4.45008658009755 False\n",
      "89   _SPACE_ X -19.06687873346149 False\n",
      "90 этим этот DET -5.5857855248461705 False\n",
      "91   _SPACE_ X -19.06687873346149 False\n",
      "92 осложнениях осложнение NOUN -11.743048167259172 True\n",
      "93   _SPACE_ X -19.06687873346149 False\n",
      "94 у у ADP -5.476072944878379 False\n",
      "95   _SPACE_ X -19.06687873346149 False\n",
      "96 пациентов пациент NOUN -10.707509627238819 True\n",
      "97   _SPACE_ X -19.06687873346149 False\n",
      "98 в в ADP -3.486155131943022 False\n",
      "99   _SPACE_ X -19.06687873346149 False\n",
      "100 министерство министерство NOUN -9.431989838900876 True\n",
      "101   _SPACE_ X -19.06687873346149 False\n",
      "102 не не PART -4.031841462794542 False\n",
      "103   _SPACE_ X -19.06687873346149 False\n",
      "104 поступало поступать VERB -8.77253222520656 False\n",
      "105 , —  _PUNKTUATION_ X -19.06687873346149 False\n",
      "106 отметили отмечать VERB -8.84723098704463 False\n",
      "107   _SPACE_ X -19.06687873346149 False\n",
      "108 в в ADP -3.486155131943022 False\n",
      "109   _SPACE_ X -19.06687873346149 False\n",
      "110 ведомстве ведомство NOUN -10.171249106325007 True\n",
      "111 . _PUNKTUATION_ X -19.06687873346149 False\n",
      "112 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "113 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "114 Также также ADV -7.603921125869184 False\n",
      "115   _SPACE_ X -19.06687873346149 False\n",
      "116 в в ADP -3.486155131943022 False\n",
      "117   _SPACE_ X -19.06687873346149 False\n",
      "118 Минздраве минздрав NOUN -12.792116712219551 True\n",
      "119   _SPACE_ X -19.06687873346149 False\n",
      "120 отметили отмечать VERB -8.84723098704463 False\n",
      "121 ,  _PUNKTUATION_ X -19.06687873346149 False\n",
      "122 что что SCONJ -4.319951223892165 False\n",
      "123   _SPACE_ X -19.06687873346149 False\n",
      "124 за за ADP -5.532348753209741 False\n",
      "125   _SPACE_ X -19.06687873346149 False\n",
      "126 два два NUM -6.59735779584951 False\n",
      "127   _SPACE_ X -19.06687873346149 False\n",
      "128 года год NOUN -5.9731535421957815 False\n",
      "129   _SPACE_ X -19.06687873346149 False\n",
      "130 были быть VERB -4.356829513848956 False\n",
      "131   _SPACE_ X -19.06687873346149 False\n",
      "132 забракованы забраковывать VERB -13.166981379878997 True\n",
      "133   _SPACE_ X -19.06687873346149 False\n",
      "134 две два NUM -6.59735779584951 False\n",
      "135   _SPACE_ X -19.06687873346149 False\n",
      "136 серии серия NOUN -10.13611999790322 True\n",
      "137   _SPACE_ X -19.06687873346149 False\n",
      "138 двух два NUM -6.59735779584951 False\n",
      "139   _SPACE_ X -19.06687873346149 False\n",
      "140 торговых торговый ADJ -9.816933106892947 True\n",
      "141   _SPACE_ X -19.06687873346149 False\n",
      "142 наименований наименование NOUN -11.075963270370165 True\n",
      "143   _SPACE_ X -19.06687873346149 False\n",
      "144 лекарственных лекарственный ADJ -11.538009476819239 True\n",
      "145   _SPACE_ X -19.06687873346149 False\n",
      "146 препаратов препарат NOUN -10.247213384120835 True\n",
      "147   _SPACE_ X -19.06687873346149 False\n",
      "148 производства производство NOUN -8.59864571516994 False\n",
      "149   _SPACE_ X -19.06687873346149 False\n",
      "150 РУП руп PROPN -14.804198856420173 False\n",
      "151   _SPACE_ X -19.06687873346149 False\n",
      "152 Белмедпрепараты белмедпрепарат NOUN -19.06687873346149 True\n",
      "153   _SPACE_ X -19.06687873346149 False\n",
      "154 и и SCONJ -3.24763166905925 False\n",
      "155   _SPACE_ X -19.06687873346149 False\n",
      "156 14 _UNK_ X -19.06687873346149 False\n",
      "157   _SPACE_ X -19.06687873346149 False\n",
      "158 серий серия NOUN -10.13611999790322 True\n",
      "159   _SPACE_ X -19.06687873346149 False\n",
      "160 семи семь NUM -9.24454700788637 True\n",
      "161   _SPACE_ X -19.06687873346149 False\n",
      "162 торговых торговый ADJ -9.816933106892947 True\n",
      "163   _SPACE_ X -19.06687873346149 False\n",
      "164 наименований наименование NOUN -11.075963270370165 True\n",
      "165   _SPACE_ X -19.06687873346149 False\n",
      "166 лекарственных лекарственный ADJ -11.538009476819239 True\n",
      "167   _SPACE_ X -19.06687873346149 False\n",
      "168 препаратов препарат NOUN -10.247213384120835 True\n",
      "169   _SPACE_ X -19.06687873346149 False\n",
      "170 производства производство NOUN -8.59864571516994 False\n",
      "171   _SPACE_ X -19.06687873346149 False\n",
      "172 ОАО оао NOUN -10.528511306813845 True\n",
      "173   _SPACE_ X -19.06687873346149 False\n",
      "174 Борисовский борисовский ADJ -14.432149745231854 True\n",
      "175   _SPACE_ X -19.06687873346149 False\n",
      "176 завод завод NOUN -8.703501020839123 False\n",
      "177   _SPACE_ X -19.06687873346149 False\n",
      "178 медицинских медицинский ADJ -9.828534165075709 True\n",
      "179   _SPACE_ X -19.06687873346149 False\n",
      "180 препаратов препарат NOUN -10.247213384120835 True\n",
      "181 . _PUNKTUATION_ X -19.06687873346149 False\n",
      "182 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "183 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "184 Ранее ранее ADV -9.877966308898927 True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185   _SPACE_ X -19.06687873346149 False\n",
      "186 в в ADP -3.486155131943022 False\n",
      "187   _SPACE_ X -19.06687873346149 False\n",
      "188 СМИ сми PROPN -10.691940589626123 False\n",
      "189   _SPACE_ X -19.06687873346149 False\n",
      "190 появилась появляться VERB -8.062597938932536 False\n",
      "191   _SPACE_ X -19.06687873346149 False\n",
      "192 информация информация NOUN -8.870385802442234 False\n",
      "193   _SPACE_ X -19.06687873346149 False\n",
      "194 о о ADP -5.629415606889456 False\n",
      "195   _SPACE_ X -19.06687873346149 False\n",
      "196 том то PRON -5.111209789976896 False\n",
      "197 ,  _PUNKTUATION_ X -19.06687873346149 False\n",
      "198 что что SCONJ -4.319951223892165 False\n",
      "199   _SPACE_ X -19.06687873346149 False\n",
      "200 Росздравнадзор росздравнадзор NOUN -19.06687873346149 True\n",
      "201   _SPACE_ X -19.06687873346149 False\n",
      "202 по по ADP -5.1820044394556914 False\n",
      "203   _SPACE_ X -19.06687873346149 False\n",
      "204 итогам итог NOUN -9.545896884898616 True\n",
      "205   _SPACE_ X -19.06687873346149 False\n",
      "206 проверок проверка NOUN -10.048425523148962 True\n",
      "207   _SPACE_ X -19.06687873346149 False\n",
      "208 в в ADP -3.486155131943022 False\n",
      "209   _SPACE_ X -19.06687873346149 False\n",
      "210 Мордовии мордовия PROPN -13.68698137992103 False\n",
      "211 ,  _PUNKTUATION_ X -19.06687873346149 False\n",
      "212 Вологодской вологодский ADJ -12.322819547150143 True\n",
      "213   _SPACE_ X -19.06687873346149 False\n",
      "214 области область NOUN -8.188095962474803 False\n",
      "215   _SPACE_ X -19.06687873346149 False\n",
      "216 и и SCONJ -3.24763166905925 False\n",
      "217   _SPACE_ X -19.06687873346149 False\n",
      "218 Калининграде калининград PROPN -12.442813505661595 False\n",
      "219   _SPACE_ X -19.06687873346149 False\n",
      "220 отозвал отзывать VERB -11.963556670935377 True\n",
      "221   _SPACE_ X -19.06687873346149 False\n",
      "222 недоброкачественные недоброкачественный ADJ -14.191681410260339 True\n",
      "223   _SPACE_ X -19.06687873346149 False\n",
      "224 растворы раствор NOUN -9.911839677270397 True\n",
      "225   _SPACE_ X -19.06687873346149 False\n",
      "226 для для ADP -5.761858855363178 False\n",
      "227   _SPACE_ X -19.06687873346149 False\n",
      "228 инъекций инъекция NOUN -12.724757314740337 True\n",
      "229 ,  _PUNKTUATION_ X -19.06687873346149 False\n",
      "230 где где ADV -6.780378838818701 False\n",
      "231   _SPACE_ X -19.06687873346149 False\n",
      "232 якобы якобы PART -10.318256411144269 True\n",
      "233   _SPACE_ X -19.06687873346149 False\n",
      "234 обнаружены обнаруживать VERB -9.052834190360718 False\n",
      "235   _SPACE_ X -19.06687873346149 False\n",
      "236 стекло стекло NOUN -9.211531556253297 True\n",
      "237   _SPACE_ X -19.06687873346149 False\n",
      "238 и и SCONJ -3.24763166905925 False\n",
      "239   _SPACE_ X -19.06687873346149 False\n",
      "240 посторонние посторонний ADJ -10.16333529779677 True\n",
      "241   _SPACE_ X -19.06687873346149 False\n",
      "242 механические механический ADJ -10.420588968710842 True\n",
      "243   _SPACE_ X -19.06687873346149 False\n",
      "244 частицы частица NOUN -10.096192464926506 True\n",
      "245 . _PUNKTUATION_ X -19.06687873346149 False\n",
      "246 \n",
      " _SPACE_ X -19.06687873346149 False\n"
     ]
    }
   ],
   "source": [
    "text = reading('news2')\n",
    "tokens = text_structuring(text, complexity_type, global_threshold, use_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Substitution(Token):\n",
    "    def __init__(self, w):\n",
    "        super().__init__(w)\n",
    "        self.similarity = None\n",
    "        self.fitness = None\n",
    "        self.closeness = None\n",
    "    \n",
    "    # для слов из словаря и тезауруса: определяем тег пайморфи, переводим в формат universal - так быстрее, чем майстемом\n",
    "    def tagging(self, w):\n",
    "        tag = morph.parse(w)[0].tag.POS\n",
    "        if tag in pymorphy_tags:\n",
    "            return pymorphy_tags[tag]\n",
    "        else:\n",
    "            return 'X' # Х - universal тег для неизвестных слов\n",
    "    \n",
    "    # приписываем недостающие атрибуты словам, взятым из словаря или тезауруса\n",
    "    def setting_atr(self, target):\n",
    "        self.pos = self.tagging(self.lexem)\n",
    "        self.complexity_params(complexity_type)\n",
    "        self.similarity = self.cos_sim(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Complex_word(Token):\n",
    "    def __init__(self, w):\n",
    "        super().__init__(w)\n",
    "        self.substituts = None\n",
    "        self.place = None\n",
    "        self.context = None\n",
    "        self.easier = []\n",
    "        \n",
    "    # список замен, в зависимости от выбранной базы\n",
    "    def search_substituts(self, base_type='model'):\n",
    "        \n",
    "        # поиск по модели\n",
    "        def model_search(lexem, pos):\n",
    "            query = str(lexem+'_'+pos)\n",
    "            #print(query)\n",
    "            if query in model:\n",
    "                # формируем список квазисинонимов той же части речи\n",
    "                # при этом превращаем их в объекты соответствующего класса\n",
    "                syn_tokens = []\n",
    "                for syn, sim in model.most_similar(positive=[query]):\n",
    "                    syn_text = syn[:syn.find('_')] # текст до части речи\n",
    "                    \n",
    "                    syn_tok = Substitution(syn_text) # из текстовой строки инициализируем объект класса\n",
    "                    syn_tok.pos = syn[syn_tok.len+1:] # часть речи\n",
    "                    \n",
    "                    syn_tok.complexity_params(complexity_type) # сложность по функции в зависимости от выбранного параметра\n",
    "                    \n",
    "                    syn_tok.similarity = sim # а близость по параметру модели\n",
    "                    \n",
    "                    syn_tokens.append(syn_tok)\n",
    "                    \n",
    "                return syn_tokens\n",
    "            else:\n",
    "                return [] \n",
    "\n",
    "        # поиск по YARN\n",
    "        def yarn_search(target, filepath = 'yarn-synsets1.csv'):\n",
    "            with open(filepath, \"r\", newline=\"\") as file: # постепенный просмотр файла с синсетами (множествами синонимов)\n",
    "                reader = csv.DictReader(file, delimiter=';')\n",
    "                lst = []\n",
    "                for i,row in enumerate(reader):\n",
    "                    cur_line = row['words'].split(';') # считываем колонку с синсетами\n",
    "                    if len(cur_line)>1:\n",
    "                        if target.lexem in cur_line:\n",
    "                            del(reader)\n",
    "                            for c in cur_line:\n",
    "                                if ' ' not in c and c!=target.lexem: # формируем список однословных синонимов\n",
    "                                    sub_tok = Substitution(c)\n",
    "                                    \n",
    "                                    sub_tok.setting_atr(target)\n",
    "                                    \n",
    "                                    if sub_tok not in lst:\n",
    "                                        lst.append(sub_tok) \n",
    "                            #TODO: выделить неоднословные в отдельный класс и поискать их частотность по n-граммам?\n",
    "                            break\n",
    "                #print(lst)\n",
    "                return lst \n",
    "        \n",
    "        #поиск по ASIS\n",
    "        def asis_search(target):\n",
    "            if target.lexem in asis:\n",
    "                lst = []\n",
    "                for s in asis[target.lexem]:\n",
    "                    if ' ' not in s: # формируем список однословных синонимов\n",
    "                        sub_tok = Substitution(s)\n",
    "                        sub_tok.setting_atr(target)\n",
    "                        lst.append(sub_tok)\n",
    "                return lst\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        \n",
    "        if base_type == 'model':\n",
    "            self.substituts = model_search(self.lexem, self.pos)\n",
    "            \n",
    "        if base_type == 'yarn':\n",
    "            self.substituts = yarn_search(self)\n",
    "        \n",
    "        if base_type == 'asis':\n",
    "            self.substituts = asis_search(self)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def find_easier(self, use_min = False, threshold = global_threshold):\n",
    "        for sub in self.substituts:\n",
    "            if not sub.is_complex(threshold = threshold, use_min = use_min):\n",
    "                if sub.complexity > self.complexity:\n",
    "                    self.easier.append(sub)\n",
    "        return self\n",
    "    \n",
    "    def make_window(self, tokens, window = 10):\n",
    "        context = [self]\n",
    "        left_ind = self.num-1\n",
    "        right_ind = self.num+1\n",
    "        ind = 0\n",
    "        # добавляем по одному слову слева и/или справа, пока не наберется window + само слово\n",
    "        while len(context)<window+1:\n",
    "            while left_ind >= 0:\n",
    "                left_w = tokens[left_ind]\n",
    "                left_ind-=1\n",
    "                # проверка, что это слово\n",
    "                if any(exception in left_w.lexem for exception in ['_PUNKTUATION_', '_SPACE_', '_UNK_']):\n",
    "                    continue\n",
    "                else:\n",
    "                    context[:0] = [left_w] #вставляем слово слева от цепочки\n",
    "                    ind+=1 # индекс слова сдвигается\n",
    "                    break\n",
    "\n",
    "            while right_ind < len(tokens):\n",
    "                right_w = tokens[right_ind]\n",
    "                right_ind+=1\n",
    "                # проверка, что это слово и что его нужно рассматривать как сложное (не нарицательное)\n",
    "                if any(exception in right_w.lexem for exception in ['_PUNKTUATION_', '_SPACE_', '_UNK_']):\n",
    "                    continue\n",
    "                else:\n",
    "                    context.append(right_w) # справа от цепочки\n",
    "                    break\n",
    "        self.place = ind\n",
    "        self.context = context\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "изымать VERB yarn : брать VERB -8.611951584274115 0.10304389896830814\n",
      "пациент NOUN yarn : больной ADJ -8.563840649103 0.3274560641921585\n",
      "выводить VERB yarn : открывать VERB -8.24556163363519 0.2906104975356157\n",
      "осложнение NOUN yarn : проблема NOUN -8.221393716707802 0.18532289510563849\n",
      "осложнение NOUN yarn : задача NOUN -8.337681827282445 0.016228775087925692\n",
      "пациент NOUN yarn : больной ADJ -8.563840649103 0.3274560641921585\n"
     ]
    }
   ],
   "source": [
    "# Отбор сложных слов, превращение их в подкласс сложных слов и поиск замен\n",
    "# несложные слова также остаются на своих местах\n",
    "def selecting_complex(tokens, base_type=['yarn','model','asis'][0], threshold = global_threshold, use_min = False):\n",
    "    complex_words = []\n",
    "    for token in tokens:\n",
    "        if token.is_complex(threshold, use_min):\n",
    "            comp_token = Complex_word(token) # токен становится Сложным словом\n",
    "            \n",
    "            comp_token.search_substituts(base_type=base_type)\n",
    "            \n",
    "            complex_words.append(comp_token)\n",
    "            \n",
    "            # код для принтов\n",
    "            \n",
    "            '''\n",
    "            if comp_token.substituts:\n",
    "                for syn in comp_token.substituts:\n",
    "                    print (comp_token.lexem, comp_token.complexity, base_type, ':', syn.lexem, syn.complexity, syn.similarity)\n",
    "            '''\n",
    "            comp_token.find_easier(threshold, use_min)\n",
    "            if comp_token.easier:\n",
    "                for syn in comp_token.easier:\n",
    "                    print (comp_token.lexem, comp_token.pos, base_type, ':', syn.lexem, syn.pos, syn.complexity, syn.similarity)\n",
    "            \n",
    "        else:\n",
    "            complex_words.append(token)\n",
    "    return complex_words\n",
    "\n",
    "complex_words = selecting_complex(tokens, 'yarn', threshold = global_threshold, use_min = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### При use_min = True в основном возвращает пустоту - очень мало там слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### теперь, когда для каждого сложного слова есть список его синонимов с нужными параметрами, можно делать модель на н-граммах\n",
    "### контекстное окно: 2n+1 слов (по n слева и справа), можно задавать. Вероятность на основе логарифма + Лапласса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngrams_dict = {0: {}, 1: unigrams, 2: bigrams, 3: trigrams}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вообще должно быть не деление на V, а деление на объем корпуса - количество вхождений слов. Те подсчитать сумму значений частот униграмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram_prob(ngram, n, use_log= False):\n",
    "    c1 = ' '.join(ngram[-n:]) # в числителе частота строки длины n\n",
    "    c2 = ' '.join(ngram[-n:-1]) # в знаменателе - строки без последнего символа\n",
    "    d = ngrams_dict.get(n) # для поиска числителя берем словарь n-грамм\n",
    "    d2 = ngrams_dict.get(n-1) # для поиска знаменателя - словарь n-1-грамм\n",
    "    V = len(d)\n",
    "    #len(ngrams_dict.get(n-1,len(unigrams))) # сглаживание лапласса: добавляем размер словаря знаменателя\n",
    "    p1 = d.get(c1,0)+1\n",
    "    p2 = d2.get(c2,0)+V\n",
    "    \n",
    "    result =  p1/p2\n",
    "    \n",
    "    print('\\t',c1, '/', c2, ':' , p1-1, '/', p2-V, '(', p1, '/', p2, ')', '=', result)\n",
    "    \n",
    "    if use_log:\n",
    "        return math.log(p1)-math.log(p2)\n",
    "    else:\n",
    "        return p1/p2\n",
    "    \n",
    "def context_prob(ngrams, use_log= False):\n",
    "    if use_log:\n",
    "        p_context = 0.0\n",
    "    else:\n",
    "        p_context = 1.0\n",
    "    for ngram in ngrams:\n",
    "        p = ngram_prob(ngram, 3, use_log= use_log)\n",
    "        #print(ngram, p)\n",
    "        if use_log:\n",
    "            p_context+=p\n",
    "        else:\n",
    "            p_context*=p\n",
    "    return p_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "изъяли 3 : ['бракованный', 'белорусский', 'лекарство', 'изымать', 'из', 'обращение', 'лекарственный']\n",
      "[('бракованный', 'белорусский', 'лекарство'), ('белорусский', 'лекарство', 'изымать'), ('лекарство', 'изымать', 'из'), ('изымать', 'из', 'обращение'), ('из', 'обращение', 'лекарственный')]\n",
      "\t бракованный белорусский лекарство / бракованный белорусский : 0 / 0 ( 1 / 2876543 ) = 3.476395103427969e-07\n",
      "\t белорусский лекарство изымать / белорусский лекарство : 0 / 0 ( 1 / 2876543 ) = 3.476395103427969e-07\n",
      "\t лекарство изымать из / лекарство изымать : 0 / 0 ( 1 / 2876543 ) = 3.476395103427969e-07\n",
      "\t изымать из обращение / изымать из : 26 / 450 ( 27 / 2876993 ) = 9.38479864219343e-06\n",
      "\t из обращение лекарственный / из обращение : 0 / 158 ( 1 / 2876701 ) = 3.4762041658135483e-07\n",
      "\n",
      "Вероятность слова _изъяли_ в контексте ['бракованный', 'белорусский', 'лекарство', 'изымать', 'из', 'обращение', 'лекарственный'] = -71.0648734045552, близость к контексту = 0.06318795363628794\n",
      "\n",
      "\t бракованный белорусский лекарство / бракованный белорусский : 0 / 0 ( 1 / 2876543 ) = 3.476395103427969e-07\n",
      "\t белорусский лекарство брать / белорусский лекарство : 0 / 0 ( 1 / 2876543 ) = 3.476395103427969e-07\n",
      "\t лекарство брать из / лекарство брать : 0 / 0 ( 1 / 2876543 ) = 3.476395103427969e-07\n",
      "\t брать из обращение / брать из : 0 / 315 ( 1 / 2876858 ) = 3.476014457439331e-07\n",
      "\t из обращение лекарственный / из обращение : 0 / 158 ( 1 / 2876701 ) = 3.4762041658135483e-07\n",
      "\n",
      "Вероятность замены _брать_ в контексте ['бракованный', 'белорусский', 'лекарство', 'брать', 'из', 'обращение', 'лекарственный'] = -74.36066334546535, разница = 3.2957899409101543, используем? - НЕТ\n",
      "Близость к контексту = 0.14833864599418264, разница с исходным = -0.0851506923578947\n",
      "\n",
      "пациентам 3 : ['использоваться', 'для', 'инъекция', 'пациент', 'и', 'быть', 'выводить']\n",
      "[('использоваться', 'для', 'инъекция'), ('для', 'инъекция', 'пациент'), ('инъекция', 'пациент', 'и'), ('пациент', 'и', 'быть'), ('и', 'быть', 'выводить')]\n",
      "\t использоваться для инъекция / использоваться для : 0 / 1017 ( 1 / 2877560 ) = 3.475166460473457e-07\n",
      "\t для инъекция пациент / для инъекция : 0 / 110 ( 1 / 2876653 ) = 3.4762621699593243e-07\n",
      "\t инъекция пациент и / инъекция пациент : 0 / 0 ( 1 / 2876543 ) = 3.476395103427969e-07\n",
      "\t пациент и быть / пациент и : 0 / 128 ( 1 / 2876671 ) = 3.4762404181778176e-07\n",
      "\t и быть выводить / и быть : 8 / 3 ( 9 / 2876546 ) = 3.128752330051388e-06\n",
      "\n",
      "Вероятность слова _пациентам_ в контексте ['использоваться', 'для', 'инъекция', 'пациент', 'и', 'быть', 'выводить'] = -72.16371160844305, близость к контексту = 0.018838279596461813\n",
      "\n",
      "\t использоваться для инъекция / использоваться для : 0 / 1017 ( 1 / 2877560 ) = 3.475166460473457e-07\n",
      "\t для инъекция больной / для инъекция : 0 / 110 ( 1 / 2876653 ) = 3.4762621699593243e-07\n",
      "\t инъекция больной и / инъекция больной : 0 / 0 ( 1 / 2876543 ) = 3.476395103427969e-07\n",
      "\t больной и быть / больной и : 0 / 505 ( 1 / 2877048 ) = 3.4757849017465123e-07\n",
      "\t и быть выводить / и быть : 8 / 3 ( 9 / 2876546 ) = 3.128752330051388e-06\n",
      "\n",
      "Вероятность замены _больной_ в контексте ['использоваться', 'для', 'инъекция', 'больной', 'и', 'быть', 'выводить'] = -72.16384265411995, разница = 0.00013104567689481428, используем? - НЕТ\n",
      "Близость к контексту = 0.17350192916197332, разница с исходным = -0.1546636495655115\n",
      "\n",
      "выведены 3 : ['пациент', 'и', 'быть', 'выводить', 'из', 'обращение', 'об']\n",
      "[('пациент', 'и', 'быть'), ('и', 'быть', 'выводить'), ('быть', 'выводить', 'из'), ('выводить', 'из', 'обращение'), ('из', 'обращение', 'об')]\n",
      "\t пациент и быть / пациент и : 0 / 128 ( 1 / 2876671 ) = 3.4762404181778176e-07\n",
      "\t и быть выводить / и быть : 8 / 3 ( 9 / 2876546 ) = 3.128752330051388e-06\n",
      "\t быть выводить из / быть выводить : 176 / 560 ( 177 / 2877103 ) = 6.152021669019149e-05\n",
      "\t выводить из обращение / выводить из : 0 / 2088 ( 1 / 2878631 ) = 3.473873518349521e-07\n",
      "\t из обращение об / из обращение : 0 / 158 ( 1 / 2876701 ) = 3.4762041658135483e-07\n",
      "\n",
      "Вероятность слова _выведены_ в контексте ['пациент', 'и', 'быть', 'выводить', 'из', 'обращение', 'об'] = -66.98814534204917, близость к контексту = -0.031232112809915818\n",
      "\n",
      "\t пациент и быть / пациент и : 0 / 128 ( 1 / 2876671 ) = 3.4762404181778176e-07\n",
      "\t и быть открывать / и быть : 24 / 3 ( 25 / 2876546 ) = 8.690978694587188e-06\n",
      "\t быть открывать из / быть открывать : 0 / 13 ( 1 / 2876556 ) = 3.4763793925791814e-07\n",
      "\t открывать из обращение / открывать из : 0 / 12 ( 1 / 2876555 ) = 3.47638060110097e-07\n",
      "\t из обращение об / из обращение : 0 / 158 ( 1 / 2876701 ) = 3.4762041658135483e-07\n",
      "\n",
      "Вероятность замены _открывать_ в контексте ['пациент', 'и', 'быть', 'открывать', 'из', 'обращение', 'об'] = -71.14173225090096, разница = 4.153586908851793, используем? - НЕТ\n",
      "Близость к контексту = 0.12569435483549968, разница с исходным = -0.15692646764541548\n",
      "\n",
      "осложнениях 3 : ['связь', 'с', 'этот', 'осложнение', 'у', 'пациент', 'в']\n",
      "[('связь', 'с', 'этот'), ('с', 'этот', 'осложнение'), ('этот', 'осложнение', 'у'), ('осложнение', 'у', 'пациент'), ('у', 'пациент', 'в')]\n",
      "\t связь с этот / связь с : 196 / 4 ( 197 / 2876547 ) = 6.848488830531885e-05\n",
      "\t с этот осложнение / с этот : 0 / 10 ( 1 / 2876553 ) = 3.476383018147067e-07\n",
      "\t этот осложнение у / этот осложнение : 0 / 9 ( 1 / 2876552 ) = 3.476384226671376e-07\n",
      "\t осложнение у пациент / осложнение у : 0 / 4 ( 1 / 2876547 ) = 3.476390269305525e-07\n",
      "\t у пациент в / у пациент : 3 / 203 ( 4 / 2876746 ) = 1.3904599154739416e-06\n",
      "\n",
      "Вероятность слова _осложнениях_ в контексте ['связь', 'с', 'этот', 'осложнение', 'у', 'пациент', 'в'] = -67.69108078420743, близость к контексту = 0.051135679217402336\n",
      "\n",
      "\t связь с этот / связь с : 196 / 4 ( 197 / 2876547 ) = 6.848488830531885e-05\n",
      "\t с этот проблема / с этот : 131 / 10 ( 132 / 2876553 ) = 4.588825583954128e-05\n",
      "\t этот проблема у / этот проблема : 4 / 3244 ( 5 / 2879787 ) = 1.7362395204923142e-06\n",
      "\t проблема у пациент / проблема у : 0 / 4 ( 1 / 2876547 ) = 3.476390269305525e-07\n",
      "\t у пациент в / у пациент : 3 / 203 ( 4 / 2876746 ) = 1.3904599154739416e-06\n",
      "\n",
      "Вероятность замены _проблема_ в контексте ['связь', 'с', 'этот', 'проблема', 'у', 'пациент', 'в'] = -61.199964927583835, разница = -6.491115856623594, используем? - ДА\n",
      "Близость к контексту = 0.16200956391928142, разница с исходным = -0.11087388470187909\n",
      "\n",
      "\t связь с этот / связь с : 196 / 4 ( 197 / 2876547 ) = 6.848488830531885e-05\n",
      "\t с этот задача / с этот : 171 / 10 ( 172 / 2876553 ) = 5.979378791212955e-05\n",
      "\t этот задача у / этот задача : 0 / 2423 ( 1 / 2878966 ) = 3.4734692941840926e-07\n",
      "\t задача у пациент / задача у : 0 / 88 ( 1 / 2876631 ) = 3.4762887558397304e-07\n",
      "\t у пациент в / у пациент : 3 / 203 ( 4 / 2876746 ) = 1.3904599154739416e-06\n",
      "\n",
      "Вероятность замены _задача_ в контексте ['связь', 'с', 'этот', 'задача', 'у', 'пациент', 'в'] = -62.54445435586746, разница = -5.146626428339971, используем? - ДА\n",
      "Близость к контексту = 0.14132655150510412, разница с исходным = -0.09019087228770178\n",
      "\n",
      "Лучшее значение вероятности: проблема\n",
      "\n",
      "пациентов 3 : ['этот', 'осложнение', 'у', 'пациент', 'в', 'министерство', 'не']\n",
      "[('этот', 'осложнение', 'у'), ('осложнение', 'у', 'пациент'), ('у', 'пациент', 'в'), ('пациент', 'в', 'министерство'), ('в', 'министерство', 'не')]\n",
      "\t этот осложнение у / этот осложнение : 0 / 9 ( 1 / 2876552 ) = 3.476384226671376e-07\n",
      "\t осложнение у пациент / осложнение у : 0 / 4 ( 1 / 2876547 ) = 3.476390269305525e-07\n",
      "\t у пациент в / у пациент : 3 / 203 ( 4 / 2876746 ) = 1.3904599154739416e-06\n",
      "\t пациент в министерство / пациент в : 0 / 13 ( 1 / 2876556 ) = 3.4763793925791814e-07\n",
      "\t в министерство не / в министерство : 8 / 1920 ( 9 / 2878463 ) = 3.1266686422580385e-06\n",
      "\n",
      "Вероятность слова _пациентов_ в контексте ['этот', 'осложнение', 'у', 'пациент', 'в', 'министерство', 'не'] = -70.77772683316874, близость к контексту = 0.035363689602233184\n",
      "\n",
      "\t этот осложнение у / этот осложнение : 0 / 9 ( 1 / 2876552 ) = 3.476384226671376e-07\n",
      "\t осложнение у больной / осложнение у : 0 / 4 ( 1 / 2876547 ) = 3.476390269305525e-07\n",
      "\t у больной в / у больной : 5 / 1124 ( 6 / 2877667 ) = 2.085022346226996e-06\n",
      "\t больной в министерство / больной в : 0 / 4 ( 1 / 2876547 ) = 3.476390269305525e-07\n",
      "\t в министерство не / в министерство : 8 / 1920 ( 9 / 2878463 ) = 3.1266686422580385e-06\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вероятность замены _больной_ в контексте ['этот', 'осложнение', 'у', 'больной', 'в', 'министерство', 'не'] = -70.3725786984716, разница = -0.40514813469714284, используем? - ДА\n",
      "Близость к контексту = 0.17245920367605433, разница с исходным = -0.13709551407382115\n",
      "\n",
      "Лучшее значение вероятности: больной\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in complex_words:\n",
    "    if isinstance(token, Complex_word):\n",
    "        if token.easier: \n",
    "            # окна контекста строим для тех сложных слов, для которых есть варианты замен, которые проще, чем слово\n",
    "            token.make_window(complex_words, window = 5)\n",
    "            \n",
    "            context_lem = [c.lexem for c in token.context]\n",
    "            \n",
    "            # определение, насколько слово близко с другими словами из контекста\n",
    "            closeness = np.mean([token.cos_sim(w) for w in token.context if w.lexem!=token.lexem])\n",
    "            \n",
    "            print(token.text, token.place, ':', context_lem)\n",
    "            \n",
    "            # генерация n-грамм\n",
    "            text_3grams = [n for n in ngrams(context_lem, 3)]\n",
    "            \n",
    "            print(text_3grams)\n",
    "            \n",
    "            p_context = context_prob(text_3grams, use_log=True)\n",
    "                \n",
    "            print('\\nВероятность слова _{0}_ в контексте {1} = {2}, близость к контексту = {3}\\n'.format(token.text, context_lem, p_context,closeness))\n",
    "            \n",
    "            best_fitness = -1000\n",
    "            best_sub = None\n",
    "            \n",
    "            for sub in token.easier:\n",
    "                \n",
    "                # контекст с новым словом\n",
    "                sub_context = token.context[:token.place]+[sub]+token.context[token.place+1:]\n",
    "                \n",
    "                # определение, насколько слово близко с другими словами из контекста\n",
    "                sub.closeness = np.mean([sub.cos_sim(w) for w in sub_context if w.lexem!=token.lexem])\n",
    "            \n",
    "                sub_context_lem = [t.lexem for t in sub_context]\n",
    "                \n",
    "                sub_3grams = [n for n in ngrams(sub_context_lem, 3)]\n",
    "                \n",
    "                p_changed_context = context_prob(sub_3grams, use_log=True)\n",
    "                \n",
    "                fit='НЕТ'\n",
    "                if p_changed_context>p_context:\n",
    "                    fit='ДА'\n",
    "                    sub.fitness = p_changed_context\n",
    "                    if sub.fitness > best_fitness:\n",
    "                        best_fitness = sub.fitness\n",
    "                        best_sub = sub\n",
    "                    elif sub.fitness == best_fitness:\n",
    "                        if best_sub.complexity < sub.complexity:\n",
    "                            best_fitness = sub.fitness\n",
    "                            best_sub = sub\n",
    "                        \n",
    "                print('\\nВероятность замены _{0}_ в контексте {1} = {2}, разница = {3}, используем? - {4}'.format(sub.text, sub_context_lem, p_changed_context, p_context-p_changed_context, fit))\n",
    "                print('Близость к контексту = {0}, разница с исходным = {1}\\n'.format(sub.closeness, closeness-sub.closeness))\n",
    "            if best_sub:\n",
    "                print('Лучшее значение вероятности: {0}\\n'.format(best_sub.text))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "поправить вычисление вероятности - в знаменателе число уникальных нграмм, а не униграмм, ну и переписать с условием n>/=1\n",
    "изменить вычисление вероятности для логарифма\n",
    "Написать перплексию для получившейся модели - хз как????  \n",
    "поправить нахождение best_sub - если их больше 1, надо как-то это обрабатывать  \n",
    "прогнать статистическую модель на текстах и сформировать таким образом корпус положительных замен, которые потом можно подредачить ручками   \n",
    "заодно при редактировании ручками можно создать еще один файл, в котором редачить, а потом сравнить с первым, чтобы посчитать точность и проч  \n",
    "потом все-таки посчитать "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perplexity(docs):\n",
    "    docs = np.asarray(docs)\n",
    "    out_of_dict = []\n",
    "   \n",
    "    tmp_sum_docs = 0.0\n",
    "    N = 0.0\n",
    "    for doc in log_progress(docs):\n",
    "        tmp_sum_words = 0.0\n",
    "        for word in log_progress(words):\n",
    "            freq = frequencies.get(word, TOL)\n",
    "            freq_empirical = frequencies_empirical.get(word, 0.0)\n",
    "            N += freq_empirical\n",
    "            if freq == TOL:\n",
    "                out_of_dict.append(word)\n",
    "            tmp_sum_words += freq_empirical * np.log(freq)\n",
    "        tmp_sum_docs += tmp_sum_words\n",
    "       \n",
    "    return np.exp(-tmp_sum_docs / N), out_of_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

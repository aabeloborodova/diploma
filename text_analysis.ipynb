{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aleksandra\\AppData\\Roaming\\Python\\Python36\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# импорты\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "from string import punctuation\n",
    "import math\n",
    "from tqdm import tqdm_notebook\n",
    "import csv\n",
    "import gensim\n",
    "\n",
    "import pymystem3\n",
    "m = pymystem3.Mystem() #для использования лемматизации\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpack(data):\n",
    "    input = open(data, 'rb')\n",
    "    obj = pickle.load(input)\n",
    "    input.close()\n",
    "    return obj\n",
    "\n",
    "# загружаем частоты лем униграмм\n",
    "unigrams = unpack('1stemgrams.data')\n",
    "# убрали пробел из начала слов\n",
    "unigrams = {w[1:]:f for w,f in unigrams.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigrams = unpack('2grams.data')\n",
    "# убрали пробел из начала слов, пунктуацию (кроме дефисов) и двойные пробелы\n",
    "bigrams  = {''.join([i for i in w[1:] if i not in punctuation.replace('-','')]).replace('  ',' '):f for w,f in bigrams.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['дальнейший допрос',\n",
       " 'оканчивать преступление',\n",
       " 'развитие христианство',\n",
       " 'и затонуть',\n",
       " 'подразумевать она',\n",
       " 'и однозначный',\n",
       " '2 введение',\n",
       " 'желать услышать',\n",
       " 'являться недооценка',\n",
       " 'закрепляться по']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigrams = unpack('3grams.data')\n",
    "# убрали пробел из начала слов, пунктуацию (кроме дефисов) и двойные пробелы\n",
    "trigrams = {''.join([i for i in w[1:] if i not in punctuation.replace('-','')]).replace('  ',' '):f for w,f in trigrams.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['в лифт я',\n",
       " 'что сам русский',\n",
       " 'почему ты думать',\n",
       " 'фестиваль в кольмар',\n",
       " 'твердость и прочность',\n",
       " 'тот число руководитель',\n",
       " 'судьба решать иначе',\n",
       " 'мысль рождаться в',\n",
       " 'мой учение то',\n",
       " 'да только что']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trigrams.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# читает из файла, убирает двойные пробелы и ручные переносы, последний \\n\n",
    "def reading(file):\n",
    "    f = open('texts\\\\{}.txt'.format(file), 'r', encoding='utf-8')\n",
    "    text = f.read()\n",
    "    text = text.replace('  ', ' ')\n",
    "    text = text.replace('-\\n', '')\n",
    "    if text[-1] == '\\n': # убираем последний \\n, если такой есть \n",
    "        text = text[:-1]\n",
    "    f.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minimummaker(): #ф-ция превращения текстового файла с минимумом в питоновский список. Запаковка списка\n",
    "    with open('min.txt', encoding='utf-8') as file:\n",
    "        lemtokens = [morph.parse(i)[0].normal_form for i in re.findall('\\w+', file.read().lower())] #делаем список лемм слов\n",
    "        minimum = list(set(lemtokens)) #убираем повторы\n",
    "        output = open('minimum.pkl', 'wb')\n",
    "        pickle.dump(minimum, output, 2)\n",
    "        output.close()\n",
    "\n",
    "def loadminimum(): #распаковка cписка с минимумом. Возвращает неупорядоченный список\n",
    "    input = open('minimum.pkl', 'rb')\n",
    "    minimum = pickle.load(input)\n",
    "    input.close()\n",
    "    return minimum\n",
    "\n",
    "#minimummaker()\n",
    "minimum = loadminimum()\n",
    "#print(minimum[-50:]) #проверка работы\n",
    "#print(len(minimum)) #2549 слов в списке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# загрузка модели\n",
    "def model_loading(file):\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(file, binary=False)\n",
    "    model.init_sims(replace=True)\n",
    "    print('Done!') \n",
    "    return model\n",
    "\n",
    "model = model_loading('news_upos_cbow_600_2_2018.vec') #на загрузку тратится минут 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# загрузка словаря ASIS\n",
    "with open('syns.data', 'rb') as f:\n",
    "     asis = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Таблица конверсии в UPoS из тэгов Mystem\n",
    "# словарь, переводящий теги mystem в universal теги моделей\n",
    "mystem_tags = {'A' : 'ADJ',\n",
    "       'ADV' : 'ADV',\n",
    "       'ADVPRO' : 'ADV',\n",
    "       'ANUM' : 'ADJ',\n",
    "       'APRO' : 'DET',\n",
    "       'COM' : 'ADJ',\n",
    "       'CONJ' : 'SCONJ',\n",
    "       'INTJ' : 'INTJ',\n",
    "       'NONLEX' : 'X',\n",
    "       'NUM' : 'NUM',\n",
    "       'PART' : 'PART',\n",
    "       'PR' : 'ADP',\n",
    "       'S' : 'NOUN',\n",
    "       'SPRO' : 'PRON',\n",
    "       'UNKN' : 'X',\n",
    "       'V' : 'VERB',\n",
    "       'X' : 'X',\n",
    "      'PROPN' : 'PROPN'} #последних 2 тегов в майстеме нет, но они задаются в классе для слов в соответсвующими пометами\n",
    "\n",
    "# словарь, переводящий теги пайморфи в universal теги моделей\n",
    "pymorphy_tags = {'ADJF':'ADJ',\n",
    "    'ADJS' : 'ADJ',\n",
    "    'ADVB' : 'ADV',\n",
    "    'COMP' : 'ADV',\n",
    "    'GRND' : 'VERB',\n",
    "    'INFN' : 'VERB',\n",
    "    'NOUN' : 'NOUN',\n",
    "    'PRED' : 'ADV',\n",
    "    'PRTF' : 'ADJ',\n",
    "    'PRTS' : 'VERB',\n",
    "    'VERB' : 'VERB'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Пример вывода атрибутов объектов класса \\ntoken.num, token.text, token.lem, token.pos, token.complexity, token.is_complex(threshold, use_min):\\n\\n223 Кукушкин _NAMED_ENTITY_ S -19.06687873346149 False\\n224   _SPACE_ None -19.06687873346149 False\\n225 является являться V -7.502177090710664 False\\n226   _SPACE_ None -19.06687873346149 False\\n227 должником должник S -11.184186527172464 True\\n228   _SPACE_ None -19.06687873346149 False\\n229 банка банк S -9.300701131889967 False\\n230 .  _PUNKTUATION_ None -19.06687873346149 False\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Token():\n",
    "    def __init__(self, w):\n",
    "        \n",
    "        self.num = None # номер в тексте\n",
    "        self.complexity = None # сложность слова\n",
    "        \n",
    "        # три варианта инициализации: \n",
    "        ## из анализа текста, \n",
    "        ## из уже имеющегося объекта (для дочернего класса ComplexWord) \n",
    "        ## из строки\n",
    "        \n",
    "        if isinstance(w, dict): # если получили результат работы mystem\n",
    "            \n",
    "            self.text = w['text']  # сам токен\n",
    "            self.len = len(w['text']) # его длина\n",
    "            \n",
    "            # определяет, сделан ли анализ и, соответственно, рассматривать ли как слово, требующее упрощения\n",
    "            gram = w.get('analysis')\n",
    "            if gram:\n",
    "                self.lexem = gram[0]['lex']  # лемма\n",
    "                \n",
    "                if not self.named_entity(gram[0]):  # именованная сущность или нет\n",
    "                    self.pos = self.pos_tag(gram[0]['gr'])  # часть речи\n",
    "                else:\n",
    "                    self.pos = 'PROPN' # universal tag for named entity - у майстема таких нет\n",
    "                \n",
    "                    \n",
    "            elif any(p in w['text'] for p in punctuation+'–«»'): # если это знак пунктуации (может быть с пробелом!)\n",
    "                self.lexem = '_PUNKTUATION_'\n",
    "                self.pos = None\n",
    "            \n",
    "            elif not re.findall('\\S',w['text']): # если это только пробельные символы\n",
    "                self.lexem = '_SPACE_'\n",
    "                self.pos = None\n",
    "                \n",
    "            # остальное - неизвестная и ненужная ерунда?\n",
    "            else:\n",
    "                self.lexem = '_UNK_'\n",
    "                self.pos = 'X' # universal tag for unknown\n",
    "            \n",
    "            \n",
    "        elif isinstance(w, Token): # для определения объектов дочернего класса ComplexWord\n",
    "            self.text = w.text\n",
    "            self.num = w.num\n",
    "            self.lexem = w.lexem \n",
    "            self.len = w.len\n",
    "            self.pos = w.pos\n",
    "            self.complexity = w.complexity\n",
    "            \n",
    "        \n",
    "        elif isinstance(w, str): # если хотим как класс токен определить строку, полученную из словаря или модели\n",
    "            self.text = w\n",
    "            self.pos = None\n",
    "            self.lexem = w\n",
    "            self.len = len(w)\n",
    "            self.num = None\n",
    "            self.complexity = None\n",
    "            \n",
    "        \n",
    "    # вытаскивает часть речи из разбора майстем\n",
    "    def pos_tag(self,gram):\n",
    "        if ',' in gram:\n",
    "            gram = gram.split(',')[0]\n",
    "        if '=' in gram:\n",
    "            gram = gram.split('=')[0]\n",
    "        return gram\n",
    "        \n",
    "    # определяет по тегам, является ли именованной сущностью\n",
    "    def named_entity(self,gram):\n",
    "        markers = {'сокр': ' - сокращение', 'фам': ' - фамилия', 'имя': ' - имя собственное', 'гео': ' - название места', }\n",
    "        if any(m in gram['gr'] for m in markers.keys()):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def complexity_params(self, param = 'freq'):\n",
    "        # если по частотности\n",
    "        if param == 'freq':\n",
    "            self.complexity = unigrams.get(self.lexem, 0)\n",
    "            \n",
    "        # если по коэффициенту информативности. Отрицательное значение. Чем он меньше, тем сложнее\n",
    "        elif param == 'inf':\n",
    "            self.complexity = math.log((unigrams.get(self.lexem, 0)+1)/(sum(f for f in unigrams.values())+1))\n",
    "        \n",
    "    \n",
    "    def is_complex(self, threshold = '600', use_min = False, len_threshold = 1000):\n",
    "        exceptions = ['_PUNKTUATION_', '_SPACE_', '_UNK_']\n",
    "        # проверка, что это слово и что его нужно рассматривать как сложное (не нарицательное)\n",
    "        if not any(exception in self.lexem for exception in exceptions) and self.pos not in ['PROPN']:\n",
    "            \n",
    "            # если показатель сложности - вхождение в минимум\n",
    "            if use_min:\n",
    "                if self.lexem not in minimum:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "\n",
    "            # если показатель сложности - пороговое значение сложности\n",
    "            # также может использоваться длина. По умолчанию слишком большое - 1000 (т.е. этот параметр не учитывается)\n",
    "            else:\n",
    "                if self.complexity < float(threshold) or self.len > len_threshold:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def convert_universal(self):\n",
    "        if self.pos in mystem_tags:\n",
    "            self.pos = mystem_tags[self.pos]\n",
    "        else:\n",
    "            self.pos = 'X' # Х - universal тег для неизвестных слов\n",
    "        return self\n",
    "'''Пример вывода атрибутов объектов класса \n",
    "token.num, token.text, token.lem, token.pos, token.complexity, token.is_complex(threshold, use_min):\n",
    "\n",
    "223 Кукушкин _NAMED_ENTITY_ S -19.06687873346149 False\n",
    "224   _SPACE_ None -19.06687873346149 False\n",
    "225 является являться V -7.502177090710664 False\n",
    "226   _SPACE_ None -19.06687873346149 False\n",
    "227 должником должник S -11.184186527172464 True\n",
    "228   _SPACE_ None -19.06687873346149 False\n",
    "229 банка банк S -9.300701131889967 False\n",
    "230 .  _PUNKTUATION_ None -19.06687873346149 False\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''# вытаскивает часть речи из разбора майстем\n",
    "def pos(gram):\n",
    "    if ',' in gram:\n",
    "        gram = gram.split(',')[0]\n",
    "    if '=' in gram:\n",
    "        gram = gram.split('=')[0]\n",
    "    return gram\n",
    "'''\n",
    "\n",
    "#анализ текста \n",
    "def text_structuring(text, param, threshold, use_min):\n",
    "    # анализирует текст \n",
    "    analysis = m.analyze(text)\n",
    "    tokens = []\n",
    "    for i, w in enumerate(analysis): # состаляем список объектов Tokens\n",
    "        token = Token(w)\n",
    "        token.num = i # добавляем токену в атрибуты его номер в тексте\n",
    "        token.complexity_params(param) # переопределяем сложность на основе выбранного параметра\n",
    "        token.convert_universal() # превращаем POS в universal формат\n",
    "        print(token.num, token.text, token.lexem, token.pos, token.complexity, token.is_complex(threshold, use_min))\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ПАРАМЕТРЫ анализа слов\n",
    "complexity_type = 'inf'\n",
    "global_threshold = -8.5\n",
    "use_min=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 В в ADP -3.486155131943022 False\n",
      "1   _SPACE_ X -19.06687873346149 False\n",
      "2 Астраханской астраханский ADJ -11.87544940342511 True\n",
      "3   _SPACE_ X -19.06687873346149 False\n",
      "4 области область NOUN -8.188095962474803 False\n",
      "5   _SPACE_ X -19.06687873346149 False\n",
      "6 ветерану ветеран NOUN -10.97250028848853 True\n",
      "7   _SPACE_ X -19.06687873346149 False\n",
      "8 ВОВ вов PROPN -14.907995650101817 False\n",
      "9   _SPACE_ X -19.06687873346149 False\n",
      "10 вернули вернуть VERB -9.947667294896409 False\n",
      "11   _SPACE_ X -19.06687873346149 False\n",
      "12 похищенные похищать VERB -11.006654493020532 True\n",
      "13   _SPACE_ X -19.06687873346149 False\n",
      "14 медали медаль NOUN -10.586972126831268 True\n",
      "15 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "16 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "17 В в ADP -3.486155131943022 False\n",
      "18   _SPACE_ X -19.06687873346149 False\n",
      "19 Астраханской астраханский ADJ -11.87544940342511 True\n",
      "20   _SPACE_ X -19.06687873346149 False\n",
      "21 области область NOUN -8.188095962474803 False\n",
      "22   _SPACE_ X -19.06687873346149 False\n",
      "23 ветерану ветеран NOUN -10.97250028848853 True\n",
      "24   _SPACE_ X -19.06687873346149 False\n",
      "25 Великой великий ADJ -7.707722354468047 False\n",
      "26   _SPACE_ X -19.06687873346149 False\n",
      "27 Отечественной отечественный ADJ -9.66113606515282 True\n",
      "28   _SPACE_ X -19.06687873346149 False\n",
      "29 войны война NOUN -7.816461543327748 False\n",
      "30   _SPACE_ X -19.06687873346149 False\n",
      "31 вернули вернуть VERB -9.947667294896409 False\n",
      "32   _SPACE_ X -19.06687873346149 False\n",
      "33 украденные украсть VERB -10.412361351182033 True\n",
      "34   _SPACE_ X -19.06687873346149 False\n",
      "35 у у ADP -5.476072944878379 False\n",
      "36   _SPACE_ X -19.06687873346149 False\n",
      "37 него он PRON -4.13305326453326 False\n",
      "38   _SPACE_ X -19.06687873346149 False\n",
      "39 наградные наградные NOUN -19.06687873346149 True\n",
      "40   _SPACE_ X -19.06687873346149 False\n",
      "41 медали медаль NOUN -10.586972126831268 True\n",
      "42 .  _PUNKTUATION_ X -19.06687873346149 False\n",
      "43 Об об ADP -7.262754885511919 True\n",
      "44   _SPACE_ X -19.06687873346149 False\n",
      "45 этом это PRON -4.869477438777567 False\n",
      "46   _SPACE_ X -19.06687873346149 False\n",
      "47 сообщает сообщать VERB -8.634205145827439 False\n",
      "48   _SPACE_ X -19.06687873346149 False\n",
      "49 портал портал NOUN -12.693558943884478 True\n",
      "50  \" _PUNKTUATION_ X -19.06687873346149 False\n",
      "51 Утро утро NOUN -7.969832414948298 False\n",
      "52 . _PUNKTUATION_ X -19.06687873346149 False\n",
      "53 ру _UNK_ X -19.06687873346149 False\n",
      "54 \"  _PUNKTUATION_ X -19.06687873346149 False\n",
      "55 со со ADP -6.752575195748552 True\n",
      "56   _SPACE_ X -19.06687873346149 False\n",
      "57 ссылкой ссылка NOUN -10.338452641756877 True\n",
      "58   _SPACE_ X -19.06687873346149 False\n",
      "59 на на ADP -4.174175661002731 False\n",
      "60   _SPACE_ X -19.06687873346149 False\n",
      "61 пресс-службу пресс-служба NOUN -12.117981511148177 True\n",
      "62   _SPACE_ X -19.06687873346149 False\n",
      "63 регионального региональный ADJ -10.073575594367755 True\n",
      "64   _SPACE_ X -19.06687873346149 False\n",
      "65 МВД мвд PROPN -10.974639326737279 False\n",
      "66 . _PUNKTUATION_ X -19.06687873346149 False\n",
      "67 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "68 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "69 —  _UNK_ X -19.06687873346149 False\n",
      "70 Оперативники оперативник NOUN -12.197864282795782 True\n",
      "71   _SPACE_ X -19.06687873346149 False\n",
      "72 задержали задерживать VERB -9.988813400795125 True\n",
      "73   _SPACE_ X -19.06687873346149 False\n",
      "74 подозреваемую подозревать VERB -10.114143966354618 True\n",
      "75   _SPACE_ X -19.06687873346149 False\n",
      "76 в в ADP -3.486155131943022 False\n",
      "77   _SPACE_ X -19.06687873346149 False\n",
      "78 совершении совершение NOUN -10.903792357878274 True\n",
      "79   _SPACE_ X -19.06687873346149 False\n",
      "80 преступления преступление NOUN -9.447944818702611 True\n",
      "81 .  _PUNKTUATION_ X -19.06687873346149 False\n",
      "82 Ею она PRON -4.9255734554932324 False\n",
      "83   _SPACE_ X -19.06687873346149 False\n",
      "84 оказалась оказываться VERB -7.507967031654563 False\n",
      "85   _SPACE_ X -19.06687873346149 False\n",
      "86 61 _UNK_ X -19.06687873346149 False\n",
      "87 - _PUNKTUATION_ X -19.06687873346149 False\n",
      "88 летняя летний ADJ -9.444031438588096 False\n",
      "89   _SPACE_ X -19.06687873346149 False\n",
      "90 женщина женщина NOUN -7.588223853768708 False\n",
      "91 ,  _PUNKTUATION_ X -19.06687873346149 False\n",
      "92 которая который DET -5.3951392436881935 False\n",
      "93   _SPACE_ X -19.06687873346149 False\n",
      "94 помогала помогать VERB -8.13600640154562 False\n",
      "95   _SPACE_ X -19.06687873346149 False\n",
      "96 ветерану ветеран NOUN -10.97250028848853 True\n",
      "97   _SPACE_ X -19.06687873346149 False\n",
      "98 по по ADP -5.1820044394556914 False\n",
      "99   _SPACE_ X -19.06687873346149 False\n",
      "100 дому дом NOUN -7.446301380034336 False\n",
      "101 , —  _PUNKTUATION_ X -19.06687873346149 False\n",
      "102 сообщили сообщать VERB -8.634205145827439 False\n",
      "103   _SPACE_ X -19.06687873346149 False\n",
      "104 правоохранители правоохранитель NOUN -14.589541918983283 True\n",
      "105 . _PUNKTUATION_ X -19.06687873346149 False\n",
      "106 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "107 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "108 Женщина женщина NOUN -7.588223853768708 False\n",
      "109   _SPACE_ X -19.06687873346149 False\n",
      "110 объяснила объяснять VERB -8.424936082660874 False\n",
      "111   _SPACE_ X -19.06687873346149 False\n",
      "112 свои свой DET -5.485855828512082 False\n",
      "113   _SPACE_ X -19.06687873346149 False\n",
      "114 действия действие NOUN -8.051517218274066 False\n",
      "115   _SPACE_ X -19.06687873346149 False\n",
      "116 тем то PRON -5.111209789976896 False\n",
      "117 ,  _PUNKTUATION_ X -19.06687873346149 False\n",
      "118 что что SCONJ -4.319951223892165 False\n",
      "119   _SPACE_ X -19.06687873346149 False\n",
      "120 между между ADP -7.221517739852308 False\n",
      "121   _SPACE_ X -19.06687873346149 False\n",
      "122 ней она PRON -4.9255734554932324 False\n",
      "123   _SPACE_ X -19.06687873346149 False\n",
      "124 и и SCONJ -3.24763166905925 False\n",
      "125   _SPACE_ X -19.06687873346149 False\n",
      "126 ветераном ветеран NOUN -10.97250028848853 True\n",
      "127   _SPACE_ X -19.06687873346149 False\n",
      "128 возник возникать VERB -8.524383446466064 True\n",
      "129   _SPACE_ X -19.06687873346149 False\n",
      "130 конфликт конфликт NOUN -10.099502040248822 True\n",
      "131 ,  _PUNKTUATION_ X -19.06687873346149 False\n",
      "132 после после ADP -6.881269033189423 False\n",
      "133   _SPACE_ X -19.06687873346149 False\n",
      "134 чего что PRON -4.319951223892165 False\n",
      "135   _SPACE_ X -19.06687873346149 False\n",
      "136 она она PRON -4.9255734554932324 False\n",
      "137   _SPACE_ X -19.06687873346149 False\n",
      "138 забрала забирать VERB -9.812521476068532 True\n",
      "139   _SPACE_ X -19.06687873346149 False\n",
      "140 сковородку сковородка NOUN -12.13736196269784 True\n",
      "141   _SPACE_ X -19.06687873346149 False\n",
      "142 и и SCONJ -3.24763166905925 False\n",
      "143   _SPACE_ X -19.06687873346149 False\n",
      "144 награды награда NOUN -10.257165192953222 False\n",
      "145 ,  _PUNKTUATION_ X -19.06687873346149 False\n",
      "146 которые который DET -5.3951392436881935 False\n",
      "147   _SPACE_ X -19.06687873346149 False\n",
      "148 впоследствии впоследствии ADV -9.601353654626301 True\n",
      "149   _SPACE_ X -19.06687873346149 False\n",
      "150 выбросила выбрасывать VERB -10.114532190235263 True\n",
      "151   _SPACE_ X -19.06687873346149 False\n",
      "152 в в ADP -3.486155131943022 False\n",
      "153   _SPACE_ X -19.06687873346149 False\n",
      "154 урну урна NOUN -12.071112577156638 True\n",
      "155 . _PUNKTUATION_ X -19.06687873346149 False\n",
      "156 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "157 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "158 Полиция полиция NOUN -9.704847848244874 False\n",
      "159   _SPACE_ X -19.06687873346149 False\n",
      "160 обследовала обследовать VERB -11.754325235358891 True\n",
      "161   _SPACE_ X -19.06687873346149 False\n",
      "162 территорию территория NOUN -9.167750669702814 False\n",
      "163   _SPACE_ X -19.06687873346149 False\n",
      "164 городской городской ADJ -9.074556467224872 False\n",
      "165   _SPACE_ X -19.06687873346149 False\n",
      "166 свалки свалка NOUN -11.606963967220384 True\n",
      "167   _SPACE_ X -19.06687873346149 False\n",
      "168 и и SCONJ -3.24763166905925 False\n",
      "169   _SPACE_ X -19.06687873346149 False\n",
      "170 нашла находить VERB -7.451604282941571 False\n",
      "171   _SPACE_ X -19.06687873346149 False\n",
      "172 часть часть NOUN -7.373550217519937 False\n",
      "173   _SPACE_ X -19.06687873346149 False\n",
      "174 похищенных похищать VERB -11.006654493020532 True\n",
      "175   _SPACE_ X -19.06687873346149 False\n",
      "176 наград награда NOUN -10.257165192953222 False\n",
      "177   _SPACE_ X -19.06687873346149 False\n",
      "178 среди среди ADP -8.093024057697555 False\n",
      "179   _SPACE_ X -19.06687873346149 False\n",
      "180 твёрдых твердый ADJ -9.473318862324462 True\n",
      "181   _SPACE_ X -19.06687873346149 False\n",
      "182 бытовых бытовой ADJ -10.493116190557357 False\n",
      "183   _SPACE_ X -19.06687873346149 False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184 отходов отходы NOUN -12.430932177774844 True\n",
      "185 .  _PUNKTUATION_ X -19.06687873346149 False\n",
      "186 В в ADP -3.486155131943022 False\n",
      "187   _SPACE_ X -19.06687873346149 False\n",
      "188 отношении отношение NOUN -7.621322458862115 False\n",
      "189   _SPACE_ X -19.06687873346149 False\n",
      "190 задержанной задерживать VERB -9.988813400795125 True\n",
      "191   _SPACE_ X -19.06687873346149 False\n",
      "192 возбуждено возбуждать VERB -9.872871016106204 True\n",
      "193   _SPACE_ X -19.06687873346149 False\n",
      "194 уголовное уголовный ADJ -9.858339983431934 True\n",
      "195   _SPACE_ X -19.06687873346149 False\n",
      "196 дело дело NOUN -6.454827778200301 False\n",
      "197 . _PUNKTUATION_ X -19.06687873346149 False\n",
      "198 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "199 \n",
      " _SPACE_ X -19.06687873346149 False\n",
      "200 Напомним напоминать VERB -8.79944275302013 False\n",
      "201 ,  _PUNKTUATION_ X -19.06687873346149 False\n",
      "202 ранее ранее ADV -9.877966308898927 True\n",
      "203   _SPACE_ X -19.06687873346149 False\n",
      "204 Лайф лайф PROPN -14.956004869288178 False\n",
      "205   _SPACE_ X -19.06687873346149 False\n",
      "206 сообщал сообщать VERB -8.634205145827439 False\n",
      "207 ,  _PUNKTUATION_ X -19.06687873346149 False\n",
      "208 что что SCONJ -4.319951223892165 False\n",
      "209   _SPACE_ X -19.06687873346149 False\n",
      "210 у у ADP -5.476072944878379 False\n",
      "211   _SPACE_ X -19.06687873346149 False\n",
      "212 93 _UNK_ X -19.06687873346149 False\n",
      "213 - _PUNKTUATION_ X -19.06687873346149 False\n",
      "214 летнего летний ADJ -9.444031438588096 False\n",
      "215   _SPACE_ X -19.06687873346149 False\n",
      "216 Василия василий PROPN -9.180639074334914 False\n",
      "217   _SPACE_ X -19.06687873346149 False\n",
      "218 Легонького легонький ADJ -13.029007813539351 True\n",
      "219   _SPACE_ X -19.06687873346149 False\n",
      "220 украли украсть VERB -10.412361351182033 True\n",
      "221   _SPACE_ X -19.06687873346149 False\n",
      "222 из из ADP -5.430146815838169 False\n",
      "223   _SPACE_ X -19.06687873346149 False\n",
      "224 квартиры квартира NOUN -8.362735792650131 False\n",
      "225   _SPACE_ X -19.06687873346149 False\n",
      "226 10 _UNK_ X -19.06687873346149 False\n",
      "227   _SPACE_ X -19.06687873346149 False\n",
      "228 наградных наградные NOUN -19.06687873346149 True\n",
      "229   _SPACE_ X -19.06687873346149 False\n",
      "230 медалей медаль NOUN -10.586972126831268 True\n",
      "231  \" _PUNKTUATION_ X -19.06687873346149 False\n",
      "232 За за ADP -5.532348753209741 False\n",
      "233   _SPACE_ X -19.06687873346149 False\n",
      "234 отвагу отвага NOUN -11.840669723360818 True\n",
      "235 \", \" _PUNKTUATION_ X -19.06687873346149 False\n",
      "236 За за ADP -5.532348753209741 False\n",
      "237   _SPACE_ X -19.06687873346149 False\n",
      "238 взятие взятие NOUN -11.427236445603477 True\n",
      "239   _SPACE_ X -19.06687873346149 False\n",
      "240 Берлина берлин PROPN -10.198324692930289 False\n",
      "241 \"  _PUNKTUATION_ X -19.06687873346149 False\n",
      "242 и и SCONJ -3.24763166905925 False\n",
      "243  \" _PUNKTUATION_ X -19.06687873346149 False\n",
      "244 За за ADP -5.532348753209741 False\n",
      "245   _SPACE_ X -19.06687873346149 False\n",
      "246 взятие взятие NOUN -11.427236445603477 True\n",
      "247   _SPACE_ X -19.06687873346149 False\n",
      "248 Кёнигсберга кенигсберг PROPN -12.822711832797753 False\n",
      "249 \" _PUNKTUATION_ X -19.06687873346149 False\n",
      "250 . _PUNKTUATION_ X -19.06687873346149 False\n",
      "251 \n",
      " _SPACE_ X -19.06687873346149 False\n"
     ]
    }
   ],
   "source": [
    "text = reading('news3')\n",
    "tokens = text_structuring(text, complexity_type, global_threshold, use_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Substitution(Token):\n",
    "    def __init__(self, w):\n",
    "        super().__init__(w)\n",
    "        self.similarity = None\n",
    "        self.fitness = None\n",
    "    \n",
    "    # для слов из словаря и тезауруса: определяем тег пайморфи, переводим в формат universal - так быстрее, чем майстемом\n",
    "    def tagging(self, w):\n",
    "        tag = morph.parse(w)[0].tag.POS\n",
    "        if tag in pymorphy_tags:\n",
    "            return pymorphy_tags[tag]\n",
    "        else:\n",
    "            return 'X' # Х - universal тег для неизвестных слов\n",
    "\n",
    "    # в случае, когда Substitution получено не через модель, и его близость неизвестна\n",
    "    def measuse_similarity(self, target):\n",
    "        #print(self.lexem, target.lexem, self.pos, target.pos)\n",
    "        subst_query = str(self.lexem+'_'+self.pos)\n",
    "        target_query = str(target.lexem+'_'+target.pos)\n",
    "        if subst_query in model and target_query in model:\n",
    "            self.similarity = model.similarity(subst_query, target_query)\n",
    "        # может понадобиться return self\n",
    "    \n",
    "    # приписываем атрибуты словам, взятым из словаря или тезауруса\n",
    "    def setting_atr(self, target):\n",
    "        self.pos = self.tagging(self.lexem)\n",
    "        self.complexity_params(complexity_type)\n",
    "        self.measuse_similarity(target)\n",
    "        \n",
    "    def language_model(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Complex_word(Token):\n",
    "    def __init__(self, w):\n",
    "        super().__init__(w)\n",
    "        self.substituts = None\n",
    "        self.place = None\n",
    "        self.context = None\n",
    "        self.easier = []\n",
    "        \n",
    "    # список замен, в зависимости от выбранной базы\n",
    "    def search_substituts(self, base_type='model'):\n",
    "        \n",
    "        # поиск по модели\n",
    "        def model_search(lexem, pos):\n",
    "            query = str(lexem+'_'+pos)\n",
    "            #print(query)\n",
    "            if query in model:\n",
    "                # формируем список квазисинонимов той же части речи\n",
    "                # при этом превращаем их в объекты соответствующего класса\n",
    "                syn_tokens = []\n",
    "                for syn, sim in model.most_similar(positive=[query]):\n",
    "                    syn_text = syn[:syn.find('_')] # текст до части речи\n",
    "                    \n",
    "                    syn_tok = Substitution(syn_text) # из текстовой строки инициализируем объект класса\n",
    "                    syn_tok.pos = syn[syn_tok.len+1:] # часть речи\n",
    "                    \n",
    "                    syn_tok.complexity_params(complexity_type) # сложность по функции в зависимости от выбранного параметра\n",
    "                    \n",
    "                    syn_tok.similarity = sim # а близость по параметру модели\n",
    "                    \n",
    "                    syn_tokens.append(syn_tok)\n",
    "                    \n",
    "                return syn_tokens\n",
    "            else:\n",
    "                return [] \n",
    "\n",
    "        # поиск по YARN\n",
    "        def yarn_search(target, filepath = 'yarn-synsets1.csv'):\n",
    "            with open(filepath, \"r\", newline=\"\") as file: # постепенный просмотр файла с синсетами (множествами синонимов)\n",
    "                reader = csv.DictReader(file, delimiter=';')\n",
    "                lst = []\n",
    "                for i,row in enumerate(reader):\n",
    "                    cur_line = row['words'].split(';') # считываем колонку с синсетами\n",
    "                    if len(cur_line)>1:\n",
    "                        if target.lexem in cur_line:\n",
    "                            del(reader)\n",
    "                            for c in cur_line:\n",
    "                                if ' ' not in c and c!=target.lexem: # формируем список однословных синонимов\n",
    "                                    sub_tok = Substitution(c)\n",
    "                                    \n",
    "                                    sub_tok.setting_atr(target)\n",
    "                                    \n",
    "                                    if sub_tok not in lst:\n",
    "                                        lst.append(sub_tok) \n",
    "                            #TODO: выделить неоднословные в отдельный класс и поискать их частотность по n-граммам?\n",
    "                            break\n",
    "                #print(lst)\n",
    "                return lst \n",
    "        \n",
    "        #поиск по ASIS\n",
    "        def asis_search(target):\n",
    "            if target.lexem in asis:\n",
    "                lst = []\n",
    "                for s in asis[target.lexem]:\n",
    "                    if ' ' not in s: # формируем список однословных синонимов\n",
    "                        sub_tok = Substitution(s)\n",
    "                        sub_tok.setting_atr(target)\n",
    "                        lst.append(sub_tok)\n",
    "                return lst\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        \n",
    "        if base_type == 'model':\n",
    "            self.substituts = model_search(self.lexem, self.pos)\n",
    "            \n",
    "        if base_type == 'yarn':\n",
    "            self.substituts = yarn_search(self)\n",
    "        \n",
    "        if base_type == 'asis':\n",
    "            self.substituts = asis_search(self)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def find_easier(self, use_min = False, threshold = global_threshold):\n",
    "        for sub in self.substituts:\n",
    "            if not sub.is_complex(threshold = threshold, use_min = use_min):\n",
    "                if sub.complexity > self.complexity:\n",
    "                    self.easier.append(sub)\n",
    "        return self\n",
    "    \n",
    "    def make_window(self, tokens, window = 10):\n",
    "        context = [self]\n",
    "        left_ind = self.num-1\n",
    "        right_ind = self.num+1\n",
    "        ind = 0\n",
    "        # добавляем по одному слову слева и/или справа, пока не наберется window + само слово\n",
    "        while len(context)<window+1:\n",
    "            while left_ind >= 0:\n",
    "                left_w = tokens[left_ind]\n",
    "                left_ind-=1\n",
    "                # проверка, что это слово\n",
    "                if any(exception in left_w.lexem for exception in ['_PUNKTUATION_', '_SPACE_', '_UNK_']):\n",
    "                    continue\n",
    "                else:\n",
    "                    context[:0] = [left_w] #вставляем слово слева от цепочки\n",
    "                    ind+=1 # индекс слова сдвигается\n",
    "                    break\n",
    "\n",
    "            while right_ind < len(tokens):\n",
    "                right_w = tokens[right_ind]\n",
    "                right_ind+=1\n",
    "                # проверка, что это слово и что его нужно рассматривать как сложное (не нарицательное)\n",
    "                if any(exception in right_w.lexem for exception in ['_PUNKTUATION_', '_SPACE_', '_UNK_']):\n",
    "                    continue\n",
    "                else:\n",
    "                    context.append(right_w) # справа от цепочки\n",
    "                    break\n",
    "        self.place = ind\n",
    "        self.context = context\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ветеран NOUN yarn : старик NOUN -8.345441216450578 0.23961324224267147\n",
      "ветеран NOUN yarn : старик NOUN -8.345441216450578 0.23961324224267147\n",
      "отечественный ADJ yarn : родной ADJ -9.113982160667819 0.020030045830958295\n",
      "портал NOUN yarn : вход NOUN -9.731845917527988 0.0002885425674101231\n",
      "портал NOUN yarn : дверь NOUN -7.711707098541537 0.04702153840192069\n",
      "подозревать VERB yarn : думать VERB -7.108101460589846 0.03465083751933694\n",
      "ветеран NOUN yarn : старик NOUN -8.345441216450578 0.23961324224267147\n",
      "ветеран NOUN yarn : старик NOUN -8.345441216450578 0.23961324224267147\n",
      "возникать VERB yarn : подниматься VERB -8.366785291997186 0.20159820914507315\n",
      "возникать VERB yarn : появляться VERB -8.062597938932536 0.35273497014407196\n",
      "возникать VERB yarn : начинаться VERB -8.153027397811316 0.3254855920577131\n",
      "возникать VERB yarn : происходить VERB -7.775560500548489 0.4064410044068008\n",
      "забирать VERB yarn : собирать VERB -8.770268596015583 0.24647246517422808\n",
      "впоследствии ADV yarn : затем ADV -8.178040913119712 0.31609775090326603\n",
      "впоследствии ADV yarn : с X -4.45008658009755 None\n",
      "впоследствии ADV yarn : после X -6.881269033189423 None\n",
      "впоследствии ADV yarn : потом ADV -7.003049606872163 None\n",
      "урна NOUN yarn : ваза NOUN -11.285740223616473 0.18364909329940507\n",
      "городской ADJ yarn : социальный ADJ -8.836644657528602 0.1692542972850649\n",
      "твердый ADJ yarn : крепкий ADJ -9.446616532660201 0.19761365385260093\n",
      "бытовой ADJ yarn : обыкновенный ADJ -9.573768333394547 0.09706062873600835\n",
      "возбуждать VERB yarn : вызывать VERB -8.250787236909202 0.19681934685503943\n",
      "возбуждать VERB yarn : приглашать VERB -9.026764583393536 0.07855554080163785\n",
      "отвага NOUN yarn : мужество NOUN -10.635679255212228 0.6157282259832405\n"
     ]
    }
   ],
   "source": [
    "# Отбор сложных слов, превращение их в подкласс сложных слов и поиск замен\n",
    "# несложные слова также остаются на своих местах\n",
    "def selecting_complex(tokens, base_type=['yarn','model','asis'][0], threshold = global_threshold, use_min = False):\n",
    "    complex_words = []\n",
    "    for token in tokens:\n",
    "        if token.is_complex(threshold, use_min):\n",
    "            comp_token = Complex_word(token) # токен становится Сложным словом\n",
    "            \n",
    "            comp_token.search_substituts(base_type=base_type)\n",
    "            \n",
    "            complex_words.append(comp_token)\n",
    "            \n",
    "            # код для принтов\n",
    "            \n",
    "            '''\n",
    "            if comp_token.substituts:\n",
    "                for syn in comp_token.substituts:\n",
    "                    print (comp_token.lexem, comp_token.complexity, base_type, ':', syn.lexem, syn.complexity, syn.similarity)\n",
    "            '''\n",
    "            comp_token.find_easier(threshold, use_min)\n",
    "            if comp_token.easier:\n",
    "                for syn in comp_token.easier:\n",
    "                    print (comp_token.lexem, comp_token.pos, base_type, ':', syn.lexem, syn.pos, syn.complexity, syn.similarity)\n",
    "            \n",
    "        else:\n",
    "            complex_words.append(token)\n",
    "    return complex_words\n",
    "\n",
    "complex_words = selecting_complex(tokens, 'yarn', threshold = global_threshold, use_min = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решить проблему нахождения более простых синонимов. Пока возвращает только пустоту при use_min = True (по видимому, слов нет в минимуме?). Сейчас структура не очень логичная: сложные слова в тексте - те, что не вошли в минимум, но отнесение к сложным/простым синонимов происходит на основании порогового значения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### теперь, когда для каждого сложного слова есть список его синонимов с нужными параметрами, можно делать модель на н-граммах\n",
    "### контекстное окно: 2n слов (по n слева и справа), можно задавать. Вероятность на основе логарифма + Лапласса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngrams_dict = {0: {}, 1: unigrams, 2: bigrams, 3: trigrams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prob(ngram, n):\n",
    "    c1 = ' '.join(ngram[-n:]) # в числителе частота строки длины n\n",
    "    c2 = ' '.join(ngram[-n:-1]) # в знаменателе - строки без последнего символа\n",
    "    d = ngrams_dict.get(n) # для поиска числителя берем словарь n-грамм\n",
    "    d2 = ngrams_dict.get(n-1) # для поиска знаменателя - словарь n-1-грамм\n",
    "    V = len(unigrams)\n",
    "    #len(ngrams_dict.get(n-1,len(unigrams))) # сглаживание лапласса: добавляем размер словаря знаменателя\n",
    "    p1 = d.get(c1,0)\n",
    "    p2 = d2.get(c2,0)\n",
    "    print('\\t',c1, '/', c2, ':' , p1, '/', p2, '(', p1+1, '/', p2+V, ')')\n",
    "    return (p1+1)/(p2+V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t в астраханский область / в астраханский : 101 / 186 ( 102 / 282161 )\n",
      "\t астраханский область ветеран / астраханский область : 0 / 238 ( 1 / 282213 )\n",
      "\t область ветеран вов / область ветеран : 0 / 0 ( 1 / 281975 )\n",
      "\t ветеран вов вернуть / ветеран вов : 0 / 0 ( 1 / 281975 )\n",
      "\t вов вернуть похищать / вов вернуть : 0 / 0 ( 1 / 281975 )\n",
      "\n",
      "Вероятность слова _ветерану_ в контексте ['в', 'астраханский', 'область', 'ветеран', 'вов', 'вернуть', 'похищать'] = -57.124398760191426\n",
      "\n",
      "\t в астраханский область / в астраханский : 101 / 186 ( 102 / 282161 )\n",
      "\t астраханский область старик / астраханский область : 0 / 238 ( 1 / 282213 )\n",
      "\t область старик вов / область старик : 0 / 0 ( 1 / 281975 )\n",
      "\t старик вов вернуть / старик вов : 0 / 0 ( 1 / 281975 )\n",
      "\t вов вернуть похищать / вов вернуть : 0 / 0 ( 1 / 281975 )\n",
      "\n",
      "Вероятность замены _старик_ в контексте ['в', 'астраханский', 'область', 'старик', 'вов', 'вернуть', 'похищать'] = -57.124398760191426, разница = 0.0, меняем? - НЕТ\n",
      "\n",
      "\t в астраханский область / в астраханский : 101 / 186 ( 102 / 282161 )\n",
      "\t астраханский область ветеран / астраханский область : 0 / 238 ( 1 / 282213 )\n",
      "\t область ветеран великий / область ветеран : 0 / 0 ( 1 / 281975 )\n",
      "\t ветеран великий отечественный / ветеран великий : 104 / 104 ( 105 / 282079 )\n",
      "\t великий отечественный война / великий отечественный : 1432 / 1651 ( 1433 / 283626 )\n",
      "\n",
      "Вероятность слова _ветерану_ в контексте ['в', 'астраханский', 'область', 'ветеран', 'великий', 'отечественный', 'война'] = -45.20911979557953\n",
      "\n",
      "\t в астраханский область / в астраханский : 101 / 186 ( 102 / 282161 )\n",
      "\t астраханский область старик / астраханский область : 0 / 238 ( 1 / 282213 )\n",
      "\t область старик великий / область старик : 0 / 0 ( 1 / 281975 )\n",
      "\t старик великий отечественный / старик великий : 0 / 0 ( 1 / 281975 )\n",
      "\t великий отечественный война / великий отечественный : 1432 / 1651 ( 1433 / 283626 )\n",
      "\n",
      "Вероятность замены _старик_ в контексте ['в', 'астраханский', 'область', 'старик', 'великий', 'отечественный', 'война'] = -49.86271138671335, разница = 4.653591591133818, меняем? - НЕТ\n",
      "\n",
      "\t область ветеран великий / область ветеран : 0 / 0 ( 1 / 281975 )\n",
      "\t ветеран великий отечественный / ветеран великий : 104 / 104 ( 105 / 282079 )\n",
      "\t великий отечественный война / великий отечественный : 1432 / 1651 ( 1433 / 283626 )\n",
      "\t отечественный война вернуть / отечественный война : 0 / 2180 ( 1 / 284155 )\n",
      "\t война вернуть украсть / война вернуть : 0 / 0 ( 1 / 281975 )\n",
      "\n",
      "Вероятность слова _Отечественной_ в контексте ['область', 'ветеран', 'великий', 'отечественный', 'война', 'вернуть', 'украсть'] = -49.84029095233098\n",
      "\n",
      "\t область ветеран великий / область ветеран : 0 / 0 ( 1 / 281975 )\n",
      "\t ветеран великий родной / ветеран великий : 0 / 104 ( 1 / 282079 )\n",
      "\t великий родной война / великий родной : 0 / 0 ( 1 / 281975 )\n",
      "\t родной война вернуть / родной война : 0 / 0 ( 1 / 281975 )\n",
      "\t война вернуть украсть / война вернуть : 0 / 0 ( 1 / 281975 )\n",
      "\n",
      "Вероятность замены _родной_ в контексте ['область', 'ветеран', 'великий', 'родной', 'война', 'вернуть', 'украсть'] = -61.748237226564285, разница = 11.907946274233304, меняем? - НЕТ\n",
      "\n",
      "\t об это сообщать / об это : 360 / 45138 ( 361 / 327113 )\n",
      "\t это сообщать портал / это сообщать : 0 / 463 ( 1 / 282438 )\n",
      "\t сообщать портал утро / сообщать портал : 0 / 0 ( 1 / 281975 )\n",
      "\t портал утро со / портал утро : 0 / 0 ( 1 / 281975 )\n",
      "\t утро со ссылка / утро со : 0 / 4 ( 1 / 281979 )\n",
      "\n",
      "Вероятность слова _портал_ в контексте ['об', 'это', 'сообщать', 'портал', 'утро', 'со', 'ссылка'] = -56.00913260013538\n",
      "\n",
      "\t об это сообщать / об это : 360 / 45138 ( 361 / 327113 )\n",
      "\t это сообщать вход / это сообщать : 0 / 463 ( 1 / 282438 )\n",
      "\t сообщать вход утро / сообщать вход : 0 / 0 ( 1 / 281975 )\n",
      "\t вход утро со / вход утро : 0 / 0 ( 1 / 281975 )\n",
      "\t утро со ссылка / утро со : 0 / 4 ( 1 / 281979 )\n",
      "\n",
      "Вероятность замены _вход_ в контексте ['об', 'это', 'сообщать', 'вход', 'утро', 'со', 'ссылка'] = -56.00913260013538, разница = 0.0, меняем? - НЕТ\n",
      "\n",
      "\t об это сообщать / об это : 360 / 45138 ( 361 / 327113 )\n",
      "\t это сообщать дверь / это сообщать : 0 / 463 ( 1 / 282438 )\n",
      "\t сообщать дверь утро / сообщать дверь : 0 / 0 ( 1 / 281975 )\n",
      "\t дверь утро со / дверь утро : 0 / 0 ( 1 / 281975 )\n",
      "\t утро со ссылка / утро со : 0 / 4 ( 1 / 281979 )\n",
      "\n",
      "Вероятность замены _дверь_ в контексте ['об', 'это', 'сообщать', 'дверь', 'утро', 'со', 'ссылка'] = -56.00913260013538, разница = 0.0, меняем? - НЕТ\n",
      "\n",
      "\t мвд оперативник задерживать / мвд оперативник : 0 / 0 ( 1 / 281975 )\n",
      "\t оперативник задерживать подозревать / оперативник задерживать : 0 / 4 ( 1 / 281979 )\n",
      "\t задерживать подозревать в / задерживать подозревать : 0 / 0 ( 1 / 281975 )\n",
      "\t подозревать в совершение / подозревать в : 0 / 29 ( 1 / 282004 )\n",
      "\t в совершение преступление / в совершение : 131 / 438 ( 132 / 282413 )\n",
      "\n",
      "Вероятность слова _подозреваемую_ в контексте ['мвд', 'оперативник', 'задерживать', 'подозревать', 'в', 'совершение', 'преступление'] = -56.86673569524496\n",
      "\n",
      "\t мвд оперативник задерживать / мвд оперативник : 0 / 0 ( 1 / 281975 )\n",
      "\t оперативник задерживать думать / оперативник задерживать : 0 / 4 ( 1 / 281979 )\n",
      "\t задерживать думать в / задерживать думать : 0 / 0 ( 1 / 281975 )\n",
      "\t думать в совершение / думать в : 0 / 586 ( 1 / 282561 )\n",
      "\t в совершение преступление / в совершение : 131 / 438 ( 132 / 282413 )\n",
      "\n",
      "Вероятность замены _думать_ в контексте ['мвд', 'оперативник', 'задерживать', 'думать', 'в', 'совершение', 'преступление'] = -56.868708896490965, разница = 0.0019732012460025317, меняем? - НЕТ\n",
      "\n",
      "\t женщина который помогать / женщина который : 0 / 3021 ( 1 / 284996 )\n",
      "\t который помогать ветеран / который помогать : 0 / 942 ( 1 / 282917 )\n",
      "\t помогать ветеран по / помогать ветеран : 0 / 0 ( 1 / 281975 )\n",
      "\t ветеран по дом / ветеран по : 0 / 4 ( 1 / 281979 )\n",
      "\t по дом сообщать / по дом : 0 / 3086 ( 1 / 285061 )\n",
      "\n",
      "Вероятность слова _ветерану_ в контексте ['женщина', 'который', 'помогать', 'ветеран', 'по', 'дом', 'сообщать'] = -61.772759315460696\n",
      "\n",
      "\t женщина который помогать / женщина который : 0 / 3021 ( 1 / 284996 )\n",
      "\t который помогать старик / который помогать : 0 / 942 ( 1 / 282917 )\n",
      "\t помогать старик по / помогать старик : 0 / 26 ( 1 / 282001 )\n",
      "\t старик по дом / старик по : 0 / 87 ( 1 / 282062 )\n",
      "\t по дом сообщать / по дом : 0 / 3086 ( 1 / 285061 )\n",
      "\n",
      "Вероятность замены _старик_ в контексте ['женщина', 'который', 'помогать', 'старик', 'по', 'дом', 'сообщать'] = -61.77314582281459, разница = 0.0003865073538946717, меняем? - НЕТ\n",
      "\n",
      "\t между она и / между она : 396 / 595 ( 397 / 282570 )\n",
      "\t она и ветеран / она и : 0 / 17 ( 1 / 281992 )\n",
      "\t и ветеран возникать / и ветеран : 0 / 99 ( 1 / 282074 )\n",
      "\t ветеран возникать конфликт / ветеран возникать : 0 / 0 ( 1 / 281975 )\n",
      "\t возникать конфликт после / возникать конфликт : 0 / 123 ( 1 / 282098 )\n",
      "\n",
      "Вероятность слова _ветераном_ в контексте ['между', 'она', 'и', 'ветеран', 'возникать', 'конфликт', 'после'] = -55.7668875141538\n",
      "\n",
      "\t между она и / между она : 396 / 595 ( 397 / 282570 )\n",
      "\t она и старик / она и : 0 / 17 ( 1 / 281992 )\n",
      "\t и старик возникать / и старик : 0 / 1358 ( 1 / 283333 )\n",
      "\t старик возникать конфликт / старик возникать : 0 / 0 ( 1 / 281975 )\n",
      "\t возникать конфликт после / возникать конфликт : 0 / 123 ( 1 / 282098 )\n",
      "\n",
      "Вероятность замены _старик_ в контексте ['между', 'она', 'и', 'старик', 'возникать', 'конфликт', 'после'] = -55.771340950636976, разница = 0.004453436483174755, меняем? - НЕТ\n",
      "\n",
      "\t она и ветеран / она и : 0 / 17 ( 1 / 281992 )\n",
      "\t и ветеран возникать / и ветеран : 0 / 99 ( 1 / 282074 )\n",
      "\t ветеран возникать конфликт / ветеран возникать : 0 / 0 ( 1 / 281975 )\n",
      "\t возникать конфликт после / возникать конфликт : 0 / 123 ( 1 / 282098 )\n",
      "\t конфликт после что / конфликт после : 0 / 0 ( 1 / 281975 )\n",
      "\n",
      "Вероятность слова _возник_ в контексте ['она', 'и', 'ветеран', 'возникать', 'конфликт', 'после', 'что'] = -61.74871590186413\n",
      "\n",
      "\t она и ветеран / она и : 0 / 17 ( 1 / 281992 )\n",
      "\t и ветеран подниматься / и ветеран : 0 / 99 ( 1 / 282074 )\n",
      "\t ветеран подниматься конфликт / ветеран подниматься : 0 / 0 ( 1 / 281975 )\n",
      "\t подниматься конфликт после / подниматься конфликт : 0 / 0 ( 1 / 281975 )\n",
      "\t конфликт после что / конфликт после : 0 / 0 ( 1 / 281975 )\n",
      "\n",
      "Вероятность замены _подниматься_ в контексте ['она', 'и', 'ветеран', 'подниматься', 'конфликт', 'после', 'что'] = -61.7482797880918, разница = -0.00043611377233077064, меняем? - ДА\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t она и ветеран / она и : 0 / 17 ( 1 / 281992 )\n",
      "\t и ветеран появляться / и ветеран : 0 / 99 ( 1 / 282074 )\n",
      "\t ветеран появляться конфликт / ветеран появляться : 0 / 0 ( 1 / 281975 )\n",
      "\t появляться конфликт после / появляться конфликт : 0 / 0 ( 1 / 281975 )\n",
      "\t конфликт после что / конфликт после : 0 / 0 ( 1 / 281975 )\n",
      "\n",
      "Вероятность замены _появляться_ в контексте ['она', 'и', 'ветеран', 'появляться', 'конфликт', 'после', 'что'] = -61.7482797880918, разница = -0.00043611377233077064, меняем? - ДА\n",
      "\n",
      "\t она и ветеран / она и : 0 / 17 ( 1 / 281992 )\n",
      "\t и ветеран начинаться / и ветеран : 0 / 99 ( 1 / 282074 )\n",
      "\t ветеран начинаться конфликт / ветеран начинаться : 0 / 0 ( 1 / 281975 )\n",
      "\t начинаться конфликт после / начинаться конфликт : 0 / 6 ( 1 / 281981 )\n",
      "\t конфликт после что / конфликт после : 0 / 0 ( 1 / 281975 )\n",
      "\n",
      "Вероятность замены _начинаться_ в контексте ['она', 'и', 'ветеран', 'начинаться', 'конфликт', 'после', 'что'] = -61.74830106634755, разница = -0.0004148355165796147, меняем? - ДА\n",
      "\n",
      "\t она и ветеран / она и : 0 / 17 ( 1 / 281992 )\n",
      "\t и ветеран происходить / и ветеран : 0 / 99 ( 1 / 282074 )\n",
      "\t ветеран происходить конфликт / ветеран происходить : 0 / 0 ( 1 / 281975 )\n",
      "\t происходить конфликт после / происходить конфликт : 0 / 12 ( 1 / 281987 )\n",
      "\t конфликт после что / конфликт после : 0 / 0 ( 1 / 281975 )\n",
      "\n",
      "Вероятность замены _происходить_ в контексте ['она', 'и', 'ветеран', 'происходить', 'конфликт', 'после', 'что'] = -61.74832234415055, разница = -0.00039355771357918456, меняем? - ДА\n",
      "\n",
      "Лучшее значение вероятности: подниматься\n",
      "\n",
      "\t после что она / после что : 138 / 58 ( 139 / 282033 )\n",
      "\t что она забирать / что она : 0 / 120 ( 1 / 282095 )\n",
      "\t она забирать сковородка / она забирать : 0 / 161 ( 1 / 282136 )\n",
      "\t забирать сковородка и / забирать сковородка : 0 / 0 ( 1 / 281975 )\n",
      "\t сковородка и награда / сковородка и : 0 / 77 ( 1 / 282052 )\n",
      "\n",
      "Вероятность слова _забрала_ в контексте ['после', 'что', 'она', 'забирать', 'сковородка', 'и', 'награда'] = -56.8148695306031\n",
      "\n",
      "\t после что она / после что : 138 / 58 ( 139 / 282033 )\n",
      "\t что она собирать / что она : 0 / 120 ( 1 / 282095 )\n",
      "\t она собирать сковородка / она собирать : 0 / 15 ( 1 / 281990 )\n",
      "\t собирать сковородка и / собирать сковородка : 0 / 0 ( 1 / 281975 )\n",
      "\t сковородка и награда / сковородка и : 0 / 77 ( 1 / 282052 )\n",
      "\n",
      "Вероятность замены _собирать_ в контексте ['после', 'что', 'она', 'собирать', 'сковородка', 'и', 'награда'] = -56.814351915732445, разница = -0.0005176148706524941, меняем? - ДА\n",
      "\n",
      "Лучшее значение вероятности: собирать\n",
      "\n",
      "\t и награда который / и награда : 0 / 357 ( 1 / 282332 )\n",
      "\t награда который впоследствии / награда который : 0 / 57 ( 1 / 282032 )\n",
      "\t который впоследствии выбрасывать / который впоследствии : 0 / 709 ( 1 / 282684 )\n",
      "\t впоследствии выбрасывать в / впоследствии выбрасывать : 0 / 0 ( 1 / 281975 )\n",
      "\t выбрасывать в урна / выбрасывать в : 0 / 552 ( 1 / 282527 )\n",
      "\n",
      "Вероятность слова _впоследствии_ в контексте ['и', 'награда', 'который', 'впоследствии', 'выбрасывать', 'в', 'урна'] = -61.753802819776055\n",
      "\n",
      "\t и награда который / и награда : 0 / 357 ( 1 / 282332 )\n",
      "\t награда который затем / награда который : 0 / 57 ( 1 / 282032 )\n",
      "\t который затем выбрасывать / который затем : 0 / 512 ( 1 / 282487 )\n",
      "\t затем выбрасывать в / затем выбрасывать : 0 / 4 ( 1 / 281979 )\n",
      "\t выбрасывать в урна / выбрасывать в : 0 / 552 ( 1 / 282527 )\n",
      "\n",
      "Вероятность замены _затем_ в контексте ['и', 'награда', 'который', 'затем', 'выбрасывать', 'в', 'урна'] = -61.753119871160266, разница = -0.0006829486157897691, меняем? - ДА\n",
      "\n",
      "\t и награда который / и награда : 0 / 357 ( 1 / 282332 )\n",
      "\t награда который с / награда который : 0 / 57 ( 1 / 282032 )\n",
      "\t который с выбрасывать / который с : 0 / 9 ( 1 / 281984 )\n",
      "\t с выбрасывать в / с выбрасывать : 0 / 8 ( 1 / 281983 )\n",
      "\t выбрасывать в урна / выбрасывать в : 0 / 552 ( 1 / 282527 )\n",
      "\n",
      "Вероятность замены _с_ в контексте ['и', 'награда', 'который', 'с', 'выбрасывать', 'в', 'урна'] = -61.75135185642446, разница = -0.002450963351591895, меняем? - ДА\n",
      "\n",
      "\t и награда который / и награда : 0 / 357 ( 1 / 282332 )\n",
      "\t награда который после / награда который : 0 / 57 ( 1 / 282032 )\n",
      "\t который после выбрасывать / который после : 0 / 101 ( 1 / 282076 )\n",
      "\t после выбрасывать в / после выбрасывать : 0 / 0 ( 1 / 281975 )\n",
      "\t выбрасывать в урна / выбрасывать в : 0 / 552 ( 1 / 282527 )\n",
      "\n",
      "Вероятность замены _после_ в контексте ['и', 'награда', 'который', 'после', 'выбрасывать', 'в', 'урна'] = -61.75164969195225, разница = -0.002153127823802947, меняем? - ДА\n",
      "\n",
      "\t и награда который / и награда : 0 / 357 ( 1 / 282332 )\n",
      "\t награда который потом / награда который : 0 / 57 ( 1 / 282032 )\n",
      "\t который потом выбрасывать / который потом : 0 / 1218 ( 1 / 283193 )\n",
      "\t потом выбрасывать в / потом выбрасывать : 3 / 32 ( 4 / 282007 )\n",
      "\t выбрасывать в урна / выбрасывать в : 0 / 552 ( 1 / 282527 )\n",
      "\n",
      "Вероятность замены _потом_ в контексте ['и', 'награда', 'который', 'потом', 'выбрасывать', 'в', 'урна'] = -60.36942091545674, разница = -1.3843819043193122, меняем? - ДА\n",
      "\n",
      "Лучшее значение вероятности: потом\n",
      "\n",
      "\t впоследствии выбрасывать в / впоследствии выбрасывать : 0 / 0 ( 1 / 281975 )\n",
      "\t выбрасывать в урна / выбрасывать в : 0 / 552 ( 1 / 282527 )\n",
      "\t в урна полиция / в урна : 0 / 210 ( 1 / 282185 )\n",
      "\t урна полиция обследовать / урна полиция : 0 / 0 ( 1 / 281975 )\n",
      "\t полиция обследовать территория / полиция обследовать : 0 / 0 ( 1 / 281975 )\n",
      "\n",
      "Вероятность слова _урну_ в контексте ['впоследствии', 'выбрасывать', 'в', 'урна', 'полиция', 'обследовать', 'территория'] = -61.7505686439437\n",
      "\n",
      "\t впоследствии выбрасывать в / впоследствии выбрасывать : 0 / 0 ( 1 / 281975 )\n",
      "\t выбрасывать в ваза / выбрасывать в : 0 / 552 ( 1 / 282527 )\n",
      "\t в ваза полиция / в ваза : 0 / 203 ( 1 / 282178 )\n",
      "\t ваза полиция обследовать / ваза полиция : 0 / 0 ( 1 / 281975 )\n",
      "\t полиция обследовать территория / полиция обследовать : 0 / 0 ( 1 / 281975 )\n",
      "\n",
      "Вероятность замены _ваза_ в контексте ['впоследствии', 'выбрасывать', 'в', 'ваза', 'полиция', 'обследовать', 'территория'] = -61.7505438372147, разница = -2.4806729001625172e-05, меняем? - ДА\n",
      "\n",
      "Лучшее значение вероятности: ваза\n",
      "\n",
      "\t полиция обследовать территория / полиция обследовать : 0 / 0 ( 1 / 281975 )\n",
      "\t обследовать территория городской / обследовать территория : 0 / 21 ( 1 / 281996 )\n",
      "\t территория городской свалка / территория городской : 0 / 3 ( 1 / 281978 )\n",
      "\t городской свалка и / городской свалка : 0 / 44 ( 1 / 282019 )\n",
      "\t свалка и находить / свалка и : 0 / 80 ( 1 / 282055 )\n",
      "\n",
      "Вероятность слова _городской_ в контексте ['полиция', 'обследовать', 'территория', 'городской', 'свалка', 'и', 'находить'] = -61.74839328152462\n",
      "\n",
      "\t полиция обследовать территория / полиция обследовать : 0 / 0 ( 1 / 281975 )\n",
      "\t обследовать территория социальный / обследовать территория : 0 / 21 ( 1 / 281996 )\n",
      "\t территория социальный свалка / территория социальный : 0 / 0 ( 1 / 281975 )\n",
      "\t социальный свалка и / социальный свалка : 0 / 0 ( 1 / 281975 )\n",
      "\t свалка и находить / свалка и : 0 / 80 ( 1 / 282055 )\n",
      "\n",
      "Вероятность замены _социальный_ в контексте ['полиция', 'обследовать', 'территория', 'социальный', 'свалка', 'и', 'находить'] = -61.748226612311136, разница = -0.00016666921348473807, меняем? - ДА\n",
      "\n",
      "Лучшее значение вероятности: социальный\n",
      "\n",
      "\t похищать награда среди / похищать награда : 0 / 0 ( 1 / 281975 )\n",
      "\t награда среди твердый / награда среди : 0 / 0 ( 1 / 281975 )\n",
      "\t среди твердый бытовой / среди твердый : 0 / 0 ( 1 / 281975 )\n",
      "\t твердый бытовой отходы / твердый бытовой : 32 / 34 ( 33 / 282009 )\n",
      "\t бытовой отходы в / бытовой отходы : 5 / 80 ( 6 / 282055 )\n",
      "\n",
      "Вероятность слова _твёрдых_ в контексте ['похищать', 'награда', 'среди', 'твердый', 'бытовой', 'отходы', 'в'] = -56.46000568049872\n",
      "\n",
      "\t похищать награда среди / похищать награда : 0 / 0 ( 1 / 281975 )\n",
      "\t награда среди крепкий / награда среди : 0 / 0 ( 1 / 281975 )\n",
      "\t среди крепкий бытовой / среди крепкий : 0 / 4 ( 1 / 281979 )\n",
      "\t крепкий бытовой отходы / крепкий бытовой : 0 / 0 ( 1 / 281975 )\n",
      "\t бытовой отходы в / бытовой отходы : 5 / 80 ( 6 / 282055 )\n",
      "\n",
      "Вероятность замены _крепкий_ в контексте ['похищать', 'награда', 'среди', 'крепкий', 'бытовой', 'отходы', 'в'] = -59.95640685672285, разница = 3.4964011762241327, меняем? - НЕТ\n",
      "\n",
      "\t награда среди твердый / награда среди : 0 / 0 ( 1 / 281975 )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t среди твердый бытовой / среди твердый : 0 / 0 ( 1 / 281975 )\n",
      "\t твердый бытовой отходы / твердый бытовой : 32 / 34 ( 33 / 282009 )\n",
      "\t бытовой отходы в / бытовой отходы : 5 / 80 ( 6 / 282055 )\n",
      "\t отходы в отношение / отходы в : 0 / 20 ( 1 / 281995 )\n",
      "\n",
      "Вероятность слова _бытовых_ в контексте ['награда', 'среди', 'твердый', 'бытовой', 'отходы', 'в', 'отношение'] = -56.4600766062572\n",
      "\n",
      "\t награда среди твердый / награда среди : 0 / 0 ( 1 / 281975 )\n",
      "\t среди твердый обыкновенный / среди твердый : 0 / 0 ( 1 / 281975 )\n",
      "\t твердый обыкновенный отходы / твердый обыкновенный : 0 / 0 ( 1 / 281975 )\n",
      "\t обыкновенный отходы в / обыкновенный отходы : 0 / 0 ( 1 / 281975 )\n",
      "\t отходы в отношение / отходы в : 0 / 20 ( 1 / 281995 )\n",
      "\n",
      "Вероятность замены _обыкновенный_ в контексте ['награда', 'среди', 'твердый', 'обыкновенный', 'отходы', 'в', 'отношение'] = -61.74793939329908, разница = 5.287862787041874, меняем? - НЕТ\n",
      "\n",
      "\t в отношение задерживать / в отношение : 0 / 12093 ( 1 / 294068 )\n",
      "\t отношение задерживать возбуждать / отношение задерживать : 0 / 0 ( 1 / 281975 )\n",
      "\t задерживать возбуждать уголовный / задерживать возбуждать : 0 / 0 ( 1 / 281975 )\n",
      "\t возбуждать уголовный дело / возбуждать уголовный : 441 / 444 ( 442 / 282419 )\n",
      "\t уголовный дело напоминать / уголовный дело : 0 / 4019 ( 1 / 285994 )\n",
      "\n",
      "Вероятность слова _возбуждено_ в контексте ['в', 'отношение', 'задерживать', 'возбуждать', 'уголовный', 'дело', 'напоминать'] = -55.714276990384576\n",
      "\n",
      "\t в отношение задерживать / в отношение : 0 / 12093 ( 1 / 294068 )\n",
      "\t отношение задерживать вызывать / отношение задерживать : 0 / 0 ( 1 / 281975 )\n",
      "\t задерживать вызывать уголовный / задерживать вызывать : 0 / 0 ( 1 / 281975 )\n",
      "\t вызывать уголовный дело / вызывать уголовный : 0 / 0 ( 1 / 281975 )\n",
      "\t уголовный дело напоминать / уголовный дело : 0 / 4019 ( 1 / 285994 )\n",
      "\n",
      "Вероятность замены _вызывать_ в контексте ['в', 'отношение', 'задерживать', 'вызывать', 'уголовный', 'дело', 'напоминать'] = -61.804013503179135, разница = 6.089736512794559, меняем? - НЕТ\n",
      "\n",
      "\t в отношение задерживать / в отношение : 0 / 12093 ( 1 / 294068 )\n",
      "\t отношение задерживать приглашать / отношение задерживать : 0 / 0 ( 1 / 281975 )\n",
      "\t задерживать приглашать уголовный / задерживать приглашать : 0 / 0 ( 1 / 281975 )\n",
      "\t приглашать уголовный дело / приглашать уголовный : 0 / 0 ( 1 / 281975 )\n",
      "\t уголовный дело напоминать / уголовный дело : 0 / 4019 ( 1 / 285994 )\n",
      "\n",
      "Вероятность замены _приглашать_ в контексте ['в', 'отношение', 'задерживать', 'приглашать', 'уголовный', 'дело', 'напоминать'] = -61.804013503179135, разница = 6.089736512794559, меняем? - НЕТ\n",
      "\n",
      "\t наградные медаль за / наградные медаль : 0 / 0 ( 1 / 281975 )\n",
      "\t медаль за отвага / медаль за : 3 / 165 ( 4 / 282140 )\n",
      "\t за отвага за / за отвага : 0 / 130 ( 1 / 282105 )\n",
      "\t отвага за взятие / отвага за : 0 / 0 ( 1 / 281975 )\n",
      "\t за взятие берлин / за взятие : 12 / 67 ( 13 / 282042 )\n",
      "\n",
      "Вероятность слова _отвагу_ в контексте ['наградные', 'медаль', 'за', 'отвага', 'за', 'взятие', 'берлин'] = -57.79790824510816\n",
      "\n",
      "\t наградные медаль за / наградные медаль : 0 / 0 ( 1 / 281975 )\n",
      "\t медаль за мужество / медаль за : 0 / 165 ( 1 / 282140 )\n",
      "\t за мужество за / за мужество : 0 / 73 ( 1 / 282048 )\n",
      "\t мужество за взятие / мужество за : 0 / 8 ( 1 / 281983 )\n",
      "\t за взятие берлин / за взятие : 12 / 67 ( 13 / 282042 )\n",
      "\n",
      "Вероятность замены _мужество_ в контексте ['наградные', 'медаль', 'за', 'мужество', 'за', 'взятие', 'берлин'] = -59.184028904292475, разница = 1.3861206591843143, меняем? - НЕТ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in complex_words:\n",
    "    if isinstance(token, Complex_word):\n",
    "        if token.easier: \n",
    "            # окна контекста строим для тех сложных слов, для которых есть варианты замен, которые проще, чем слово\n",
    "            token.make_window(complex_words, window = 5)\n",
    "            \n",
    "            # вывод\n",
    "            context_lem = [c.lexem for c in token.context]\n",
    "            #print(token.text, token.place, ':', context_lem)\n",
    "            \n",
    "            # вычисляем вероятность цепочки\n",
    "            \n",
    "            # генерация 3-грамм и 2-грамм\n",
    "            text_3grams = [n for n in ngrams(context_lem, 3)]\n",
    "            \n",
    "            # добавляем слева искусственные н-граммы для вычисления вероятности\n",
    "            #text_3grams = [('', '', text_3grams[0][0]), ('', text_3grams[0][0], text_3grams[0][1])] + text_3grams\n",
    "            \n",
    "            #print(text_3grams)\n",
    "            p_context = 1.0\n",
    "            for ngram in text_3grams:\n",
    "                \n",
    "                p = math.log(prob(ngram, 3))\n",
    "                #print(ngram, p)\n",
    "                p_context+=p\n",
    "                \n",
    "            print('\\nВероятность слова _{0}_ в контексте {1} = {2}\\n'.format(token.text, context_lem, p_context))\n",
    "            \n",
    "            best_fitness = -1000\n",
    "            best_sub = None\n",
    "            for sub in token.easier:\n",
    "                sub_context_lem = context_lem[:token.place]+[sub.lexem]+context_lem[token.place+1:]\n",
    "                #сделать из этого куска функцию\n",
    "                sub_3grams = [n for n in ngrams(sub_context_lem, 3)]\n",
    "                # добавляем слева искусственные н-граммы для вычисления вероятности\n",
    "                #sub_3grams = [('', '', sub_3grams[0][0]), ('', sub_3grams[0][0], sub_3grams[0][1])] + sub_3grams\n",
    "            \n",
    "                p_changed_context = 1.0\n",
    "                for ngram in sub_3grams:\n",
    "\n",
    "                    p = math.log(prob(ngram, 3))\n",
    "                    #print(ngram, p)\n",
    "                    p_changed_context+=p\n",
    "                    \n",
    "                fit='НЕТ'\n",
    "                if p_changed_context>p_context:\n",
    "                    fit='ДА'\n",
    "                    sub.fitness = p_changed_context\n",
    "                    if sub.fitness > best_fitness:\n",
    "                        best_fitness = sub.fitness\n",
    "                        best_sub = sub\n",
    "                print('\\nВероятность замены _{0}_ в контексте {1} = {2}, разница = {3}, меняем? - {4}\\n'.format(sub.text, sub_context_lem, p_changed_context, p_context-p_changed_context, fit))\n",
    "            \n",
    "            if best_sub:\n",
    "                print('Лучшее значение вероятности: {0}\\n'.format(best_sub.lexem))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams.get('в астраханский',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
